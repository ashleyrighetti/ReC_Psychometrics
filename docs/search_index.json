[["exploratory-factor-analysis-1.html", "EXPLORATORY FACTOR ANALYSIS", " EXPLORATORY FACTOR ANALYSIS The next two lessons are devoted to exploratory factor analysis. The two approaches are principal components analysis (PCA) and principal axis factoring (PAF). In truth, only PAF is considered factor analysis. I will explain why in the lesson. These approaches are loosely termed exploratory because the statistical process (not the researcher) produces the factor (think scale or subscale) and identifies which items belong to it. This is contrasted with confirmatory approaches (which use structural equation modeling) where the researcher assigns items to factors and analyzes the goodness of fit. "],["PCA.html", "Chapter 8 Principal Components Analysis 8.1 Navigating this Lesson 8.2 Exploratory Principal Components Analysis 8.3 PCA Workflow 8.4 Research Vignette 8.5 Working the Vignette 8.6 APA Style Results 8.7 Back to the FutuRe: The relationship between PCA and item analysis 8.8 Practice Problems", " Chapter 8 Principal Components Analysis Screencasted Lecture Link In this lesson on principal components analysis (PCA) I provide an introduction to the exploratory factor analysis (EFA) arena. We will review the theoretical and technical aspects of PCA, we will work through a research vignette, and then consider the relationship of PCA to item analysis and reliability coefficients. Please note, although PCA is frequently grouped into EFA techniques, it is exploratory but it is not factor analysis. Well discuss the difference in the lecture. 8.1 Navigating this Lesson There is about two hours of lecture. If you work through the materials with me it would be plan for an additional hour-and-a-half. While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the Github site that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OERs introduction 8.1.1 Learning Objectives Focusing on this weeks materials, make sure you can: Distinguish between PCA and PAF on several levels: which path diagram represents each best keywords associated with each: factor loadings, linear components, describe versus explain. Recognize/define an identity matrix  what test would you use to diagnose it? Recognize/define multicollinearity and singularity  what test would you use to diagnose it? Describe the pattern of loadings (i.e., the relative weights of an item on its own scale compared to other scales)that supports the structure of the instrument. Compare the results from item analysis and PCA. 8.1.2 Planning for Practice In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results should map onto the ones obtained in the lecture. The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonettes (2020)Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PCA. As a third option, you are welcome to use data to which you have access and is suitable for PCA. These could include other vignettes from this OER, other simualated data, or your own data (presuming you have permissoin to use it). In either case, please plan to: Properly format and prepare the data. Conduct diagnostic tests to determine the suitability of the data for PCA. Conducting tests to guide the decisions about number of components to extract. Conducting orthogonal and oblique extractions (at least two each with different numbers of components). Selecting one solution and preparing an APA style results section (with table and figure). Compare your results in light of any other psychometrics lessons where you have used this data. 8.1.3 Readings &amp; Resources In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list. Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In An introduction to psychometric theory with applications in R. Retrieved from https://personality-project.org/r/book/#chapter6 pp. 145 to 150 (well continue with the rest in the next lecture). Stop at 6.2 Exploratory Factor Analysis. A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts. Revelle, W. (2019). How To: Use the psych package for Factor Analysis and data reduction. pp. 13 throuh 24 provide technical information about what we are doing Dekay, Nicole (2020). How to Make a Survey: A Quick Reference Guide on Psychometric Steps and Statistics. https://www.linkedin.com/posts/humanalysts_psychometricians-psychometrics-statistics-activity-6739634099712999424-VbUm Lewis, J. A., &amp; Neville, H. A. (2015). Construction and initial validation of the Gendered Racial Microaggressions Scale for Black women. Journal of Counseling Psychology, 62(2), 289302. https://doi.org/10.1037/cou0000062 Our research vignette for this lesson. 8.1.4 Packages The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. #will install the package if not already installed #if(!require(psych)){install.packages(&quot;psych&quot;)} #if(!require(tidyverse)){install.packages(&quot;tidyverse&quot;)} #if(!require(MASS)){install.packages(&quot;MASS&quot;)} #if(!require(sjstats)){install.packages(&quot;sjstats&quot;)} #if(!require(apaTables)){install.packages(&quot;apaTables&quot;)} #if(!require(qualtRics)){install.packages(&quot;qualtRics&quot;)} 8.2 Exploratory Principal Components Analysis The psychometric version of parsimony is seen in our attempt to describe (components) or to explain (factors) in the relationships between many observed variables in terms of a more limited set of components or latent factors. That is, we are trying to understand the structure of a set of variables, construct a questionnaire to measure an underlying latent variable, and reduce a data set to a more manageable size (think fewer columnsscale scores) while retainng as much of the information as possible 8.2.1 Some Framing Ideas (in very lay terms) Exploratory versus confirmatory factor analysis. Both exploratory and confirmatory approaches to components/factor analysis are used in scale construction. Think of scales as being interchangeable with factors and components. That said, factors and components are not interchangeable terms. Exploratory: Even though we may have an a priori model in mind, we explore the structure of the items by using diagnostics (KMO, Barletts, determinant), factor extraction, and rotation to determine the number of scales (i.e., components or factors) that exist within the raw data or correlation matrix. The algorithms (including matrix algebra) determine the relationship of each item to its respective scales (i.e., components or factors). Confirmatory: Starting with an a priori theory, we specify the structure (i.e., number and levels of factors) and which items belong to factors. We use structural equation modeling as the framework. And we only work with factors. We evaluate the quality of the model with a number of fit indices. Within the exploratory category we will focus on two further distinctions (there are even more): component analysis (principal components, or PCA) and principal axis factoring (PAF; one of the approaches that is commonly termed exploratory factor analysis, or EFA). In this first lecturette we focus on the differences between PCA and EFA. Option #1/Component model: PCA approximates the correlation matrix in terms of the product of components where each is a weighted linear sum of the variables. In the figure below, note how the arrows in the Components Analysis (a path model) point from variables to the component. Perhaps an oversimplification, think of each of these as a predictor variable contributing to an outcome. Option #2/Factor model: EFA (and in the next lesson, PAF/principal axis factoring) approximates the correlation matrix by the product of the two factors; this approach presumes that the factors are the causes (rather than as consequences). In the figure below, note how the arrows in the Factor Analysis model (a structural model) point from latent variable (or factor) to the observed variables (items). Factor analysis has been termed causal modeling because the latent variables are theorized to cause the responses to the individual items. There are other popular approaches, including parallel analysis (which is what the authors used in this lessons research vignette). Well-crafted figures provide important clues to the analyses. In structural models, rectangles and squares indicate the presence of observed (also called manifest) variables. These are variables that have a column in the dataset. In our particular case, they are the responses to the 25 items in the GRMS. Circles or ovals represent latent variables or factors. These were never raw data, but are composed of the relations of variables that were collected. They are more complex than mean or sum scores. Rather, they represent what the variables that represent them share in common. Comparison of path models for PCA and EFA for our research vignette Our focus today is on the principal component analysis (PCA) approach to scale construction. 8.3 PCA Workflow Below is a screenshot of the workflow. The original document is located in the Github site that hosts the ReCentering Psych Stats: Psychometrics OER. Image of the workflow for PCA Steps in the process include: Creating an items only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored). Conducting tests that assess the statistical assumptions of PCA to ensure that the data is appropriate for PCA. Determining the number of components (think subscales) to extract. Conducting the component extraction  this process will likely occur iteratively, exploring orthogonal (uncorrelated/independent) and oblique (correlated)components, and changing the number of components to extract Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PCA can be quite complex. Some important notions to consider that may not be obvious from lesson, are these: The values of component loadings are directly related to the correlation matrix. Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix. PCA is about dimension reduction; our goal is fewer components (think subscales) than there are items. In this lessons vignette there are 25 items on the scale and we will have 4 subscales. Principal component analysis is exploratory, but it is not factor analysis. Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution. 8.4 Research Vignette This lessons research vignette emerges from Lewis and Nevilles Gendered Racial Microaggressions Scale for Black Women (2015). The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. We simulate data from the final construction of the stress appraisal version as the basis of the lecture. Lewis and Neville (2015) reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them. The four factors, number of items, and sample item are as follows: Assumptions of Beauty and Sexual Objectification 10 items Objectified me based on physical features. Abbreviated in the simulated data as Obj# Silenced and Marginalized 7 items Someone has tried to put me in my place. Abbreviated in the simulated data as Marg# Strong Black Woman Stereotype 5 items I have been told that I am too assertive. Abbreviated in the simulated data as Strong# Angry Black Woman Stereotype 3 items Someone accused me of being angry when speaking calm. Abbreviated in the simulated data as Angry# Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items. set.seed(210921) GRMSmat &lt;- matrix(c(.69, .69, .60, .59, .55, .55, .54, .50, .41, .41, .04, -.15, .06, .12, .20, -.01, -.22, -.02, .02, .12, -.09, .06, .19, -.03, -.13, .07, -.07, .00, .07, -.18, .22, .23, -.01, .03, .02, .93, .81, .69, .67, .61, .58, .54, -.04, -.07, -.04, .00, .19, .00, .04, .08, -.08, -.08, 00, .06, .16, -.06, .08, .16, .22, .23, -.04, .01, -.05, -.11, -.16, .25, .16, .59, .55, .54, .54, .51, -.12, .08, .03, -.06, .03, .16, .01, .05, .09, -.08, -.06, .07, -.03, -.08, .18, .03, .06, .06, -.21, .21, .21, .03, -.06, .26, -.14, .70, .69, .68), ncol=4) #primary factor loadings for the four factors taken from the stress appraisal (left hand) factor loadings in Table 1 of the manuscript rownames(GRMSmat) &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;, &quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) #variable names for the 25 items colnames(GRMSmat) &lt;- c(&quot;Objectified&quot;, &quot;Marginalized&quot;, &quot;Strong&quot;, &quot;Angry&quot;) #component (subscale) names GRMSCorMat &lt;- GRMSmat %*% t(GRMSmat) #create the correlation matrix via some matrix algebra diag(GRMSCorMat) &lt;- 1 #SzyCorMat #prints the correlation matrix GRMS_M &lt;- c(1.78, 1.85, 1.97, 1.93, 2.01, 1.76, 1.91, 2.22, 1.83, 1.88, 2, 3.5, 2.43, 3.44, 2.39, 2.89, 2.7, 1.28, 2.25, 1.45, 1.57, 1.4, 2.02, 2.53, 2.39) #item means; I made these up based on the M and SDs for the factors GRMS_SD &lt;- c(1.11, 1.23, 0.97, 0.85, 1.19, 1.32, 1.04, 0.98, 1.01, 1.03, 1.01, 0.97, 1.32, 1.24, 1.31, 1.42, 1.2, 0.85, 0.94, 0.78, 1.11, 0.84, 1.14, 1.2, 1.21) #item standard deviations; I made these up based on the M and SDs for the factors GRMSCovMat &lt;- GRMS_SD %*% t(GRMS_SD) * GRMSCorMat #creates a covariance matrix (with more matrix algebra) from the correlation matrix dfGRMS &lt;- as.data.frame(round(MASS::mvrnorm(n=259, mu = GRMS_M, Sigma = GRMSCovMat, empirical = TRUE),0)) #creates the item level data from the sample size, mean, and covariance matrix; wrapped in commands to round to 0 decimal places and format as a df dfGRMS[dfGRMS&gt;5]&lt;-5 #restricts the upperbound of all variables to be 5 or less dfGRMS[dfGRMS&lt;0]&lt;-0 #resticts the lowerbound of all variable to be 0 or greater #colMeans(GRMS) #displays column means #Below is code if you would like an ID number for each case. Expecially at first, the ID number would just need to be removed, so I will not include it in the original simulation. We will add it later. #library(tidyverse) #dfGRMS &lt;- dfGRMS %&gt;% dplyr::mutate(ID = row_number()) #add ID to each row #dfGRMS &lt;- dfGRMS%&gt;%dplyr::select(ID, everything())#moving the ID number to the first column; requires Lets take a quick peek at the data to see if everthing looks right. psych::describe(dfGRMS) vars n mean sd median trimmed mad min max range skew kurtosis Obj1 1 259 1.78 1.09 2 1.78 1.48 0 5 5 0.16 -0.63 Obj2 2 259 1.90 1.14 2 1.90 1.48 0 5 5 0.10 -0.61 Obj3 3 259 1.97 1.01 2 1.96 1.48 0 4 4 0.10 -0.56 Obj4 4 259 1.92 0.89 2 1.90 1.48 0 4 4 0.18 -0.17 Obj5 5 259 2.05 1.19 2 2.05 1.48 0 5 5 0.00 -0.63 Obj6 6 259 1.81 1.29 2 1.75 1.48 0 5 5 0.32 -0.70 Obj7 7 259 1.95 1.09 2 1.95 1.48 0 5 5 0.16 -0.22 Obj8 8 259 2.21 1.00 2 2.21 1.48 0 5 5 -0.13 -0.39 Obj9 9 259 1.81 0.99 2 1.84 1.48 0 4 4 0.02 -0.40 Obj10 10 259 1.88 1.05 2 1.87 1.48 0 4 4 0.18 -0.43 Marg1 11 259 2.02 1.02 2 2.00 1.48 0 5 5 0.13 -0.28 Marg2 12 259 3.47 0.99 4 3.50 1.48 1 5 4 -0.31 -0.33 Marg3 13 259 2.44 1.30 2 2.45 1.48 0 5 5 -0.01 -0.71 Marg4 14 259 3.35 1.17 3 3.38 1.48 1 5 4 -0.14 -0.92 Marg5 15 259 2.40 1.31 2 2.39 1.48 0 5 5 0.11 -0.58 Marg6 16 259 2.85 1.37 3 2.89 1.48 0 5 5 -0.22 -0.67 Marg7 17 259 2.68 1.21 3 2.66 1.48 0 5 5 0.02 -0.32 Strong1 18 259 1.27 0.88 1 1.23 1.48 0 4 4 0.26 -0.49 Strong2 19 259 2.29 0.95 2 2.30 1.48 0 5 5 -0.17 -0.31 Strong3 20 259 1.45 0.83 1 1.44 1.48 0 4 4 0.09 -0.37 Strong4 21 259 1.60 1.10 2 1.57 1.48 0 5 5 0.27 -0.49 Strong5 22 259 1.41 0.83 1 1.41 1.48 0 4 4 -0.01 -0.40 Angry1 23 259 2.03 1.15 2 2.01 1.48 0 5 5 0.13 -0.48 Angry2 24 259 2.53 1.20 3 2.54 1.48 0 5 5 -0.04 -0.49 Angry3 25 259 2.39 1.16 2 2.41 1.48 0 5 5 -0.07 -0.45 se Obj1 0.07 Obj2 0.07 Obj3 0.06 Obj4 0.06 Obj5 0.07 Obj6 0.08 Obj7 0.07 Obj8 0.06 Obj9 0.06 Obj10 0.07 Marg1 0.06 Marg2 0.06 Marg3 0.08 Marg4 0.07 Marg5 0.08 Marg6 0.09 Marg7 0.08 Strong1 0.05 Strong2 0.06 Strong3 0.05 Strong4 0.07 Strong5 0.05 Angry1 0.07 Angry2 0.07 Angry3 0.07 The optional script below will let you save the simulated data to your computing environment as either a .csv file (think Excel lite) or .rds object (preserves any formatting you might do). #write the simulated data as a .csv #write.table(dfGRMS, file=&quot;dfGRMS.csv&quot;, sep=&quot;,&quot;, col.names=TRUE, row.names=FALSE) #bring back the simulated dat from a .csv file #dfGRMS &lt;- read.csv (&quot;dfGRMS.csv&quot;, header = TRUE) #to save the df as an .rds (think &quot;R object&quot;) file on your computer; it should save in the same file as the .rmd file you are working with #saveRDS(dfGRMS, &quot;dfGRMS.rds&quot;) #bring back the simulated dat from an .rds file #dfGRMS &lt;- readRDS(&quot;dfGRMS.rds&quot;) 8.5 Working the Vignette Below we will create a correlation matrix of our items. Whether we are conducting PCA or PAF, the dimension-reduction we are seeking is looking for clusters of correlated items in the \\(R\\)-matrix. Essentially, these are (Field, 2012): statistical entities that can be plotted as classification axes where coordinates of variables along each axis represent the strength of the relationship between that variable to each component/factor. mathematical equations, resembling regression equations, where each variable is represented according to its relative weight PCA in particular establishes which linear components exist within the data and how a particular variable might contribute to that component. Here is the correlation matrix of our items. It would be quite a daunting exercise to visually inspect this and manually cluster the correlations of items. GRMSmatrix&lt;-cor(dfGRMS) #correlation matrix created and saved as object round(GRMSmatrix, 2) Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 Marg1 Marg2 Obj1 1.00 0.41 0.38 0.33 0.34 0.34 0.37 0.35 0.25 0.24 0.11 0.00 Obj2 0.41 1.00 0.43 0.40 0.40 0.34 0.35 0.29 0.28 0.24 -0.02 -0.13 Obj3 0.38 0.43 1.00 0.36 0.34 0.31 0.30 0.28 0.22 0.24 0.01 -0.03 Obj4 0.33 0.40 0.36 1.00 0.32 0.32 0.32 0.25 0.27 0.28 0.06 -0.03 Obj5 0.34 0.40 0.34 0.32 1.00 0.23 0.26 0.26 0.27 0.25 -0.14 -0.18 Obj6 0.34 0.34 0.31 0.32 0.23 1.00 0.31 0.24 0.19 0.16 0.20 0.16 Obj7 0.37 0.35 0.30 0.32 0.26 0.31 1.00 0.26 0.19 0.26 0.23 0.11 Obj8 0.35 0.29 0.28 0.25 0.26 0.24 0.26 1.00 0.21 0.22 0.00 -0.09 Obj9 0.25 0.28 0.22 0.27 0.27 0.19 0.19 0.21 1.00 0.22 0.07 0.01 Obj10 0.24 0.24 0.24 0.28 0.25 0.16 0.26 0.22 0.22 1.00 0.06 -0.03 Marg1 0.11 -0.02 0.01 0.06 -0.14 0.20 0.23 0.00 0.07 0.06 1.00 0.66 Marg2 0.00 -0.13 -0.03 -0.03 -0.18 0.16 0.11 -0.09 0.01 -0.03 0.66 1.00 Marg3 0.05 -0.01 0.02 0.04 -0.11 0.16 0.12 -0.03 0.07 0.02 0.62 0.50 Marg4 0.17 0.05 0.10 0.07 -0.03 0.21 0.21 0.05 0.05 0.07 0.58 0.47 Marg5 0.20 0.07 0.11 0.13 0.00 0.21 0.22 0.04 0.10 0.12 0.55 0.41 Marg6 0.00 -0.09 -0.06 0.03 -0.11 0.06 0.16 0.03 0.05 0.13 0.51 0.43 Marg7 -0.14 -0.22 -0.09 -0.13 -0.17 0.00 -0.01 -0.10 -0.04 -0.04 0.42 0.42 Strong1 -0.05 -0.07 0.03 -0.02 0.11 -0.05 0.01 0.03 0.14 0.10 -0.04 0.05 Strong2 0.01 -0.06 0.01 0.04 0.13 -0.01 0.04 0.13 0.12 0.13 -0.10 -0.06 Strong3 0.04 0.05 0.07 0.14 0.15 0.02 0.10 0.10 0.13 0.14 -0.06 -0.09 Strong4 -0.11 -0.11 0.01 -0.04 0.06 -0.04 -0.05 0.05 0.11 0.09 -0.04 0.05 Strong5 0.02 0.00 -0.01 0.01 0.04 0.00 0.10 0.11 0.05 0.11 0.14 0.10 Angry1 0.11 0.15 0.20 0.15 0.11 0.15 0.01 0.08 0.08 0.03 -0.04 0.06 Angry2 -0.12 -0.04 0.08 -0.02 0.01 0.01 -0.07 -0.01 0.07 -0.05 -0.03 0.11 Angry3 -0.12 -0.09 0.03 -0.06 -0.05 0.02 -0.12 -0.11 0.05 -0.07 0.02 0.21 Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Strong3 Strong4 Strong5 Obj1 0.05 0.17 0.20 0.00 -0.14 -0.05 0.01 0.04 -0.11 0.02 Obj2 -0.01 0.05 0.07 -0.09 -0.22 -0.07 -0.06 0.05 -0.11 0.00 Obj3 0.02 0.10 0.11 -0.06 -0.09 0.03 0.01 0.07 0.01 -0.01 Obj4 0.04 0.07 0.13 0.03 -0.13 -0.02 0.04 0.14 -0.04 0.01 Obj5 -0.11 -0.03 0.00 -0.11 -0.17 0.11 0.13 0.15 0.06 0.04 Obj6 0.16 0.21 0.21 0.06 0.00 -0.05 -0.01 0.02 -0.04 0.00 Obj7 0.12 0.21 0.22 0.16 -0.01 0.01 0.04 0.10 -0.05 0.10 Obj8 -0.03 0.05 0.04 0.03 -0.10 0.03 0.13 0.10 0.05 0.11 Obj9 0.07 0.05 0.10 0.05 -0.04 0.14 0.12 0.13 0.11 0.05 Obj10 0.02 0.07 0.12 0.13 -0.04 0.10 0.13 0.14 0.09 0.11 Marg1 0.62 0.58 0.55 0.51 0.42 -0.04 -0.10 -0.06 -0.04 0.14 Marg2 0.50 0.47 0.41 0.43 0.42 0.05 -0.06 -0.09 0.05 0.10 Marg3 1.00 0.42 0.40 0.35 0.34 -0.06 -0.09 -0.09 -0.02 0.07 Marg4 0.42 1.00 0.44 0.28 0.27 -0.03 -0.08 -0.08 -0.04 0.06 Marg5 0.40 0.44 1.00 0.25 0.26 -0.10 -0.08 -0.10 -0.06 0.01 Marg6 0.35 0.28 0.25 1.00 0.29 0.10 0.09 0.10 0.11 0.23 Marg7 0.34 0.27 0.26 0.29 1.00 0.09 0.08 -0.01 0.19 0.13 Strong1 -0.06 -0.03 -0.10 0.10 0.09 1.00 0.26 0.24 0.34 0.19 Strong2 -0.09 -0.08 -0.08 0.09 0.08 0.26 1.00 0.27 0.31 0.21 Strong3 -0.09 -0.08 -0.10 0.10 -0.01 0.24 0.27 1.00 0.25 0.27 Strong4 -0.02 -0.04 -0.06 0.11 0.19 0.34 0.31 0.25 1.00 0.22 Strong5 0.07 0.06 0.01 0.23 0.13 0.19 0.21 0.27 0.22 1.00 Angry1 0.05 0.08 0.07 -0.16 0.08 0.09 -0.05 -0.10 0.06 -0.11 Angry2 0.02 0.06 -0.01 -0.08 0.19 0.22 0.05 -0.02 0.24 -0.02 Angry3 0.05 0.07 0.04 -0.09 0.20 0.18 0.03 -0.07 0.20 -0.05 Angry1 Angry2 Angry3 Obj1 0.11 -0.12 -0.12 Obj2 0.15 -0.04 -0.09 Obj3 0.20 0.08 0.03 Obj4 0.15 -0.02 -0.06 Obj5 0.11 0.01 -0.05 Obj6 0.15 0.01 0.02 Obj7 0.01 -0.07 -0.12 Obj8 0.08 -0.01 -0.11 Obj9 0.08 0.07 0.05 Obj10 0.03 -0.05 -0.07 Marg1 -0.04 -0.03 0.02 Marg2 0.06 0.11 0.21 Marg3 0.05 0.02 0.05 Marg4 0.08 0.06 0.07 Marg5 0.07 -0.01 0.04 Marg6 -0.16 -0.08 -0.09 Marg7 0.08 0.19 0.20 Strong1 0.09 0.22 0.18 Strong2 -0.05 0.05 0.03 Strong3 -0.10 -0.02 -0.07 Strong4 0.06 0.24 0.20 Strong5 -0.11 -0.02 -0.05 Angry1 1.00 0.44 0.43 Angry2 0.44 1.00 0.47 Angry3 0.43 0.47 1.00 This correlation matrix is so big that you might wish to write code so that you can examine it in sections #round(GRMSmatrix[,1:8], 2) #round(GRMSmatrix[,9:16], 2) #round(GRMSmatrix[,17:25], 2) With component and factor analytic procedures we can analyze the data with either raw data or correlation matrix. Producing the matrix helps us see how this is a structural analysis. That is, we are trying to see if our more parsimonious extraction reproduces this original correlation matrix. 8.5.1 Three Diagnostic Tests to Evaluate the Appropriateness of the Data for Component-or-Factor Analysis Below is a snip from the workflow to remind us where we are in the steps to PCA. Image of an excerpt from the workflow 8.5.1.1 Is my sample adequate for PCA? There have been a number of generic guidelines (some supported by analyses; some not) about how big the sample size should be: 10-15 participants per variable 10X as many participants as variables (Nunnally, 1978) 5 and 10 participants per variable up to 300 (Kass &amp; Tinsley, 1979) 300 (Tabachnick &amp; Fidell, 2007) 1000 = excellent, 300 = good, 100 = poor (Comrey &amp; Lee, 1992) Of course it is more complicated. Monte Carlo studies have shown that if factor loadings are large (~.6), the solution is reliable regardless of size if communalities are large (~.6), relatively small samples (~100) are sufficient, but when they are lower (well below .5), then larger samples (&gt;500 are indicated). The Kaiser-Meyer-Olkin index (KMO) is an index of sampling adequacy that can be used with the actual sample to let us know if the sample size is sufficient relative to the statistical characteristics of the data. If it is below the thresshold we should probably collect more data to see if it can achieve a satisfactory value. Kaisers 1974 recommendations were: bare minimum of .5 values between .5 and .7 as mediocre values between .7 and .8 as good values above .9 are superb Revelle has included a KMO test in the psych package. The function can use either raw or matrix data. Either way, the only variables in the matrix should be the items of interest. This means that everything else (e.g., total or subscale scores, ID numbers) should be removed. psych::KMO(dfGRMS) Kaiser-Meyer-Olkin factor adequacy Call: psych::KMO(r = dfGRMS) Overall MSA = 0.86 MSA for each item = Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 0.89 0.88 0.90 0.90 0.91 0.92 0.92 0.90 0.88 0.90 Marg1 Marg2 Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Strong3 0.83 0.88 0.91 0.91 0.91 0.87 0.90 0.79 0.79 0.80 Strong4 Strong5 Angry1 Angry2 Angry3 0.79 0.81 0.72 0.75 0.75 #psych::KMO(GRMSmatrix) We examine the KMO values for both the overall matrix and the individual items. At the matrix level, our \\(KMO = .86\\), which falls into Kaisers definition of good. At the item level, the KMO should be &gt; .50. Variables with values below .5 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference). Because removing and adding variables impacts the KMO, be sure to re-evaluate the sampling adequacy if changes are made to the items (and/or sample size). At the item level, our KMO values range between .72 (Angry1) and .92 (Obj6, Obj7). Considering both item- and matrix- levels, we conclude that the sample size and the data are adequate for component-or-factor analysis. 8.5.1.2 Are there correlations among the variables that are large enough to be analyzed? Bartletts test lets us know if a matrix is an identity matrix. In an identity matrix, then all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0. A signifcant Barletts (i.e., \\(p &lt; .05\\)) tells that the \\(R\\)-matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed. The cortest.bartlett() function is in the psych package and can be run either from the raw data or R matrix formats. psych::cortest.bartlett(dfGRMS) #from the raw data R was not square, finding R from data $chisq [1] 1683.76 $p.value [1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005520916 $df [1] 300 #raw data produces the warning &quot;R was not square, finding R from data.&quot; This means nothing other than we fed it raw data and the function is creating a matrix from which to do the analysis. #psych::cortest.bartlett(GRMSmatrix, n = 259) #if using the matrix, must specify sample Our Bartletts test is significant: \\(\\chi ^{1}(300)=1683.76, p &lt; .001\\). This supports a component-or-factor analytic approach for investigating the data. 8.5.1.3 Is there multicollinearity or singularity in my data? The determinant of the correlation matrix should be greater than 0.00001 (that would be 4 zeros, then the 1). If it is smaller than 0.00001 then we may have an issue with multicollinearity (i.e., variables that are too highly correlated) or singularity (variables that are perfectly correlated). The determinant function we use comes from base R. It is easiest to compute when the correlation matrix is the object. However, it is also possible to specify the command to work with the raw data. det(GRMSmatrix) [1] 0.001151581 #det(cor(dfGRMS))#if using the raw data With a value of 0.00115, our determinant is greater than the 0.00001 requirement. If it were not, then we could identify problematic variables (i.e., those correlating too highly with others; those not correlating sufficiently with others) and re-run the diagnostic statitics. Summary: Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer-Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barletts Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartletts test is &lt; .05, we are fairly certain we have clusters of correlated variables. In our dataset, \\(\\chi ^{1}(300)=1683.76, p &lt; .001\\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis. 8.5.2 Principal Components Analysis Below is a snip from the workflow to remind us where we are in the steps to PCA. Image of an excerpt from the workflow We can use the principal() function from the psych package with raw or matrix data. We start by creating a principal components model that has the same number of components as there are variables in the data. This allows us to inspect the components eigenvalues and make decisions about which to extract. Note, this is different than actual factor analysis  where you must extract fewer factors than variables (e.g., extracting 18 [an arbitray number] instead of 25). #The numerous codes all result in the same. They simply swap out using the df or r-matrix, and whether I specify the number of factors or write code to instruct R to calculate it. #pca1 &lt;- psych::principal(GRMSmatrix, nfactors=25, rotate = &quot;none&quot;) #using the matrix form of the data and specifying the # factors #pca1 &lt;- psych::principal(GRMSmatrix, nfactors=length(GRMSmatrix[,1]), rotate = &quot;none&quot;) #using the matrix form of the data and letting the length function automatically calculate the # factors as a function of how many columns in the matrix #pca1 &lt;- psych::principal(dfGRMS, nfactors=25, rotate=&quot;none&quot;) #using raw data and specifying # factors pca1 &lt;- psych::principal(dfGRMS, nfactors=length(dfGRMS), rotate=&quot;none&quot;)# using raw data and letting the length function automatically calculate the # factors as a function of how many columns in the raw data pca1 Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = length(dfGRMS), rotate = &quot;none&quot;) Standardized loadings (pattern matrix) based upon correlation matrix PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 Obj1 0.55 0.40 -0.17 0.02 0.12 -0.17 -0.13 -0.12 -0.16 0.02 0.14 -0.01 Obj2 0.46 0.54 -0.15 0.14 0.02 0.18 -0.03 -0.09 0.03 0.10 -0.15 0.03 Obj3 0.48 0.43 0.04 0.21 0.13 0.08 0.11 0.01 -0.16 0.05 -0.20 -0.29 Obj4 0.50 0.42 -0.03 0.04 -0.15 0.25 0.12 0.27 0.24 -0.06 0.16 0.00 Obj5 0.33 0.57 0.13 0.02 -0.08 -0.01 -0.05 -0.01 -0.32 0.23 -0.19 0.04 Obj6 0.56 0.19 -0.09 0.15 0.17 0.12 -0.19 0.33 0.06 -0.27 -0.31 0.21 Obj7 0.59 0.22 -0.09 -0.13 0.11 0.09 0.15 -0.03 -0.21 -0.13 0.13 0.27 Obj8 0.38 0.40 0.06 -0.09 0.37 -0.44 -0.14 -0.15 0.32 -0.20 0.09 -0.25 Obj9 0.40 0.28 0.21 -0.01 -0.55 -0.05 -0.48 -0.17 0.23 0.12 0.03 0.06 Obj10 0.40 0.28 0.12 -0.19 -0.33 -0.24 0.62 -0.06 0.17 0.02 -0.09 0.04 Marg1 0.63 -0.62 -0.13 -0.10 -0.02 0.04 -0.03 -0.03 0.00 -0.01 0.02 -0.03 Marg2 0.47 -0.66 0.05 0.05 0.00 0.07 -0.06 -0.02 -0.03 -0.14 -0.03 0.05 Marg3 0.50 -0.53 -0.11 0.02 -0.08 0.06 -0.10 0.01 0.12 0.08 -0.08 -0.11 Marg4 0.57 -0.42 -0.10 0.08 0.10 -0.05 0.00 -0.09 -0.23 0.02 0.17 -0.11 Marg5 0.58 -0.35 -0.17 0.08 -0.11 -0.17 0.05 0.08 -0.14 0.27 0.20 -0.06 Marg6 0.40 -0.44 0.07 -0.42 -0.08 0.03 0.09 -0.06 0.15 -0.30 -0.10 0.03 Marg7 0.23 -0.59 0.28 0.01 0.08 -0.10 0.04 0.19 -0.01 0.21 -0.16 -0.06 Strong1 0.04 0.02 0.65 -0.13 -0.13 0.07 -0.02 -0.35 -0.33 -0.34 0.01 0.03 Strong2 0.04 0.13 0.53 -0.35 0.04 -0.35 -0.11 0.43 -0.11 0.02 0.19 0.26 Strong3 0.09 0.21 0.41 -0.45 0.04 0.44 -0.01 0.18 0.01 0.02 0.34 -0.32 Strong4 0.02 -0.04 0.70 -0.15 0.00 -0.09 -0.03 0.12 -0.04 0.05 -0.33 -0.26 Strong5 0.18 -0.06 0.34 -0.47 0.39 0.19 0.02 -0.30 0.23 0.38 -0.05 0.30 Angry1 0.19 0.07 0.30 0.70 0.11 0.01 0.12 -0.02 0.14 0.01 0.16 0.06 Angry2 0.03 -0.11 0.55 0.55 0.08 0.02 0.06 -0.08 0.09 -0.02 0.08 -0.03 Angry3 0.02 -0.21 0.49 0.59 -0.04 0.05 0.02 0.02 0.03 0.01 0.07 0.16 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22 PC23 PC24 Obj1 0.19 0.01 -0.13 0.11 -0.35 0.27 0.01 -0.21 -0.01 -0.13 0.24 0.08 Obj2 -0.12 0.00 -0.02 -0.04 0.14 0.08 0.13 -0.03 0.25 0.46 0.21 -0.03 Obj3 -0.06 0.07 -0.52 0.02 0.11 -0.09 -0.10 0.11 0.00 -0.14 -0.13 -0.02 Obj4 -0.10 0.29 0.04 -0.26 -0.20 -0.26 -0.07 -0.10 -0.16 -0.02 0.12 0.04 Obj5 -0.07 0.28 0.40 0.06 0.09 0.14 -0.18 0.11 -0.11 -0.09 -0.06 -0.03 Obj6 0.33 -0.17 0.12 0.11 0.06 -0.13 0.00 0.00 0.09 -0.10 -0.03 0.06 Obj7 -0.43 -0.36 0.03 -0.03 -0.07 0.04 0.13 0.08 -0.13 -0.06 -0.12 0.00 Obj8 -0.08 0.01 0.15 0.03 0.05 -0.09 -0.05 0.19 -0.11 0.09 0.00 -0.02 Obj9 -0.07 -0.18 -0.10 0.06 0.04 -0.06 -0.07 -0.09 -0.02 -0.05 -0.13 -0.03 Obj10 0.23 -0.11 0.02 0.12 0.12 0.03 0.05 -0.04 -0.06 -0.02 0.05 -0.05 Marg1 0.00 0.03 0.00 -0.01 0.02 0.01 0.00 0.03 -0.02 0.02 0.03 -0.06 Marg2 0.05 0.04 -0.05 -0.05 -0.01 0.09 -0.11 0.07 -0.11 0.02 0.13 -0.46 Marg3 -0.03 0.20 0.01 0.06 0.10 0.13 0.49 0.13 -0.14 -0.13 0.02 0.15 Marg4 0.14 -0.05 0.08 -0.20 0.30 -0.07 -0.08 -0.37 -0.12 0.11 -0.13 0.12 Marg5 0.12 -0.10 0.08 -0.12 -0.15 -0.19 -0.05 0.32 0.33 -0.01 0.00 0.05 Marg6 -0.15 0.18 -0.03 -0.02 -0.04 0.27 -0.27 -0.01 0.25 0.00 -0.11 0.20 Marg7 -0.25 -0.02 0.02 0.44 -0.16 -0.20 -0.09 -0.17 -0.06 0.14 0.06 0.05 Strong1 0.10 0.16 0.00 0.09 -0.09 -0.30 0.15 0.05 0.04 0.07 0.04 0.04 Strong2 -0.04 0.18 -0.20 -0.05 0.19 0.06 0.10 -0.03 0.08 0.04 0.02 -0.04 Strong3 0.13 -0.14 0.09 0.22 0.07 0.14 -0.02 0.06 0.02 0.02 -0.01 -0.03 Strong4 0.00 -0.21 0.08 -0.38 -0.23 0.11 0.12 -0.08 -0.01 0.04 -0.04 -0.02 Strong5 0.13 0.04 -0.06 -0.09 -0.01 -0.10 -0.02 0.01 -0.01 -0.08 -0.03 0.01 Angry1 0.05 0.16 0.06 0.09 -0.17 0.11 0.14 -0.11 0.15 0.06 -0.38 -0.14 Angry2 -0.21 -0.08 0.11 -0.04 0.22 0.00 -0.03 -0.09 0.21 -0.33 0.28 0.02 Angry3 0.13 -0.09 -0.12 -0.03 0.00 0.17 -0.18 0.26 -0.27 0.20 0.07 0.23 PC25 h2 u2 com Obj1 0.00 1 -0.00000000000000289 6.7 Obj2 0.01 1 0.00000000000000155 5.4 Obj3 -0.01 1 0.00000000000000089 5.8 Obj4 0.01 1 0.00000000000000000 8.1 Obj5 -0.02 1 0.00000000000000078 6.1 Obj6 0.00 1 0.00000000000000011 6.8 Obj7 0.02 1 0.00000000000000100 5.4 Obj8 0.01 1 0.00000000000000144 8.2 Obj9 0.01 1 0.00000000000000100 5.5 Obj10 0.00 1 -0.00000000000000133 4.9 Marg1 -0.43 1 -0.00000000000000133 3.0 Marg2 0.16 1 -0.00000000000000067 3.5 Marg3 0.10 1 -0.00000000000000067 4.9 Marg4 0.08 1 -0.00000000000000133 5.9 Marg5 0.08 1 0.00000000000000011 6.3 Marg6 0.07 1 -0.00000000000000089 8.3 Marg7 0.04 1 0.00000000000000133 5.5 Strong1 0.00 1 0.00000000000000200 4.3 Strong2 -0.02 1 0.00000000000000155 6.6 Strong3 0.01 1 -0.00000000000000022 7.2 Strong4 -0.02 1 0.00000000000000011 3.6 Strong5 0.02 1 0.00000000000000022 7.6 Angry1 -0.03 1 0.00000000000000044 3.6 Angry2 -0.02 1 0.00000000000000078 4.9 Angry3 -0.01 1 0.00000000000000011 5.2 PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 SS loadings 4.04 3.61 2.41 2.09 0.88 0.84 0.80 0.78 0.75 0.71 0.68 Proportion Var 0.16 0.14 0.10 0.08 0.04 0.03 0.03 0.03 0.03 0.03 0.03 Cumulative Var 0.16 0.31 0.40 0.49 0.52 0.55 0.59 0.62 0.65 0.68 0.70 Proportion Explained 0.16 0.14 0.10 0.08 0.04 0.03 0.03 0.03 0.03 0.03 0.03 Cumulative Proportion 0.16 0.31 0.40 0.49 0.52 0.55 0.59 0.62 0.65 0.68 0.70 PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22 SS loadings 0.67 0.65 0.63 0.61 0.59 0.56 0.56 0.54 0.52 0.51 0.50 Proportion Var 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 Cumulative Var 0.73 0.76 0.78 0.81 0.83 0.85 0.87 0.90 0.92 0.94 0.96 Proportion Explained 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 Cumulative Proportion 0.73 0.76 0.78 0.81 0.83 0.85 0.87 0.90 0.92 0.94 0.96 PC23 PC24 PC25 SS loadings 0.45 0.39 0.25 Proportion Var 0.02 0.02 0.01 Cumulative Var 0.97 0.99 1.00 Proportion Explained 0.02 0.02 0.01 Cumulative Proportion 0.97 0.99 1.00 Mean item complexity = 5.7 Test of the hypothesis that 25 components are sufficient. The root mean square of the residuals (RMSR) is 0 with the empirical chi square 0 with prob &lt; NA Fit based upon off diagonal values = 1 The total variance for a particular variable will have two components: some of it will be share with other variables (common variance, h2) and some of it will be specific to that measure (unique variance, u2). Random variance is also specific to one item, but not reliably so. We can examine this most easily by examining the matrix (second screen). The columns PC1 thru PC25 are the (uninteresting at this point) unrotated loadings. PC stands for principal component. Although these dont align with the specific items, at this point in the procedure there are as many components as variables. Communalities are represented as \\(h^2\\). These are the proportions of common variance present in the variables. A variable that has no specific (or random) variance would have a communality of 1.0. If a variable shares none of its variance with any other variable its communality would be 0.0. Because we extracted the same number components as variables they all equal 1.0. That is we have explained all the variance in each variable. When we specify fewer components the value of the communalities will decrease. **Uniquenessess* are represented as \\(u2\\). These are the amount of unique variance for each variable. They are calculated as \\(1 - h^2\\) (or 1 minus the communality). Technically (at this point in the analysis where we have an equal number of components as items), they should all be zero, but the psych package is very quantsy and decimals are reported to the 15th and 16th decimal places! (hence the u2 for Q1 is -0.0000000000000028865799). The final column, com represents item complexity. This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one component If it is 2.0, it loads evenly on two components, and so forth. For now, we can ignore this. I mostly wanted to reassure you that com is not communality; h2 is communality. Lets switch to the first screen of output. Eigenvalues are displayed in the row called, SS loadings (i.e., the sum of squared loadings). They represent the variance explained by the particular linear component. PC1 explains 4.04 units of variance (out of a possible 25; the # of components). As a proportion, this is 4.04/25 = 0.16 (reported in the Proportion Var row). 4.04/25 [1] 0.1616 Note: Cumulative Var is helpful in determining how many components we would like to retain to balance parsimony (few as possible) with the amount of variance we want to explain The eigenvalues are in descending order. If we were to use the eigenvalue &gt; 1.0 (aka, Kaisers) criteria to determine how many components to extract, we would select 4. Joliffes critera was 0.7 (thus, we would select 10 components). Eigenvalues are only one criteria, lets look at he scree plot. Scree plot: We can gain another view of how many components to extract by creating a scree plot. Eigenvalues are stored in the pca1 objects variable, values. We can see all the values captured by this object with the names() function: names(pca1) [1] &quot;values&quot; &quot;rotation&quot; &quot;n.obs&quot; &quot;communality&quot; &quot;loadings&quot; [6] &quot;fit&quot; &quot;fit.off&quot; &quot;fn&quot; &quot;Call&quot; &quot;uniquenesses&quot; [11] &quot;complexity&quot; &quot;chi&quot; &quot;EPVAL&quot; &quot;R2&quot; &quot;objective&quot; [16] &quot;residual&quot; &quot;rms&quot; &quot;factors&quot; &quot;dof&quot; &quot;null.dof&quot; [21] &quot;null.model&quot; &quot;criteria&quot; &quot;STATISTIC&quot; &quot;PVAL&quot; &quot;weights&quot; [26] &quot;r.scores&quot; &quot;Vaccounted&quot; &quot;Structure&quot; &quot;scores&quot; Plotting the eigenvalues produces a scree plot. We can use this to further guage the number of factors we should extract. plot(pca1$values, type=&quot;b&quot;) #type = &quot;b&quot; gives us &quot;both&quot; lines and points; type = &quot;l&quot; gives lines and is relatively worthless We look for the point of inflexion. That is, where the baseline levels out into a plateau. There are four components above the plateau. 8.5.3 Specifying the Number of Components Below is a snip from the workflow to remind us where we are in the steps to PCA. Image of an excerpt from the workflow Having determined the number of components, we rerun the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville CITATION indicated that they (incuding Lewis and Neville CITATION) examined solutions with 2, 3, 4, and 5 factors (they did a parallel factor analysis; we are still examining components). #pca2 &lt;- psych::principal(GRMSmatrix, nfactors=4, rotate=&quot;none&quot;) pca2 &lt;- psych::principal(dfGRMS, nfactors=4, rotate=&quot;none&quot;) #can copy prior script, but change nfactors and object name pca2 Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = &quot;none&quot;) Standardized loadings (pattern matrix) based upon correlation matrix PC1 PC2 PC3 PC4 h2 u2 com Obj1 0.55 0.40 -0.17 0.02 0.50 0.50 2.1 Obj2 0.46 0.54 -0.15 0.14 0.55 0.45 2.2 Obj3 0.48 0.43 0.04 0.21 0.46 0.54 2.4 Obj4 0.50 0.42 -0.03 0.04 0.42 0.58 2.0 Obj5 0.33 0.57 0.13 0.02 0.44 0.56 1.7 Obj6 0.56 0.19 -0.09 0.15 0.38 0.62 1.4 Obj7 0.59 0.22 -0.09 -0.13 0.43 0.57 1.5 Obj8 0.38 0.40 0.06 -0.09 0.32 0.68 2.2 Obj9 0.40 0.28 0.21 -0.01 0.28 0.72 2.4 Obj10 0.40 0.28 0.12 -0.19 0.30 0.70 2.5 Marg1 0.63 -0.62 -0.13 -0.10 0.80 0.20 2.1 Marg2 0.47 -0.66 0.05 0.05 0.66 0.34 1.8 Marg3 0.50 -0.53 -0.11 0.02 0.54 0.46 2.1 Marg4 0.57 -0.42 -0.10 0.08 0.52 0.48 2.0 Marg5 0.58 -0.35 -0.17 0.08 0.49 0.51 1.9 Marg6 0.40 -0.44 0.07 -0.42 0.54 0.46 3.0 Marg7 0.23 -0.59 0.28 0.01 0.48 0.52 1.7 Strong1 0.04 0.02 0.65 -0.13 0.45 0.55 1.1 Strong2 0.04 0.13 0.53 -0.35 0.42 0.58 1.9 Strong3 0.09 0.21 0.41 -0.45 0.42 0.58 2.5 Strong4 0.02 -0.04 0.70 -0.15 0.51 0.49 1.1 Strong5 0.18 -0.06 0.34 -0.47 0.38 0.62 2.2 Angry1 0.19 0.07 0.30 0.70 0.62 0.38 1.5 Angry2 0.03 -0.11 0.55 0.55 0.61 0.39 2.1 Angry3 0.02 -0.21 0.49 0.59 0.63 0.37 2.2 PC1 PC2 PC3 PC4 SS loadings 4.04 3.61 2.41 2.09 Proportion Var 0.16 0.14 0.10 0.08 Cumulative Var 0.16 0.31 0.40 0.49 Proportion Explained 0.33 0.30 0.20 0.17 Cumulative Proportion 0.33 0.63 0.83 1.00 Mean item complexity = 2 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 360.58 with prob &lt; 0.00000000015 Fit based upon off diagonal values = 0.94 Our eigenvalues/SS loadings remain the same. With 4 components, we explain 49% of the variance (we can see this in the Cumulative Var row. Communality is the proportion of common variance within a variable. Principal components analysis assumes that all variance is common; therefore, before extraction, all variance was set at 1.0. Therefore, changing from 25 to 4 components will change this value (\\(h2\\)) as well as its associated uniqueness (\\(u2\\)), which is calculated as 1.0 minus the communality. The communalities (\\(h2\\)) and uniquenesses (\\(u2\\)) are changed. Now we see that 50% of the variance associate with Obj1 is common/shared (the \\(h2\\) value). Recall that we could represent this scale with all 25 items as components. But we want a more parsimonious explanation. By respecifying a smaller number of components, we lose some information. That is, the retained components (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 50%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction. We can examine the communalities through the lens of Kaisers criterion (the eigenvalue &gt; 1 criteria) to see if we think that four was a good number of components to extract. Kaisers criterion is believed to be accurate if: when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70 looking at our data, only 1 communality (Marg1) is &gt; .70, so, this does not support extracting four components when the sample size is greater than 250 (ours was 259) and the average communality is &gt; .60 we can extract the communalities from our object and calculate the mean the average communality Using the names() function again, we see that communality is available. Thus, we can easily calculate their mean. To get this value lets first examine the possible contents of the object we created from this PCA analysis by asking for its names. names(pca2) [1] &quot;values&quot; &quot;rotation&quot; &quot;n.obs&quot; &quot;communality&quot; &quot;loadings&quot; [6] &quot;fit&quot; &quot;fit.off&quot; &quot;fn&quot; &quot;Call&quot; &quot;uniquenesses&quot; [11] &quot;complexity&quot; &quot;chi&quot; &quot;EPVAL&quot; &quot;R2&quot; &quot;objective&quot; [16] &quot;residual&quot; &quot;rms&quot; &quot;factors&quot; &quot;dof&quot; &quot;null.dof&quot; [21] &quot;null.model&quot; &quot;criteria&quot; &quot;STATISTIC&quot; &quot;PVAL&quot; &quot;weights&quot; [26] &quot;r.scores&quot; &quot;Vaccounted&quot; &quot;Structure&quot; &quot;scores&quot; We see that it includes communalities. Thus, we can easily calculate their mean. mean(pca2$communality) [1] 0.4857967 #sum(pca2$communality) #checking my work by calculating the sum and dividing by 25 #12.14492/25 We see that the average communality is 0.48. These two criteria would suggest that we may not have the best solution. That said (in our defense): We used the scree plot as a guide and it was very clear. We have an adequate sample size and that was supported with the KMO. Are the number of components consistent with theory? We have not yet inspected the component loadings. This will provide us with more information. We could do several things: rerun with a different number of components (recall Lewis and Neville (2015) ran models with 2, 3, 4, and 5 factors) conduct more diagnostics reproduced correlation matrix the difference between the reproduced correlation matrix and the correlation matrix in the data The factor.model() function in psych produces the reproduced correlation matrix by using the loadings in our extracted object. Conceptually, this matrix is the correlations that should be produced if we did not have the raw data but we only had the component loadings. We could do fancy matrix algebra and produce these. The questions, though, is: How close did we get? How different is the reproduced correlation matrix from GRMSmatrix  the \\(R\\)-matrix produced from our raw data. round(psych::factor.model(pca2$loadings),3)#produces the reproduced correlation matrix Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 Obj1 0.495 0.501 0.435 0.447 0.386 0.403 0.428 0.359 0.294 0.311 Obj2 0.501 0.547 0.477 0.466 0.442 0.395 0.390 0.372 0.303 0.297 Obj3 0.435 0.477 0.460 0.426 0.408 0.380 0.349 0.339 0.318 0.281 Obj4 0.447 0.466 0.426 0.424 0.396 0.367 0.384 0.351 0.307 0.307 Obj5 0.386 0.442 0.408 0.396 0.444 0.282 0.306 0.358 0.315 0.307 Obj6 0.403 0.395 0.380 0.367 0.282 0.383 0.362 0.271 0.257 0.240 Obj7 0.428 0.390 0.349 0.384 0.306 0.362 0.426 0.322 0.278 0.317 Obj8 0.359 0.372 0.339 0.351 0.358 0.271 0.322 0.319 0.278 0.295 Obj9 0.294 0.303 0.318 0.307 0.315 0.257 0.278 0.278 0.281 0.267 Obj10 0.311 0.297 0.281 0.307 0.307 0.240 0.317 0.295 0.267 0.298 Marg1 0.116 -0.042 0.010 0.052 -0.162 0.232 0.258 -0.005 0.051 0.082 Marg2 -0.016 -0.143 -0.045 -0.043 -0.213 0.144 0.118 -0.086 0.012 -0.003 Marg3 0.081 -0.038 0.015 0.031 -0.148 0.195 0.184 -0.028 0.030 0.034 Marg4 0.165 0.062 0.109 0.115 -0.060 0.264 0.243 0.038 0.089 0.084 Marg5 0.208 0.112 0.137 0.149 -0.029 0.286 0.270 0.064 0.096 0.098 Marg6 0.020 -0.123 -0.082 -0.007 -0.114 0.070 0.187 0.022 0.054 0.128 Marg7 -0.164 -0.258 -0.133 -0.145 -0.226 -0.007 -0.028 -0.135 -0.017 -0.046 Strong1 -0.086 -0.081 0.028 0.003 0.110 -0.049 -0.016 0.078 0.161 0.129 Strong2 -0.028 -0.037 0.020 0.040 0.148 -0.055 0.047 0.132 0.163 0.185 Strong3 0.056 0.039 0.059 0.103 0.198 -0.011 0.124 0.189 0.186 0.237 Strong4 -0.131 -0.136 -0.013 -0.037 0.070 -0.080 -0.043 0.048 0.144 0.112 Strong5 0.003 -0.065 -0.026 0.031 0.060 -0.013 0.123 0.109 0.128 0.189 Angry1 0.094 0.175 0.278 0.143 0.152 0.200 0.005 0.054 0.154 -0.003 Angry2 -0.108 -0.048 0.106 -0.022 0.030 0.037 -0.129 -0.047 0.096 -0.056 Angry3 -0.147 -0.098 0.059 -0.069 -0.040 0.018 -0.161 -0.101 0.048 -0.106 Marg1 Marg2 Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Obj1 0.116 -0.016 0.081 0.165 0.208 0.020 -0.164 -0.086 -0.028 Obj2 -0.042 -0.143 -0.038 0.062 0.112 -0.123 -0.258 -0.081 -0.037 Obj3 0.010 -0.045 0.015 0.109 0.137 -0.082 -0.133 0.028 0.020 Obj4 0.052 -0.043 0.031 0.115 0.149 -0.007 -0.145 0.003 0.040 Obj5 -0.162 -0.213 -0.148 -0.060 -0.029 -0.114 -0.226 0.110 0.148 Obj6 0.232 0.144 0.195 0.264 0.286 0.070 -0.007 -0.049 -0.055 Obj7 0.258 0.118 0.184 0.243 0.270 0.187 -0.028 -0.016 0.047 Obj8 -0.005 -0.086 -0.028 0.038 0.064 0.022 -0.135 0.078 0.132 Obj9 0.051 0.012 0.030 0.089 0.096 0.054 -0.017 0.161 0.163 Obj10 0.082 -0.003 0.034 0.084 0.098 0.128 -0.046 0.129 0.185 Marg1 0.800 0.690 0.648 0.619 0.592 0.554 0.469 -0.062 -0.089 Marg2 0.690 0.664 0.579 0.544 0.498 0.458 0.512 0.027 -0.063 Marg3 0.648 0.579 0.538 0.517 0.492 0.412 0.396 -0.065 -0.114 Marg4 0.619 0.544 0.517 0.516 0.499 0.368 0.349 -0.063 -0.114 Marg5 0.592 0.498 0.492 0.499 0.491 0.338 0.290 -0.108 -0.142 Marg6 0.554 0.458 0.412 0.368 0.338 0.537 0.365 0.108 0.146 Marg7 0.469 0.512 0.396 0.349 0.290 0.365 0.481 0.175 0.073 Strong1 -0.062 0.027 -0.065 -0.063 -0.108 0.108 0.175 0.447 0.394 Strong2 -0.089 -0.063 -0.114 -0.114 -0.142 0.146 0.073 0.394 0.419 Strong3 -0.080 -0.102 -0.119 -0.113 -0.125 0.164 0.002 0.334 0.404 Strong4 -0.035 0.064 -0.043 -0.052 -0.105 0.142 0.223 0.476 0.415 Strong5 0.157 0.117 0.077 0.057 0.032 0.326 0.167 0.289 0.344 Angry1 -0.040 0.092 0.040 0.103 0.083 -0.232 0.092 0.118 -0.071 Angry2 -0.042 0.142 0.028 0.053 0.004 -0.133 0.229 0.286 0.081 Angry3 0.014 0.200 0.080 0.095 0.042 -0.115 0.270 0.240 0.024 Strong3 Strong4 Strong5 Angry1 Angry2 Angry3 Obj1 0.056 -0.131 0.003 0.094 -0.108 -0.147 Obj2 0.039 -0.136 -0.065 0.175 -0.048 -0.098 Obj3 0.059 -0.013 -0.026 0.278 0.106 0.059 Obj4 0.103 -0.037 0.031 0.143 -0.022 -0.069 Obj5 0.198 0.070 0.060 0.152 0.030 -0.040 Obj6 -0.011 -0.080 -0.013 0.200 0.037 0.018 Obj7 0.124 -0.043 0.123 0.005 -0.129 -0.161 Obj8 0.189 0.048 0.109 0.054 -0.047 -0.101 Obj9 0.186 0.144 0.128 0.154 0.096 0.048 Obj10 0.237 0.112 0.189 -0.003 -0.056 -0.106 Marg1 -0.080 -0.035 0.157 -0.040 -0.042 0.014 Marg2 -0.102 0.064 0.117 0.092 0.142 0.200 Marg3 -0.119 -0.043 0.077 0.040 0.028 0.080 Marg4 -0.113 -0.052 0.057 0.103 0.053 0.095 Marg5 -0.125 -0.105 0.032 0.083 0.004 0.042 Marg6 0.164 0.142 0.326 -0.232 -0.133 -0.115 Marg7 0.002 0.223 0.167 0.092 0.229 0.270 Strong1 0.334 0.476 0.289 0.118 0.286 0.240 Strong2 0.404 0.415 0.344 -0.071 0.081 0.024 Strong3 0.423 0.345 0.355 -0.158 -0.043 -0.106 Strong4 0.345 0.511 0.314 0.108 0.305 0.263 Strong5 0.355 0.314 0.377 -0.201 -0.063 -0.096 Angry1 -0.158 0.108 -0.201 0.624 0.551 0.548 Angry2 -0.043 0.305 -0.063 0.551 0.615 0.613 Angry3 -0.106 0.263 -0.096 0.548 0.613 0.626 Were not really interested in this matrix. We just need it to compare it to the GRMSmatrix to produce the residuals. We do that next. Residuals are the difference between the reproduced (i.e., those created from our component loadings) and \\(R\\)-matrix produced by the raw data. If we look at the \\(r_{_{Obj1Obj2}}\\) in our original correlation matrix (theoreticaly from the raw data [although we simulated data]), the value is 0.41 The reproduced correlation that we just calculated for this pair is 0.50. The diffference is -0.09. .41 - .50 [1] -0.09 By using the factor.residuals() function we can calculate the residuals. Here we will see this difference calculated for us, for all the elements in the matrix. round(psych::factor.residuals(GRMSmatrix, pca2$loadings), 3) Obj1 Obj2 Obj3 Obj4 Obj5 Obj6 Obj7 Obj8 Obj9 Obj10 Obj1 0.505 -0.093 -0.052 -0.118 -0.047 -0.067 -0.062 -0.010 -0.039 -0.070 Obj2 -0.093 0.453 -0.047 -0.066 -0.045 -0.055 -0.038 -0.079 -0.026 -0.060 Obj3 -0.052 -0.047 0.540 -0.068 -0.067 -0.068 -0.049 -0.059 -0.102 -0.045 Obj4 -0.118 -0.066 -0.068 0.576 -0.074 -0.046 -0.059 -0.097 -0.038 -0.030 Obj5 -0.047 -0.045 -0.067 -0.074 0.556 -0.054 -0.050 -0.097 -0.049 -0.060 Obj6 -0.067 -0.055 -0.068 -0.046 -0.054 0.617 -0.057 -0.033 -0.064 -0.077 Obj7 -0.062 -0.038 -0.049 -0.059 -0.050 -0.057 0.574 -0.067 -0.086 -0.056 Obj8 -0.010 -0.079 -0.059 -0.097 -0.097 -0.033 -0.067 0.681 -0.070 -0.070 Obj9 -0.039 -0.026 -0.102 -0.038 -0.049 -0.064 -0.086 -0.070 0.719 -0.047 Obj10 -0.070 -0.060 -0.045 -0.030 -0.060 -0.077 -0.056 -0.070 -0.047 0.702 Marg1 -0.011 0.017 0.002 0.005 0.018 -0.030 -0.027 0.004 0.015 -0.018 Marg2 0.016 0.016 0.011 0.014 0.030 0.011 -0.012 0.001 -0.002 -0.024 Marg3 -0.026 0.025 0.003 0.011 0.039 -0.034 -0.060 0.001 0.036 -0.010 Marg4 0.002 -0.013 -0.005 -0.044 0.028 -0.054 -0.028 0.014 -0.044 -0.018 Marg5 -0.012 -0.039 -0.029 -0.023 0.032 -0.078 -0.045 -0.019 0.009 0.027 Marg6 -0.018 0.035 0.020 0.032 0.007 -0.007 -0.028 0.004 -0.003 0.005 Marg7 0.020 0.036 0.043 0.014 0.054 0.008 0.016 0.031 -0.023 0.006 Strong1 0.040 0.015 0.002 -0.026 -0.001 -0.005 0.021 -0.043 -0.019 -0.030 Strong2 0.034 -0.023 -0.008 0.001 -0.020 0.042 -0.008 -0.001 -0.044 -0.056 Strong3 -0.015 0.015 0.011 0.038 -0.053 0.032 -0.019 -0.085 -0.056 -0.095 Strong4 0.026 0.031 0.021 -0.003 -0.008 0.038 -0.005 -0.003 -0.038 -0.025 Strong5 0.019 0.062 0.021 -0.020 -0.022 0.014 -0.020 0.002 -0.074 -0.080 Angry1 0.015 -0.026 -0.077 0.006 -0.045 -0.046 0.006 0.022 -0.072 0.028 Angry2 -0.007 0.012 -0.030 -0.002 -0.019 -0.031 0.061 0.040 -0.030 0.011 Angry3 0.023 0.006 -0.027 0.008 -0.010 -0.001 0.040 -0.008 0.001 0.040 Marg1 Marg2 Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Obj1 -0.011 0.016 -0.026 0.002 -0.012 -0.018 0.020 0.040 0.034 Obj2 0.017 0.016 0.025 -0.013 -0.039 0.035 0.036 0.015 -0.023 Obj3 0.002 0.011 0.003 -0.005 -0.029 0.020 0.043 0.002 -0.008 Obj4 0.005 0.014 0.011 -0.044 -0.023 0.032 0.014 -0.026 0.001 Obj5 0.018 0.030 0.039 0.028 0.032 0.007 0.054 -0.001 -0.020 Obj6 -0.030 0.011 -0.034 -0.054 -0.078 -0.007 0.008 -0.005 0.042 Obj7 -0.027 -0.012 -0.060 -0.028 -0.045 -0.028 0.016 0.021 -0.008 Obj8 0.004 0.001 0.001 0.014 -0.019 0.004 0.031 -0.043 -0.001 Obj9 0.015 -0.002 0.036 -0.044 0.009 -0.003 -0.023 -0.019 -0.044 Obj10 -0.018 -0.024 -0.010 -0.018 0.027 0.005 0.006 -0.030 -0.056 Marg1 0.200 -0.026 -0.033 -0.041 -0.044 -0.041 -0.049 0.020 -0.006 Marg2 -0.026 0.336 -0.077 -0.070 -0.090 -0.026 -0.092 0.020 0.002 Marg3 -0.033 -0.077 0.462 -0.101 -0.091 -0.066 -0.058 0.004 0.019 Marg4 -0.041 -0.070 -0.101 0.484 -0.055 -0.087 -0.074 0.038 0.032 Marg5 -0.044 -0.090 -0.091 -0.055 0.509 -0.093 -0.031 0.010 0.066 Marg6 -0.041 -0.026 -0.066 -0.087 -0.093 0.463 -0.074 -0.007 -0.056 Marg7 -0.049 -0.092 -0.058 -0.074 -0.031 -0.074 0.519 -0.085 0.006 Strong1 0.020 0.020 0.004 0.038 0.010 -0.007 -0.085 0.553 -0.133 Strong2 -0.006 0.002 0.019 0.032 0.066 -0.056 0.006 -0.133 0.581 Strong3 0.022 0.013 0.033 0.034 0.024 -0.062 -0.016 -0.090 -0.135 Strong4 -0.005 -0.012 0.019 0.015 0.040 -0.037 -0.032 -0.133 -0.100 Strong5 -0.013 -0.020 -0.011 0.001 -0.020 -0.096 -0.032 -0.096 -0.130 Angry1 0.003 -0.029 0.009 -0.023 -0.013 0.076 -0.014 -0.030 0.021 Angry2 0.009 -0.029 -0.005 0.006 -0.010 0.053 -0.044 -0.071 -0.031 Angry3 0.007 0.006 -0.027 -0.026 -0.004 0.029 -0.070 -0.061 0.009 Strong3 Strong4 Strong5 Angry1 Angry2 Angry3 Obj1 -0.015 0.026 0.019 0.015 -0.007 0.023 Obj2 0.015 0.031 0.062 -0.026 0.012 0.006 Obj3 0.011 0.021 0.021 -0.077 -0.030 -0.027 Obj4 0.038 -0.003 -0.020 0.006 -0.002 0.008 Obj5 -0.053 -0.008 -0.022 -0.045 -0.019 -0.010 Obj6 0.032 0.038 0.014 -0.046 -0.031 -0.001 Obj7 -0.019 -0.005 -0.020 0.006 0.061 0.040 Obj8 -0.085 -0.003 0.002 0.022 0.040 -0.008 Obj9 -0.056 -0.038 -0.074 -0.072 -0.030 0.001 Obj10 -0.095 -0.025 -0.080 0.028 0.011 0.040 Marg1 0.022 -0.005 -0.013 0.003 0.009 0.007 Marg2 0.013 -0.012 -0.020 -0.029 -0.029 0.006 Marg3 0.033 0.019 -0.011 0.009 -0.005 -0.027 Marg4 0.034 0.015 0.001 -0.023 0.006 -0.026 Marg5 0.024 0.040 -0.020 -0.013 -0.010 -0.004 Marg6 -0.062 -0.037 -0.096 0.076 0.053 0.029 Marg7 -0.016 -0.032 -0.032 -0.014 -0.044 -0.070 Strong1 -0.090 -0.133 -0.096 -0.030 -0.071 -0.061 Strong2 -0.135 -0.100 -0.130 0.021 -0.031 0.009 Strong3 0.577 -0.096 -0.087 0.056 0.024 0.040 Strong4 -0.096 0.489 -0.097 -0.047 -0.069 -0.064 Strong5 -0.087 -0.097 0.623 0.092 0.041 0.044 Angry1 0.056 -0.047 0.092 0.376 -0.108 -0.115 Angry2 0.024 -0.069 0.041 -0.108 0.385 -0.147 Angry3 0.040 -0.064 0.044 -0.115 -0.147 0.374 There are several strategies to evaluate this matrix: see how large the residuals are compared to the original correlations the worst possible model would occur if we extracted no components and would be the size of the original correlations if the correlations were small to start with, we expect small residuals if the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic) comparing residuals requires squaring them first (because residuals can be both positive and negative) the sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit. Subtracting this from 1.0 means that it ranges from 0 to 1. Values &gt; .95 are an indication of good fit. Analyzing the residuals means we need to extract only the upper right of the triangle them into an object. We can do this in steps. pca2_resids &lt;- psych::factor.residuals(GRMSmatrix, pca2$loadings)#first extract the resids pca2_resids &lt;- as.matrix(pca2_resids[upper.tri(pca2_resids)])#the object has the residuals in a single column head(pca2_resids) [,1] [1,] -0.09259539 [2,] -0.05244358 [3,] -0.04745227 [4,] -0.11845843 [5,] -0.06581435 [6,] -0.06842963 One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is &gt; |0.05| and false if it is smaller. The sum function will tell us how many TRUE responses are in the matrix. Further, we can write script to obtain the proportion of total number of residuals. large.resid &lt;- abs(pca2_resids) &gt; 0.05 #large.resid sum(large.resid) [1] 85 round(sum(large.resid)/nrow(pca2_resids),3) [1] 0.283 We learn that there are 85 residuals greater than the absolute value of 0.05. This represents 28% of the total number of residuals. There are no hard rules about what proportion of residuals can be greater than 0.05. A common practice is to stay below 50% (Field, 2012). Another approach to analyzing residuals is to look at their mean. Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root. round(sqrt(mean(pca2_resids^2)),3) [1] 0.048 While there are no clear guidelines to interpret these, one recommendation is to consider extracting more components if the value is higher than 0.08 (Field, 2012). Finally, we expect our residuals to be normally distributed. A histogram can help us inspect the distribution. hist(pca2_resids) Not bad! Looks reasonably normal. No outliers. 8.5.3.1 Quick recap of how to evaluate the # of components we extracted If fewer than 30 variables, the eigenvalue &gt; 1 (Kaisers) critera is fine, so long as communalities are all &gt; .70. If sample size &gt; 250 and the average communalitie are .6 or greater, this is fine. When N &gt; 200, the scree plot can be used. Regarding residuals fewer than 50% should have absolute values &gt; 0.05 model fit should be &gt; 0.90 8.5.4 Component Rotation Below is a snip from the workflow to remind us where we are in the steps to PCA. Image of an excerpt from the workflow Rotation improves the interpretation of the components by maximizing the loading on each variable on one of the extracted components while minimizing the loading on all other components. Rotation works by changing the absolute values of the variables while keeping their differential values constant. There are two big choices(to be made on theoretical grounds): Orthogonal rotation if you think that the components are independent/unrelated. most common orthogonal rotation is varimax Oblique rotation if you think that the components are related correlated. oblimin and promax are common oblique rotations Which to do? Orthogonal is easy because it minimizes cross-loadings, but Can you think of a measure where the subscales would not be correlated? 8.5.4.1 Orthogonal rotation #pcaORTH &lt;- psych::principal(GRMSmatrix, nfactors = 4, rotate = &quot;varimax&quot;) pcaORTH &lt;- psych::principal(dfGRMS, nfactors = 4, rotate = &quot;varimax&quot;) pcaORTH Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix RC1 RC2 RC3 RC4 h2 u2 com Obj1 0.69 0.08 -0.10 -0.09 0.50 0.50 1.1 Obj2 0.72 -0.10 -0.14 0.00 0.55 0.45 1.1 Obj3 0.65 -0.01 -0.04 0.17 0.46 0.54 1.1 Obj4 0.65 0.02 0.00 0.00 0.42 0.58 1.0 Obj5 0.61 -0.21 0.14 0.05 0.44 0.56 1.4 Obj6 0.56 0.23 -0.11 0.08 0.38 0.62 1.5 Obj7 0.58 0.24 0.05 -0.16 0.43 0.57 1.5 Obj8 0.54 -0.04 0.15 -0.06 0.32 0.68 1.2 Obj9 0.47 0.04 0.22 0.11 0.28 0.72 1.5 Obj10 0.47 0.06 0.25 -0.10 0.30 0.70 1.7 Marg1 0.06 0.89 -0.02 -0.08 0.80 0.20 1.0 Marg2 -0.09 0.80 0.02 0.14 0.66 0.34 1.1 Marg3 0.03 0.73 -0.08 0.02 0.54 0.46 1.0 Marg4 0.16 0.69 -0.10 0.07 0.52 0.48 1.2 Marg5 0.21 0.65 -0.15 0.01 0.49 0.51 1.3 Marg6 -0.03 0.61 0.32 -0.25 0.54 0.46 1.9 Marg7 -0.24 0.57 0.22 0.23 0.48 0.52 2.0 Strong1 0.00 -0.03 0.61 0.27 0.45 0.55 1.4 Strong2 0.06 -0.09 0.64 0.00 0.42 0.58 1.1 Strong3 0.16 -0.10 0.60 -0.15 0.42 0.58 1.3 Strong4 -0.06 0.01 0.66 0.28 0.51 0.49 1.4 Strong5 0.04 0.17 0.56 -0.18 0.38 0.62 1.4 Angry1 0.22 0.01 -0.13 0.75 0.62 0.38 1.2 Angry2 -0.03 0.03 0.13 0.77 0.61 0.39 1.1 Angry3 -0.11 0.10 0.06 0.77 0.63 0.37 1.1 RC1 RC2 RC3 RC4 SS loadings 3.84 3.79 2.32 2.20 Proportion Var 0.15 0.15 0.09 0.09 Cumulative Var 0.15 0.31 0.40 0.49 Proportion Explained 0.32 0.31 0.19 0.18 Cumulative Proportion 0.32 0.63 0.82 1.00 Mean item complexity = 1.3 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 360.58 with prob &lt; 0.00000000015 Fit based upon off diagonal values = 0.94 Essentially we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the component loadings while keeping their differential vales constant). Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance will remain the same. The print.psych() function facilitates interpretation and prioritizes the information about which we care most: cut will display loadings above .3 if some items load on no factors if some items have cross-loadings (and their relative weights) sort will reorder the loadings to make it clearer (to the best of its abilityin the case of ties) to which component/scale it belongs pca_table &lt;- psych::print.psych(pcaORTH, cut = 0.3, sort=TRUE) Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = &quot;varimax&quot;) Standardized loadings (pattern matrix) based upon correlation matrix item RC1 RC2 RC3 RC4 h2 u2 com Obj2 2 0.72 0.55 0.45 1.1 Obj1 1 0.69 0.50 0.50 1.1 Obj3 3 0.65 0.46 0.54 1.1 Obj4 4 0.65 0.42 0.58 1.0 Obj5 5 0.61 0.44 0.56 1.4 Obj7 7 0.58 0.43 0.57 1.5 Obj6 6 0.56 0.38 0.62 1.5 Obj8 8 0.54 0.32 0.68 1.2 Obj9 9 0.47 0.28 0.72 1.5 Obj10 10 0.47 0.30 0.70 1.7 Marg1 11 0.89 0.80 0.20 1.0 Marg2 12 0.80 0.66 0.34 1.1 Marg3 13 0.73 0.54 0.46 1.0 Marg4 14 0.69 0.52 0.48 1.2 Marg5 15 0.65 0.49 0.51 1.3 Marg6 16 0.61 0.32 0.54 0.46 1.9 Marg7 17 0.57 0.48 0.52 2.0 Strong4 21 0.66 0.51 0.49 1.4 Strong2 19 0.64 0.42 0.58 1.1 Strong1 18 0.61 0.45 0.55 1.4 Strong3 20 0.60 0.42 0.58 1.3 Strong5 22 0.56 0.38 0.62 1.4 Angry3 25 0.77 0.63 0.37 1.1 Angry2 24 0.77 0.61 0.39 1.1 Angry1 23 0.75 0.62 0.38 1.2 RC1 RC2 RC3 RC4 SS loadings 3.84 3.79 2.32 2.20 Proportion Var 0.15 0.15 0.09 0.09 Cumulative Var 0.15 0.31 0.40 0.49 Proportion Explained 0.32 0.31 0.19 0.18 Cumulative Proportion 0.32 0.63 0.82 1.00 Mean item complexity = 1.3 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 360.58 with prob &lt; 0.00000000015 Fit based upon off diagonal values = 0.94 In the unrotated solution, most variables loaded on the first component. After rotation, there are four clear components/scales. Further, there is clear (or at least reasonable) component/scale membership for each item. The item Marg6 was the only one that included cross-loadings. However, the difference was so great (.61 on its primary factor; .32 on the secondary factor) that the items membership is clearly on the second component. If this were a new scale and we had not yet established ideas for subscales, the next step is to look back at the items, themselves, and try to name the scales/components. If our scale construction included a priori/planned subscales, heres where we hope the items fall where they were hypothesized to do so. Our simulated data worked perfectly and replicated the four scales that Lewis and Neville CITATION reported in the article. Assumptions of Beauty and Sexual Objectification Silenced and Marginalized Strong Woman Stereotype Angry Woman Stereotype We can also create a figure of the result. psych::fa.diagram(pcaORTH) We can extract the component loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation. names(pcaORTH) [1] &quot;values&quot; &quot;rotation&quot; &quot;n.obs&quot; &quot;communality&quot; &quot;loadings&quot; [6] &quot;fit&quot; &quot;fit.off&quot; &quot;fn&quot; &quot;Call&quot; &quot;uniquenesses&quot; [11] &quot;complexity&quot; &quot;chi&quot; &quot;EPVAL&quot; &quot;R2&quot; &quot;objective&quot; [16] &quot;residual&quot; &quot;rms&quot; &quot;factors&quot; &quot;dof&quot; &quot;null.dof&quot; [21] &quot;null.model&quot; &quot;criteria&quot; &quot;STATISTIC&quot; &quot;PVAL&quot; &quot;weights&quot; [26] &quot;r.scores&quot; &quot;rot.mat&quot; &quot;Vaccounted&quot; &quot;Structure&quot; &quot;scores&quot; pcaORTH_table &lt;- round(pcaORTH$loadings,3) write.table(pcaORTH_table, file=&quot;pcaORTH_table.csv&quot;, sep=&quot;,&quot;, col.names=TRUE, row.names=FALSE) pcaORTH_table Loadings: RC1 RC2 RC3 RC4 Obj1 0.686 -0.100 Obj2 0.720 -0.142 Obj3 0.654 0.174 Obj4 0.651 Obj5 0.614 -0.212 0.142 Obj6 0.558 0.231 -0.109 Obj7 0.583 0.241 -0.159 Obj8 0.541 0.147 Obj9 0.470 0.216 0.107 Obj10 0.470 0.253 Marg1 0.888 Marg2 0.797 0.145 Marg3 0.728 Marg4 0.158 0.691 Marg5 0.211 0.651 -0.153 Marg6 0.609 0.317 -0.253 Marg7 -0.240 0.570 0.217 0.226 Strong1 0.613 0.266 Strong2 0.638 Strong3 0.160 0.604 -0.150 Strong4 0.656 0.278 Strong5 0.171 0.559 -0.184 Angry1 0.219 -0.133 0.747 Angry2 0.134 0.771 Angry3 -0.110 0.775 RC1 RC2 RC3 RC4 SS loadings 3.839 3.788 2.316 2.204 Proportion Var 0.154 0.152 0.093 0.088 Cumulative Var 0.154 0.305 0.398 0.486 8.5.4.2 Oblique rotation Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the components, an oblique rotation will allow them to be correlated. Researchers often explore both solutions, but then report only one. #pcaOBL &lt;- psych::principal(GRMSmatrix, nfactors = 4, rotate = &quot;oblimin&quot;) pcaOBL &lt;- psych::principal(dfGRMS, nfactors = 4, rotate = &quot;oblimin&quot;) Loading required namespace: GPArotation pcaOBL Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix TC1 TC2 TC3 TC4 h2 u2 com Obj1 0.68 0.08 -0.07 -0.08 0.50 0.50 1.1 Obj2 0.72 -0.09 -0.11 0.02 0.55 0.45 1.1 Obj3 0.66 -0.02 -0.01 0.19 0.46 0.54 1.2 Obj4 0.65 0.02 0.03 0.01 0.42 0.58 1.0 Obj5 0.62 -0.21 0.17 0.06 0.44 0.56 1.4 Obj6 0.55 0.23 -0.09 0.10 0.38 0.62 1.5 Obj7 0.58 0.25 0.07 -0.15 0.43 0.57 1.5 Obj8 0.54 -0.04 0.17 -0.06 0.32 0.68 1.2 Obj9 0.47 0.04 0.23 0.11 0.28 0.72 1.6 Obj10 0.46 0.06 0.27 -0.10 0.30 0.70 1.8 Marg1 0.03 0.89 -0.03 -0.08 0.80 0.20 1.0 Marg2 -0.11 0.79 0.01 0.14 0.66 0.34 1.1 Marg3 0.01 0.73 -0.08 0.03 0.54 0.46 1.0 Marg4 0.14 0.69 -0.10 0.07 0.52 0.48 1.1 Marg5 0.20 0.65 -0.15 0.02 0.49 0.51 1.3 Marg6 -0.05 0.61 0.31 -0.26 0.54 0.46 1.9 Marg7 -0.26 0.56 0.20 0.21 0.48 0.52 2.0 Strong1 0.00 -0.04 0.61 0.24 0.45 0.55 1.3 Strong2 0.06 -0.10 0.64 -0.02 0.42 0.58 1.1 Strong3 0.15 -0.10 0.61 -0.17 0.42 0.58 1.3 Strong4 -0.06 -0.01 0.65 0.25 0.51 0.49 1.3 Strong5 0.03 0.17 0.56 -0.20 0.38 0.62 1.5 Angry1 0.23 -0.01 -0.12 0.76 0.62 0.38 1.2 Angry2 -0.03 0.01 0.13 0.77 0.61 0.39 1.1 Angry3 -0.11 0.08 0.06 0.77 0.63 0.37 1.1 TC1 TC2 TC3 TC4 SS loadings 3.82 3.79 2.33 2.20 Proportion Var 0.15 0.15 0.09 0.09 Cumulative Var 0.15 0.30 0.40 0.49 Proportion Explained 0.31 0.31 0.19 0.18 Cumulative Proportion 0.31 0.63 0.82 1.00 With component correlations of TC1 TC2 TC3 TC4 TC1 1.00 0.02 -0.03 -0.02 TC2 0.02 1.00 0.02 0.03 TC3 -0.03 0.02 1.00 0.03 TC4 -0.02 0.03 0.03 1.00 Mean item complexity = 1.3 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 360.58 with prob &lt; 0.00000000015 Fit based upon off diagonal values = 0.94 We can make it a little easier to interpret by removing all factor loadings below .30. psych::print.psych(pcaOBL, cut = 0.3, sort=TRUE) Principal Components Analysis Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = &quot;oblimin&quot;) Standardized loadings (pattern matrix) based upon correlation matrix item TC1 TC2 TC3 TC4 h2 u2 com Obj2 2 0.72 0.55 0.45 1.1 Obj1 1 0.68 0.50 0.50 1.1 Obj3 3 0.66 0.46 0.54 1.2 Obj4 4 0.65 0.42 0.58 1.0 Obj5 5 0.62 0.44 0.56 1.4 Obj7 7 0.58 0.43 0.57 1.5 Obj6 6 0.55 0.38 0.62 1.5 Obj8 8 0.54 0.32 0.68 1.2 Obj9 9 0.47 0.28 0.72 1.6 Obj10 10 0.46 0.30 0.70 1.8 Marg1 11 0.89 0.80 0.20 1.0 Marg2 12 0.79 0.66 0.34 1.1 Marg3 13 0.73 0.54 0.46 1.0 Marg4 14 0.69 0.52 0.48 1.1 Marg5 15 0.65 0.49 0.51 1.3 Marg6 16 0.61 0.31 0.54 0.46 1.9 Marg7 17 0.56 0.48 0.52 2.0 Strong4 21 0.65 0.51 0.49 1.3 Strong2 19 0.64 0.42 0.58 1.1 Strong1 18 0.61 0.45 0.55 1.3 Strong3 20 0.61 0.42 0.58 1.3 Strong5 22 0.56 0.38 0.62 1.5 Angry3 25 0.77 0.63 0.37 1.1 Angry2 24 0.77 0.61 0.39 1.1 Angry1 23 0.76 0.62 0.38 1.2 TC1 TC2 TC3 TC4 SS loadings 3.82 3.79 2.33 2.20 Proportion Var 0.15 0.15 0.09 0.09 Cumulative Var 0.15 0.30 0.40 0.49 Proportion Explained 0.31 0.31 0.19 0.18 Cumulative Proportion 0.31 0.63 0.82 1.00 With component correlations of TC1 TC2 TC3 TC4 TC1 1.00 0.02 -0.03 -0.02 TC2 0.02 1.00 0.02 0.03 TC3 -0.03 0.02 1.00 0.03 TC4 -0.02 0.03 0.03 1.00 Mean item complexity = 1.3 Test of the hypothesis that 4 components are sufficient. The root mean square of the residuals (RMSR) is 0.05 with the empirical chi square 360.58 with prob &lt; 0.00000000015 Fit based upon off diagonal values = 0.94 All of the items stayed in their respective components. Note, though, that because our specification included sort=TRUE that the relative weights wiggled around and so the items are listed in a different order than in the orthogonal rotation. Lets create a table and write it to our file. pcaOBL_table &lt;- round(pcaOBL$loadings,3) write.table(pcaOBL_table, file=&quot;pcaOBL_table.csv&quot;, sep=&quot;,&quot;, col.names=TRUE, row.names=FALSE) pcaOBL_table Loadings: TC1 TC2 TC3 TC4 Obj1 0.685 Obj2 0.724 -0.112 Obj3 0.657 0.185 Obj4 0.650 Obj5 0.618 -0.213 0.169 Obj6 0.554 0.232 Obj7 0.576 0.246 -0.151 Obj8 0.540 0.169 Obj9 0.467 0.235 0.107 Obj10 0.465 0.272 Marg1 0.891 Marg2 -0.108 0.793 0.143 Marg3 0.729 Marg4 0.142 0.691 Marg5 0.197 0.653 -0.149 Marg6 0.612 0.311 -0.265 Marg7 -0.255 0.561 0.204 0.215 Strong1 0.614 0.245 Strong2 0.641 Strong3 0.154 -0.102 0.611 -0.169 Strong4 0.654 0.254 Strong5 0.169 0.559 -0.203 Angry1 0.226 -0.123 0.756 Angry2 0.134 0.766 Angry3 -0.108 0.771 TC1 TC2 TC3 TC4 SS loadings 3.829 3.786 2.334 2.198 Proportion Var 0.153 0.151 0.093 0.088 Cumulative Var 0.153 0.305 0.398 0.486 The same four components/scales seemed have emerged, but they are in different order. The oblique rotation allows us to see the correlation between the components/scales. This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/components are uncorrelated; hence in the analysis they were fixed to 0.0. We can see that all the scales have almost no relation with each other. That is the the correlations range between -0.03 to 0.03. This is unusual and likely a biproduct of simulating data. It does, though, support the orthogonal rotation as the preferred one. Of course there is always a little complexity. In oblique rotations, there is a distinction between the pattern matrix (which reports component loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the structure matrix (takes into account the relationship between the components/scales  it is a product of the pattern matrix and the matrix containing the correlation coefficients between the components/scales). Most interpret the pattern matrix because it is simpler; however it could be that values in the pattern matrix are suppressed because of relations between the components. Therefore, the structure matrix can be a useful check and some editors will request it. Obtaining the structure matrix requires two steps. First, multiply the factor loadings with the phi matrix. #names(pcaOBL) pcaOBL$loadings %*% pcaOBL$Phi TC1 TC2 TC3 TC4 Obj1 0.69050128 0.092733819 -0.09342837 -0.096688897 Obj2 0.72526181 -0.078285639 -0.13389319 -0.007970087 Obj3 0.65222185 0.002163388 -0.02080291 0.169195306 Obj4 0.64980993 0.034592152 0.01021951 -0.003172452 Obj5 0.60724873 -0.195994047 0.14916017 0.040795794 Obj6 0.55920407 0.244637871 -0.09553755 0.084850366 Obj7 0.58233119 0.255604967 0.05567917 -0.156139162 Obj8 0.53550142 -0.026606024 0.15167765 -0.064536944 Obj9 0.45868906 0.055743636 0.22592591 0.104498059 Obj10 0.46065458 0.072992922 0.25626058 -0.099641255 Marg1 0.05429308 0.889387150 -0.01272292 -0.060136145 Marg2 -0.09540568 0.794155634 0.03439142 0.165860367 Marg3 0.02785178 0.728248568 -0.06944307 0.041476497 Marg4 0.15779653 0.693656404 -0.08609067 0.083060621 Marg5 0.21422529 0.654667296 -0.14130873 0.030721117 Marg6 -0.03873213 0.610566300 0.31476620 -0.237541460 Marg7 -0.25458376 0.564865999 0.22877602 0.242123251 Strong1 -0.02493831 -0.026459940 0.62139121 0.264583641 Strong2 0.03727021 -0.083266239 0.63714719 -0.002354881 Strong3 0.13898024 -0.091205429 0.59914813 -0.154079964 Strong4 -0.08957417 0.008131805 0.66462011 0.277765084 Strong5 0.02206318 0.175205559 0.55465506 -0.180203186 Angry1 0.21157554 0.012544720 -0.10378683 0.745823358 Angry2 -0.05377743 0.030179345 0.16057125 0.771738772 Angry3 -0.12591995 0.094122499 0.08785893 0.777830061 Then use Fields (2012) function to produce the matrix. #Field&#39;s function to produce the structure matrix factor.structure &lt;- function(fa, cut = 0.2, decimals = 2){ structure.matrix &lt;- psych::fa.sort(fa$loadings %*% fa$Phi) structure.matrix &lt;- data.frame(ifelse(abs(structure.matrix) &lt; cut, &quot;&quot;, round(structure.matrix, decimals))) return(structure.matrix) } factor.structure(pcaOBL, cut = 0.3) TC1 TC2 TC3 TC4 Obj2 0.73 Obj1 0.69 Obj3 0.65 Obj4 0.65 Obj5 0.61 Obj7 0.58 Obj6 0.56 Obj8 0.54 Obj10 0.46 Obj9 0.46 Marg1 0.89 Marg2 0.79 Marg3 0.73 Marg4 0.69 Marg5 0.65 Marg6 0.61 0.31 Marg7 0.56 Strong4 0.66 Strong2 0.64 Strong1 0.62 Strong3 0.6 Strong5 0.55 Angry3 0.78 Angry2 0.77 Angry1 0.75 Although some of the relative values changed, our items were stable regarding their component membership. 8.5.5 Component Scores Component scores (PC scores) can be created for each case (row) on each component (column). These can be used to assess the relative standing of one person on the construct/variable to another. We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicollinearity. Computation involves multiplying an individuals item-level responses by the component loadings we obtained through the PCA process. The results will be one score per component for each row/case. pcaOBL &lt;- psych::principal(dfGRMS, nfactors=4, rotate=&quot;oblimin&quot;, scores=TRUE) head(pcaOBL$scores, 10) #shows us only the first 10 (of N = 2571) TC1 TC2 TC3 TC4 [1,] -0.434736430 1.2702286 0.83668623 -0.01950134 [2,] -0.358346641 0.6998443 -1.28684407 -0.10546432 [3,] -1.245491638 -0.1908764 -1.42929276 -2.51420936 [4,] -0.004981576 -0.1127983 -1.38956225 1.86355065 [5,] 0.296789107 0.5137755 0.08833053 -2.09774692 [6,] 0.684949888 1.6400602 0.44203340 -0.27258969 [7,] -0.771083947 -1.1922357 1.09617794 0.36910174 [8,] -1.284265060 -0.4144262 0.08708796 -1.07524322 [9,] 0.666522142 0.2999858 1.81030433 1.46348684 [10,] 0.292763466 -0.3285169 0.30101493 0.23847094 dfGRMS &lt;- cbind(dfGRMS, pcaOBL$scores) #adds them to our raw dataset To bring this full circle, we can see the correlation of the component scores; the pattern maps onto what we saw previously. psych::corr.test(dfGRMS [c(&quot;TC1&quot;, &quot;TC4&quot;, &quot;TC3&quot;, &quot;TC2&quot;)]) Call:psych::corr.test(x = dfGRMS[c(&quot;TC1&quot;, &quot;TC4&quot;, &quot;TC3&quot;, &quot;TC2&quot;)]) Correlation matrix TC1 TC4 TC3 TC2 TC1 1.00 -0.02 -0.03 0.02 TC4 -0.02 1.00 0.03 0.03 TC3 -0.03 0.03 1.00 0.02 TC2 0.02 0.03 0.02 1.00 Sample Size [1] 259 Probability values (Entries above the diagonal are adjusted for multiple tests.) TC1 TC4 TC3 TC2 TC1 0.00 1.00 1.00 1 TC4 0.70 0.00 1.00 1 TC3 0.65 0.59 0.00 1 TC2 0.74 0.69 0.76 0 To see confidence intervals of the correlations, print with the short=FALSE option psych::fa.diagram (pcaOBL, error=TRUE, side=3) 8.6 APA Style Results Results The dimensionality of the 25 items from the Gendered Racial Microagressions Scale for Black Women was analyzed using principal components analysis. First, data were screened to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barletts Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartletts test is &lt; .05, we are fairly certain we have clusters of correlated variables. In our dataset, \\(\\chi ^{1}(300)=1683.76, p &lt; .001\\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis. Four criteria were used to determine the number of components to extract: a priori theory, the scree test, the eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaisers eigenvalue-greater-than-one criteria suggested four components, and, in combination explained 49% of the variance. The inflexion in the scree plot justified retaining four components. Based on the convergence of these decisions, four components were extracted. We investigated each with orthogonal (varimax) and oblique (oblimin) procedures. Given the non-significant correlations (ranging from -0.03 to 0.03) and the clear component loadings in the orthogonal rotation, we determined that an orthogonal solution was most appropriate. The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable components, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (15%), silenced and marginalized (15%), strong woman stereotype (9%), and angry woman stereotype (15%). Regarding the Table 1, I would include a table with ALL the values, bolding those with component membership. This is easy, though, because it is how the table was exported when we wrote it to a .csv file. 8.7 Back to the FutuRe: The relationship between PCA and item analysis I included the lesson on item analysis because I find it to be a useful stepping stone into principal components and principal factor analyses. How do the results we obtained from PCA compare to those found in item analysis? First, we score the total and subscales using the dataset we simulated above (dfGRMS). library(tidyverse) Warning: package &#39;tidyverse&#39; was built under R version 4.0.5 -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- v ggplot2 3.3.5 v purrr 0.3.4 v tibble 3.1.4 v dplyr 1.0.7 v tidyr 1.1.3 v stringr 1.4.0 v readr 2.0.1 v forcats 0.5.1 Warning: package &#39;ggplot2&#39; was built under R version 4.0.5 Warning: package &#39;tibble&#39; was built under R version 4.0.5 Warning: package &#39;tidyr&#39; was built under R version 4.0.5 Warning: package &#39;readr&#39; was built under R version 4.0.5 Warning: package &#39;purrr&#39; was built under R version 4.0.5 Warning: package &#39;dplyr&#39; was built under R version 4.0.5 Warning: package &#39;stringr&#39; was built under R version 4.0.5 Warning: package &#39;forcats&#39; was built under R version 4.0.5 -- Conflicts ------------------------------------------ tidyverse_conflicts() -- x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() GRMSVars &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;,&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) ObjectifiedVars &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;) MarginalizedVars &lt;- c(&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;) StrongVars &lt;- c(&quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;) AngryVars &lt;- c(&quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) dfGRMS$GRMStot &lt;- sjstats::mean_n(dfGRMS[,GRMSVars], .80)#will create the mean for each individual if 80% of variables are present Registered S3 methods overwritten by &#39;parameters&#39;: method from as.double.parameters_kurtosis datawizard as.double.parameters_skewness datawizard as.double.parameters_smoothness datawizard as.numeric.parameters_kurtosis datawizard as.numeric.parameters_skewness datawizard as.numeric.parameters_smoothness datawizard print.parameters_distribution datawizard print.parameters_kurtosis datawizard print.parameters_skewness datawizard summary.parameters_kurtosis datawizard summary.parameters_skewness datawizard dfGRMS$Objectified &lt;- sjstats::mean_n(dfGRMS[,ObjectifiedVars], .80)#will create the mean for each individual if 80% of variables are present dfGRMS$Marginalized &lt;- sjstats::mean_n(dfGRMS[,MarginalizedVars], .80)#will create the mean for each individual if 80% of variables are present dfGRMS$Strong &lt;- sjstats::mean_n(dfGRMS[,StrongVars], .80)#will create the mean for each individual if 80% of variables are present (in this case all variables must be present) dfGRMS$Angry &lt;- sjstats::mean_n(dfGRMS[,AngryVars], .80)#will create the mean for each individual if 80% of variables are present (in this case all variables must be present) While we are at it, lets just create tiny dfs with just our variables of interest. GRMStotal &lt;- dplyr::select(dfGRMS, Obj1:Angry3) Objectification &lt;- dplyr::select(dfGRMS, Obj1:Obj10) Marginalization &lt;- dplyr::select(dfGRMS, Marg1:Marg7) Strong &lt;- dplyr::select(dfGRMS, Strong1:Strong5) Angry &lt;- dplyr::select(dfGRMS, Angry1:Angry3) 8.7.1 Calculating and Extracting Item-Total Correlation Coefficients 8.7.1.1 Corrected item-total correlations from the psych::alpha() Lets first ask, Is there support for this instrument as a unidimensional measure? To do that, we get an alpha for the whole scale score. GRMSalpha &lt;- psych::alpha(GRMStotal) #creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop) GRMSalpha Reliability analysis Call: psych::alpha(x = GRMStotal) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.75 0.75 0.81 0.11 3 0.022 2.1 0.42 0.071 lower alpha upper 95% confidence boundaries 0.71 0.75 0.8 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r Obj1 0.74 0.74 0.80 0.11 2.8 0.023 0.026 0.070 Obj2 0.74 0.74 0.80 0.11 2.9 0.023 0.025 0.073 Obj3 0.74 0.74 0.80 0.11 2.8 0.023 0.027 0.069 Obj4 0.74 0.74 0.80 0.11 2.8 0.023 0.027 0.070 Obj5 0.75 0.74 0.81 0.11 2.9 0.023 0.026 0.070 Obj6 0.74 0.74 0.80 0.10 2.8 0.024 0.027 0.067 Obj7 0.74 0.74 0.80 0.10 2.8 0.024 0.027 0.066 Obj8 0.74 0.74 0.81 0.11 2.9 0.023 0.027 0.070 Obj9 0.74 0.74 0.80 0.11 2.8 0.023 0.028 0.067 Obj10 0.74 0.74 0.81 0.11 2.9 0.023 0.028 0.066 Marg1 0.73 0.74 0.79 0.10 2.8 0.024 0.023 0.074 Marg2 0.74 0.74 0.80 0.11 2.8 0.024 0.024 0.072 Marg3 0.74 0.74 0.80 0.11 2.9 0.023 0.025 0.078 Marg4 0.74 0.74 0.80 0.10 2.8 0.024 0.026 0.071 Marg5 0.74 0.74 0.80 0.11 2.8 0.024 0.026 0.067 Marg6 0.74 0.74 0.80 0.11 2.9 0.023 0.026 0.070 Marg7 0.75 0.75 0.81 0.11 3.0 0.023 0.026 0.070 Strong1 0.75 0.75 0.81 0.11 3.0 0.022 0.028 0.071 Strong2 0.75 0.75 0.81 0.11 3.0 0.022 0.027 0.074 Strong3 0.75 0.75 0.81 0.11 3.0 0.022 0.027 0.072 Strong4 0.75 0.75 0.81 0.11 3.0 0.022 0.027 0.074 Strong5 0.75 0.75 0.81 0.11 3.0 0.023 0.028 0.072 Angry1 0.75 0.75 0.81 0.11 3.0 0.023 0.027 0.069 Angry2 0.75 0.75 0.81 0.11 3.0 0.022 0.027 0.079 Angry3 0.75 0.75 0.81 0.11 3.1 0.022 0.026 0.079 Item statistics n raw.r std.r r.cor r.drop mean sd Obj1 259 0.44 0.44 0.41 0.34 1.8 1.09 Obj2 259 0.38 0.38 0.35 0.28 1.9 1.14 Obj3 259 0.45 0.46 0.43 0.36 2.0 1.01 Obj4 259 0.42 0.44 0.41 0.35 1.9 0.89 Obj5 259 0.35 0.37 0.32 0.25 2.1 1.19 Obj6 259 0.49 0.48 0.44 0.39 1.8 1.29 Obj7 259 0.49 0.49 0.47 0.40 2.0 1.09 Obj8 259 0.37 0.39 0.34 0.28 2.2 1.00 Obj9 259 0.42 0.44 0.39 0.34 1.8 0.99 Obj10 259 0.40 0.41 0.37 0.31 1.9 1.05 Marg1 259 0.53 0.50 0.52 0.46 2.0 1.02 Marg2 259 0.46 0.43 0.43 0.38 3.5 0.99 Marg3 259 0.45 0.41 0.39 0.34 2.4 1.30 Marg4 259 0.50 0.47 0.45 0.41 3.3 1.17 Marg5 259 0.49 0.45 0.43 0.38 2.4 1.31 Marg6 259 0.40 0.38 0.35 0.28 2.9 1.37 Marg7 259 0.34 0.32 0.27 0.23 2.7 1.21 Strong1 259 0.26 0.29 0.23 0.18 1.3 0.88 Strong2 259 0.22 0.25 0.19 0.13 2.3 0.95 Strong3 259 0.21 0.26 0.20 0.13 1.5 0.83 Strong4 259 0.27 0.29 0.24 0.17 1.6 1.10 Strong5 259 0.27 0.31 0.24 0.19 1.4 0.83 Angry1 259 0.33 0.31 0.27 0.22 2.0 1.15 Angry2 259 0.27 0.26 0.22 0.16 2.5 1.20 Angry3 259 0.24 0.23 0.18 0.13 2.4 1.16 Non missing response frequency for each item 0 1 2 3 4 5 miss Obj1 0.12 0.30 0.30 0.22 0.05 0.00 0 Obj2 0.12 0.25 0.31 0.24 0.07 0.01 0 Obj3 0.06 0.28 0.36 0.24 0.07 0.00 0 Obj4 0.04 0.28 0.45 0.19 0.04 0.00 0 Obj5 0.12 0.19 0.35 0.22 0.12 0.01 0 Obj6 0.17 0.27 0.24 0.20 0.09 0.02 0 Obj7 0.09 0.24 0.37 0.22 0.06 0.01 0 Obj8 0.04 0.20 0.35 0.33 0.08 0.00 0 Obj9 0.10 0.26 0.41 0.19 0.04 0.00 0 Obj10 0.09 0.27 0.39 0.17 0.08 0.00 0 Marg1 0.06 0.24 0.41 0.22 0.08 0.00 0 Marg2 0.00 0.03 0.12 0.34 0.36 0.15 0 Marg3 0.07 0.17 0.27 0.25 0.18 0.05 0 Marg4 0.00 0.05 0.20 0.29 0.25 0.20 0 Marg5 0.08 0.17 0.31 0.24 0.14 0.07 0 Marg6 0.06 0.10 0.24 0.26 0.22 0.13 0 Marg7 0.03 0.12 0.27 0.35 0.14 0.08 0 Strong1 0.20 0.42 0.31 0.08 0.00 0.00 0 Strong2 0.03 0.18 0.35 0.37 0.07 0.00 0 Strong3 0.12 0.41 0.38 0.09 0.00 0.00 0 Strong4 0.18 0.29 0.32 0.16 0.04 0.00 0 Strong5 0.14 0.38 0.41 0.07 0.00 0.00 0 Angry1 0.09 0.25 0.31 0.26 0.08 0.02 0 Angry2 0.05 0.14 0.31 0.28 0.18 0.04 0 Angry3 0.06 0.15 0.33 0.28 0.15 0.02 0 And now each of the subscales: ObjAlpha &lt;- psych::alpha(Objectification) #creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop) ObjAlpha Reliability analysis Call: psych::alpha(x = Objectification) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.8 0.8 0.79 0.29 4.1 0.018 1.9 0.65 0.28 lower alpha upper 95% confidence boundaries 0.77 0.8 0.84 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r Obj1 0.77 0.78 0.76 0.28 3.5 0.021 0.0038 0.27 Obj2 0.77 0.77 0.76 0.28 3.4 0.021 0.0030 0.26 Obj3 0.78 0.78 0.77 0.28 3.6 0.020 0.0038 0.27 Obj4 0.78 0.78 0.77 0.28 3.6 0.020 0.0044 0.26 Obj5 0.78 0.79 0.77 0.29 3.7 0.020 0.0044 0.28 Obj6 0.79 0.79 0.78 0.29 3.8 0.019 0.0040 0.28 Obj7 0.78 0.79 0.77 0.29 3.7 0.020 0.0044 0.28 Obj8 0.79 0.79 0.78 0.30 3.8 0.019 0.0045 0.30 Obj9 0.79 0.80 0.78 0.30 3.9 0.019 0.0038 0.31 Obj10 0.79 0.80 0.78 0.30 3.9 0.019 0.0039 0.31 Item statistics n raw.r std.r r.cor r.drop mean sd Obj1 259 0.67 0.67 0.62 0.56 1.8 1.09 Obj2 259 0.69 0.69 0.65 0.58 1.9 1.14 Obj3 259 0.64 0.64 0.59 0.53 2.0 1.01 Obj4 259 0.62 0.64 0.59 0.53 1.9 0.89 Obj5 259 0.62 0.61 0.54 0.48 2.1 1.19 Obj6 259 0.60 0.57 0.50 0.44 1.8 1.29 Obj7 259 0.60 0.60 0.53 0.48 2.0 1.09 Obj8 259 0.55 0.56 0.48 0.43 2.2 1.00 Obj9 259 0.50 0.52 0.42 0.38 1.8 0.99 Obj10 259 0.51 0.52 0.42 0.37 1.9 1.05 Non missing response frequency for each item 0 1 2 3 4 5 miss Obj1 0.12 0.30 0.30 0.22 0.05 0.00 0 Obj2 0.12 0.25 0.31 0.24 0.07 0.01 0 Obj3 0.06 0.28 0.36 0.24 0.07 0.00 0 Obj4 0.04 0.28 0.45 0.19 0.04 0.00 0 Obj5 0.12 0.19 0.35 0.22 0.12 0.01 0 Obj6 0.17 0.27 0.24 0.20 0.09 0.02 0 Obj7 0.09 0.24 0.37 0.22 0.06 0.01 0 Obj8 0.04 0.20 0.35 0.33 0.08 0.00 0 Obj9 0.10 0.26 0.41 0.19 0.04 0.00 0 Obj10 0.09 0.27 0.39 0.17 0.08 0.00 0 MargAlpha &lt;- psych::alpha(Marginalization) #creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop) MargAlpha Reliability analysis Call: psych::alpha(x = Marginalization) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.83 0.84 0.83 0.42 5.1 0.017 2.7 0.84 0.42 lower alpha upper 95% confidence boundaries 0.79 0.83 0.86 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r Marg1 0.77 0.78 0.75 0.37 3.5 0.022 0.0069 0.40 Marg2 0.79 0.80 0.79 0.40 4.0 0.020 0.0148 0.40 Marg3 0.80 0.81 0.80 0.42 4.3 0.020 0.0162 0.42 Marg4 0.80 0.82 0.81 0.43 4.5 0.019 0.0153 0.42 Marg5 0.81 0.82 0.81 0.44 4.7 0.018 0.0147 0.42 Marg6 0.82 0.83 0.82 0.45 4.9 0.017 0.0135 0.42 Marg7 0.82 0.84 0.82 0.46 5.1 0.017 0.0137 0.44 Item statistics n raw.r std.r r.cor r.drop mean sd Marg1 259 0.86 0.87 0.88 0.81 2.0 1.02 Marg2 259 0.77 0.78 0.75 0.68 3.5 0.99 Marg3 259 0.73 0.73 0.67 0.60 2.4 1.30 Marg4 259 0.69 0.70 0.63 0.56 3.3 1.17 Marg5 259 0.67 0.66 0.58 0.52 2.4 1.31 Marg6 259 0.64 0.62 0.53 0.47 2.9 1.37 Marg7 259 0.60 0.60 0.49 0.45 2.7 1.21 Non missing response frequency for each item 0 1 2 3 4 5 miss Marg1 0.06 0.24 0.41 0.22 0.08 0.00 0 Marg2 0.00 0.03 0.12 0.34 0.36 0.15 0 Marg3 0.07 0.17 0.27 0.25 0.18 0.05 0 Marg4 0.00 0.05 0.20 0.29 0.25 0.20 0 Marg5 0.08 0.17 0.31 0.24 0.14 0.07 0 Marg6 0.06 0.10 0.24 0.26 0.22 0.13 0 Marg7 0.03 0.12 0.27 0.35 0.14 0.08 0 StrongAlpha &lt;- psych::alpha(Strong) #creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop) StrongAlpha Reliability analysis Call: psych::alpha(x = Strong) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.63 0.63 0.59 0.26 1.7 0.035 1.6 0.59 0.26 lower alpha upper 95% confidence boundaries 0.56 0.63 0.7 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r Strong1 0.57 0.58 0.51 0.26 1.4 0.043 0.0014 0.26 Strong2 0.57 0.57 0.51 0.25 1.3 0.043 0.0027 0.25 Strong3 0.58 0.58 0.51 0.26 1.4 0.041 0.0036 0.24 Strong4 0.56 0.56 0.49 0.24 1.3 0.045 0.0010 0.25 Strong5 0.61 0.61 0.54 0.28 1.6 0.039 0.0016 0.27 Item statistics n raw.r std.r r.cor r.drop mean sd Strong1 259 0.64 0.64 0.50 0.40 1.3 0.88 Strong2 259 0.66 0.65 0.50 0.40 2.3 0.95 Strong3 259 0.61 0.64 0.49 0.38 1.5 0.83 Strong4 259 0.71 0.67 0.54 0.43 1.6 1.10 Strong5 259 0.57 0.59 0.41 0.33 1.4 0.83 Non missing response frequency for each item 0 1 2 3 4 5 miss Strong1 0.20 0.42 0.31 0.08 0.00 0 0 Strong2 0.03 0.18 0.35 0.37 0.07 0 0 Strong3 0.12 0.41 0.38 0.09 0.00 0 0 Strong4 0.18 0.29 0.32 0.16 0.04 0 0 Strong5 0.14 0.38 0.41 0.07 0.00 0 0 AngryAlpha &lt;- psych::alpha(Angry) #creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop) AngryAlpha Reliability analysis Call: psych::alpha(x = Angry) raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r 0.71 0.71 0.62 0.45 2.4 0.031 2.3 0.93 0.44 lower alpha upper 95% confidence boundaries 0.65 0.71 0.77 Reliability if an item is dropped: raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r Angry1 0.63 0.64 0.47 0.47 1.7 0.045 NA 0.47 Angry2 0.60 0.60 0.43 0.43 1.5 0.049 NA 0.43 Angry3 0.61 0.61 0.44 0.44 1.6 0.048 NA 0.44 Item statistics n raw.r std.r r.cor r.drop mean sd Angry1 259 0.78 0.79 0.61 0.51 2.0 1.2 Angry2 259 0.81 0.80 0.64 0.54 2.5 1.2 Angry3 259 0.79 0.80 0.63 0.53 2.4 1.2 Non missing response frequency for each item 0 1 2 3 4 5 miss Angry1 0.09 0.25 0.31 0.26 0.08 0.02 0 Angry2 0.05 0.14 0.31 0.28 0.18 0.04 0 Angry3 0.06 0.15 0.33 0.28 0.15 0.02 0 8.7.1.2 Correlating items with other subscale totals Obj_othR &lt;- psych::corr.test(dfGRMS[c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;, &quot;Marginalized&quot;, &quot;Strong&quot;, &quot;Angry&quot;)]) Marg_othR &lt;- psych::corr.test(dfGRMS[c(&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Objectified&quot;, &quot;Strong&quot;, &quot;Angry&quot;)]) Str_othR &lt;- psych::corr.test(dfGRMS[c(&quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Objectified&quot;, &quot;Marginalized&quot;, &quot;Angry&quot;)]) Ang_othR &lt;- psych::corr.test(dfGRMS[c(&quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;, &quot;Objectified&quot;, &quot;Marginalized&quot;, &quot;Strong&quot;)]) 8.7.1.3 Exctracting values, binding them together, and joining the files #names(Obj_other) #Extracting the item-level statistics from the alpha object Obj_othR &lt;- as.data.frame(Obj_othR$r)#Makes the item-total(other) correlation matrix a df #Adding variable names so we don&#39;t get lost Obj_othR$Items &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;, &quot;Marginalized&quot;, &quot;Strong&quot;, &quot;Angry&quot;) #deleting the rows with the total scale scores Obj_othR &lt;- Obj_othR[!Obj_othR$Items == &quot;Marginalized&quot;,] Obj_othR &lt;- Obj_othR[!Obj_othR$Items == &quot;Strong&quot;,] Obj_othR &lt;- Obj_othR[!Obj_othR$Items == &quot;Angry&quot;,] Obj_othR[ , &#39;Objectified&#39;] &lt;- NA #We need a column for this to bind the items, later. Obj_othR &lt;- dplyr::select(Obj_othR, Items, Objectified, Marginalized, Strong, Angry) #Putting items in order #Item Corrected Total Correlations ObjAlpha &lt;- as.data.frame(ObjAlpha$item.stats)#Grabbing the alpha objet we created earlier and making it a df ObjAlpha$Items &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;) #Joining the two and selecting the vars of interest ObjStats &lt;- full_join(ObjAlpha, Obj_othR, by = &quot;Items&quot;) ObjStats$Objectified &lt;- ObjStats$r.drop #Copy the item-corrected total (r.drop) into the Objectified variable ObjStats &lt;- dplyr::select(ObjStats, Items, Objectified, Marginalized, Strong, Angry) #rm(ObjAlpha, Obj_othR) #It&#39;s messay, dropping all the no-longer-necessary objects from the Global Environment #Extracting the item-level statistics from the alpha object Marg_othR &lt;- as.data.frame(Marg_othR$r)#Makes the item-total(other) correlation matrix a df #Adding variable names so we don&#39;t get lost Marg_othR$Items &lt;- c(&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Objectified&quot;, &quot;Strong&quot;, &quot;Angry&quot;) #deleting the rows with the total scale scores Marg_othR &lt;- Marg_othR[!Marg_othR$Items == &quot;Objectified&quot;,] Marg_othR &lt;- Marg_othR[!Marg_othR$Items == &quot;Strong&quot;,] Marg_othR &lt;- Marg_othR[!Marg_othR$Items == &quot;Angry&quot;,] Marg_othR[ , &#39;Marginalized&#39;] &lt;- NA #We need a column for this to bind the items, later. Marg_othR &lt;- dplyr::select(Marg_othR, Items, Objectified, Marginalized, Strong, Angry) #Item Corrected Total Correlations MargAlpha &lt;- as.data.frame(MargAlpha$item.stats)#Grabbing the alpha objet we created earlier and making it a df MargAlpha$Items &lt;- c(&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;) #Joining the two and selecting the vars of interest MargStats &lt;- full_join(MargAlpha, Marg_othR, by = &quot;Items&quot;) MargStats$Marginalized &lt;- MargStats$r.drop #Copy the item-corrected total (r.drop) into the Marginalized variable MargStats &lt;- dplyr::select(MargStats, Items, Objectified, Marginalized, Strong, Angry) #rm(MargAlpha, Marg_othR) #It&#39;s messay, dropping all the no-longer-necessary objects from the Global Environment Str_othR &lt;- as.data.frame(Str_othR$r)#Makes the item-total(other) correlation matrix a df #Adding variable names so we don&#39;t get lost Str_othR$Items &lt;- c(&quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Objectified&quot;, &quot;Marginalized&quot;, &quot;Angry&quot;) #deleting the rows with the total scale scores Str_othR &lt;- Str_othR[!Str_othR$Items == &quot;Objectified&quot;,] Str_othR &lt;- Str_othR[!Str_othR$Items == &quot;Marginalized&quot;,] Str_othR &lt;- Str_othR[!Str_othR$Items == &quot;Angry&quot;,] Str_othR[ , &#39;Strong&#39;] &lt;- NA Str_othR &lt;- dplyr::select(Str_othR, Items, Objectified, Marginalized, Strong, Angry) #Item Corrected Total Correlations StrongAlpha &lt;- as.data.frame(StrongAlpha$item.stats) #Grabbing the alpha objet we created earlier and making it a df StrongAlpha$Items &lt;- c(&quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;) #Joining the two and selecting the vars of interest StrStats &lt;- full_join(StrongAlpha, Str_othR, by = &quot;Items&quot;) StrStats$Strong &lt;- StrStats$r.drop #Copy the item-corrected total (r.drop) into the Strong variable StrStats &lt;- dplyr::select(StrStats, Items, Objectified, Marginalized, Strong, Angry) rm(StrongAlpha, Str_othR) #It&#39;s messay, dropping all the no-longer-necessary objects from the Global Environment Ang_othR &lt;- as.data.frame(Ang_othR$r)#Makes the item-total(other) correlation matrix a df #Adding variable names so we don&#39;t get lost Ang_othR$Items &lt;- c(&quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;, &quot;Objectified&quot;, &quot;Marginalized&quot;, &quot;Strong&quot;) #deleting the rows with the total scale scores Ang_othR &lt;- Ang_othR[!Ang_othR$Items == &quot;Objectified&quot;,] Ang_othR &lt;- Ang_othR[!Ang_othR$Items == &quot;Marginalized&quot;,] Ang_othR &lt;- Ang_othR[!Ang_othR$Items == &quot;Strong&quot;,] Ang_othR[ , &#39;Angry&#39;] &lt;- NA Ang_othR &lt;- dplyr::select(Ang_othR, Items, Objectified, Marginalized, Strong, Angry) #Item Corrected Total Correlations AngryAlpha &lt;- as.data.frame(AngryAlpha$item.stats) #Grabbing the alpha objet we created earlier and making it a df AngryAlpha$Items &lt;- c(&quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) #Joining the two and selecting the vars of interest AngStats &lt;- full_join(AngryAlpha, Ang_othR, by = &quot;Items&quot;) AngStats$Angry &lt;- AngStats$r.drop #Copy the item-corrected total (r.drop) into the Angry variable AngStats &lt;- dplyr::select(AngStats, Items, Objectified, Marginalized, Strong, Angry) rm(AngryAlpha, Ang_othR) #It&#39;s messay, dropping all the no-longer-necessary objects from the Global Environment #Adding all the variables into a single table ItemAnalysis &lt;- rbind(ObjStats, MargStats, StrStats, AngStats) #Preparing and adding the r.drop for total scale score TotAlpha &lt;- as.data.frame(GRMSalpha$item.stats) TotAlpha$Items &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;,&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) TotAlpha &lt;- dplyr::select(TotAlpha, Items, r.drop) #deleting the rows with the total scale scores #Adding the r.drop for the total scale score ItemAnalysis &lt;- full_join(TotAlpha, ItemAnalysis, by = &quot;Items&quot;) #Adding the values from the Othogonal rotation pcaORTH_loadings &lt;- data.frame(unclass(pcaORTH$loadings)) #I had to add &quot;unclass&quot; to the loadings to render them into a df pcaORTH_loadings$Items &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;,&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) #Item names for joining (and to make sure we know which variable is which) #Deleting those lower rows #pcaORTH_loadings &lt;- pcaORTH_loadings[!pcaORTH_loadings$Items == &quot;GRMSTot&quot;,] #pcaORTH_loadings &lt;- pcaORTH_loadings[!pcaORTH_loadings$Items == &quot;Objectified&quot;,] #pcaORTH_loadings &lt;- pcaORTH_loadings[!pcaORTH_loadings$Items == &quot;Marginalized&quot;,] #pcaORTH_loadings &lt;- pcaORTH_loadings[!pcaORTH_loadings$Items == &quot;Strong&quot;,] #pcaORTH_loadings &lt;- pcaORTH_loadings[!pcaORTH_loadings$Items == &quot;Angry&quot;,] pcaORTH_loadings &lt;- rename(pcaORTH_loadings, objORTH = RC1, margORTH = RC2, strORTH = RC3, angORTH2 = RC4) #Joining with the Item Stats Comparisons &lt;- full_join(ItemAnalysis, pcaORTH_loadings, by = &quot;Items&quot;)#I had to add &quot;unclass&quot; to the loadings to render them into a df #Adding the oblique loadings pcaOBLQ_loadings &lt;- data.frame(unclass(pcaOBL$loadings)) #I had to add &quot;unclass&quot; to the loadings to render them into a df pcaOBLQ_loadings$Items &lt;- c(&quot;Obj1&quot;, &quot;Obj2&quot;, &quot;Obj3&quot;, &quot;Obj4&quot;, &quot;Obj5&quot;, &quot;Obj6&quot;, &quot;Obj7&quot;, &quot;Obj8&quot;, &quot;Obj9&quot;, &quot;Obj10&quot;,&quot;Marg1&quot;, &quot;Marg2&quot;, &quot;Marg3&quot;, &quot;Marg4&quot;, &quot;Marg5&quot;, &quot;Marg6&quot;, &quot;Marg7&quot;, &quot;Strong1&quot;, &quot;Strong2&quot;, &quot;Strong3&quot;, &quot;Strong4&quot;, &quot;Strong5&quot;, &quot;Angry1&quot;, &quot;Angry2&quot;, &quot;Angry3&quot;) #Item names for joining (and to make sure we know which variable is which) #Deleting those lower rows #pcaOBLQ_loadings &lt;- pcaOBLQ_loadings[!pcaORTH_loadings$Items == &quot;GRMSTot&quot;,] #pcaOBLQ_loadings &lt;- pcaOBLQ_loadings[!pcaORTH_loadings$Items == &quot;Objectified&quot;,] #pcaOBLQ_loadings &lt;- pcaOBLQ_loadings[!pcaORTH_loadings$Items == &quot;Marginalized&quot;,] #pcaOBLQ_loadings &lt;- pcaOBLQ_loadings[!pcaORTH_loadings$Items == &quot;Strong&quot;,] #pcaOBLQ_loadings &lt;- pcaOBLQ_loadings[!pcaORTH_loadings$Items == &quot;Angry&quot;,] pcaOBLQ_loadings &lt;- rename(pcaOBLQ_loadings, objOBLQ = TC1, margOBLQ = TC2, strOBLQ = TC3, angOBLQ = TC4) #Joining with the Item Stats Comparisons &lt;- full_join(Comparisons, pcaOBLQ_loadings, by = &quot;Items&quot;)#I had to add &quot;unclass&quot; to the loadings to render them into a df write.csv(Comparisons, file = &quot;GRMS_Comparisons.csv&quot;, sep = &quot;,&quot;, row.names=FALSE, col.names=TRUE)#Writes the table to a .csv file where you can open it with Excel and format Warning in write.csv(Comparisons, file = &quot;GRMS_Comparisons.csv&quot;, sep = &quot;,&quot;, : attempt to set &#39;col.names&#39; ignored Warning in write.csv(Comparisons, file = &quot;GRMS_Comparisons.csv&quot;, sep = &quot;,&quot;, : attempt to set &#39;sep&#39; ignored saveRDS(Comparisons, &quot;GRMS_Comparisons.rds&quot;)#Writes the file as an .rds so that if anything is specially formatted, it is retained 8.7.1.4 Interpreting the result The result of this work is a table that includes: r.drop Corrected item-total (entire GRMS) coefficients Item-total correlations of the items correlated with their own subscale (bold; correlation does not include the item being correlated) and the other subscales PCA: Orthogonal rotation factor loadings of the four-scales with a rotation that maximizes the independents (uncorrelatedness) of the scales PCA: Oblique rotation factor loadings of the four-scales with a rotation that permits correlation between subscales Image of a table of values from the item analysis and PCA solutions with orthogonal and oblique rotations We are looking for items that load higher on their own scales than they do on other scales when they are close or have a number of strong loadings, it means that its not going to discriminate well (think within-in scale discriminant validity). if there are a number of these, there will likely be stronger correlations between subscales (indicating that the oblique rotation was an appropriate choice). with low/no cross-loadings, this supports the choices of an orthogonal (uncorrelated) solution when the item has a strong, positive loading on its own scale, it supports within-scale convergent validity. similarities and differences across the item-analysis, PCA orthogonal, and PCA oblique solutions. Our biggest interest is in whether items change scale membership and/or have cross-loadings. This scale is performing extremely well with a great deal of stability This, in part, is likely facilitated by the data simulation where we had the benefit of factors telling items where to load. 8.8 Practice Problems In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The most complex is to use data of your own.In either case, please plan to: Properly format and prepare the data. Conduct diagnostic tests to determine the suitability of the data for PCA. Conducting tests to guide the decisions about number of components to extract. Conducting orthogonal and oblique extractions (at least two each with different numbers of components). Selecting one solution and preparing an APA style results section (with table and figure). 8.8.1 Problem #1: Play around with this simulation. Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If PCA is new to you, perhaps you just change the number in set.seed(210921) from 210921 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Check and, if needed, format data 5 _____ 2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartletts, determinant). 5 _____ 3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory). 5 _____ 4. Conduct an orthogonal extraction and rotation. 5 _____ 5. Conduct an oblique extraction and rotation. 5 _____ 6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. 5 _____ 7. APA style results section with table and figure of one of the solutions. 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 8.8.2 Problem #2: Conduct a PCA with the Szymanski and Bissonette (2020) research vignette that was used in prior lessons. The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonettes (2020)Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PCA. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Check and, if needed, format data 5 _____ 2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartletts, determinant). 5 _____ 3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory). 5 _____ 4. Conduct an orthogonal extraction and rotation. 5 _____ 5. Conduct an oblique extraction and rotation. 5 _____ 6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. 5 _____ 7. APA style results section with table and figure of one of the solutions. 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ 8.8.3 Problem #3: Try something entirely new. Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete PCA. The data should allow for at least two (ideally three) components/subscales. Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric. Assignment Component Points Possible Points Earned 1. Check and, if needed, format data 5 _____ 2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartletts, determinant). 5 _____ 3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory). 5 _____ 4. Conduct an orthogonal extraction and rotation. 5 _____ 5. Conduct an oblique extraction and rotation. 5 _____ 6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors. 5 _____ 7. APA style results section with table and figure of one of the solutions. 5 _____ 8. Explanation to grader 5 _____ Totals 40 _____ References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
