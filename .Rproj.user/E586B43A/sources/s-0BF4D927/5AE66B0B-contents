---
title: "Item Analysis of Multiple Choice Exams"
author: "Lynette H. Bikos, PhD, ABPP"
date: "9/20/2019"
output: word_document
bibliography:  ["itemanal.bib"]
csl: apa-single-spaced.csl
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Clear global env, include=FALSE}
# Clear your Global Environment
rm(list=ls())
```

![Image of PhD Comicstrip](grading.jpg){#id .class width=900 height=300px}

Screencast Playlist link:  https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=30895b20-a981-4026-99f9-aad0016c7f29 

## Navigating this lectuRette
About 1 hour 15 minutes in length of screencasts. Plan for an extra 30 minutes to work through the statistics and make sense of them.

### Quiz pRep
Be able to respond to quiz items about:

* why a test bank might be a good idea
* the "ideal" difficulty level (and how guessing, speeded tests, interitem correlations, and purpose of the test) make that less clear
* the effects of skew
* interpreting item quality and characteristics on the basis of difficulty, correlation, and discrimination


### Planning for youR homewoRk
No homework assignment associated with this lecturette. 

### Readings & ResouRces

Classic psychometric texts tend to not cover item analysis for achievement tests.  And/or, they skip over these fundamentals and move straight to item response theory/Rasch modelig (IRT).  After scouring the internet, I landed on these two resources as concise, accessible, summaries. These are available as PDFs in the Readings/Resources folders, but I have also listed the website at the time of the lecture.

* Item analysis for teachers:  Making sense of assessment results.  Ella Cline Shear School of Education P-12 Resources Page. SUNY Geneseo.  Retrieved September 20, 2019.  Retreived from https://www.geneseo.edu/sites/default/files/sites/education/p12resources-item-analysis-and-instruction.pdf
  + Great conceptual explanation of what item analysis is and how it is operationalized in the context of achievement exams.  Useful interpretive rules of thumb.
* Understanding item analysis.  Office of Educational Assessment, University of Washington.  Retrieved September 20, 2019.  Retrieved from https://www.washington.edu/assessment/scanning-scoring/scoring/reports/item-analysis/
  + It is common for excellent instructions/descriptions to accompany the scoring software used by institutions.  UW appears to use ScorePak and this resource provides both conceptual and interpretive information.
* Revelle, W. (2017). An overview of the psych package.  Retrieved from http://personality-project.org/r/overview.pdf 
  + Pages 85-85 provide a vignette for conducting item analysis on multiple choice items.


# Item Analysis in the Educational/Achievement Context

Multiple choice, true/false, and other *objectively* formatted/scored items are part-n-parcel to educational/achievement assessment.  But how do we know if the items are performing the way they should?  This lecture focuses on item analysis in the context of multiple choice and true false items.  Using these practices can help you identify what selection of items you'd like for your exams.  These can be critical tools in helping you improve your ability to assess student performance.  

In-so-doing, we walk through a bit of "what we used to do," to current common practices, to a glimpse of our future. We owe all of this to rapid advances in technology.

*Test banks* are instructor-created resources for developing/storing/protecting items for use in future exams.  We create test banks when we carefully distribute/collect/protect items "that work" (from statistical perspective). Why would we want to do this?

* Once a test is "out" it's out. Instructors can presume that resourceful students are using it to study; yet all students won't have equal access to it.
* Developing "good" items takes a good deal of time; does the instructor want to redo this each term?
* Should we be piloting *all new items* on students each term and then having the debates about whether the item should be rescored? 
  + Better is to introduce a proportion of new items each year and evaluate them for inclusion in the test bank;  EPPP, SAT, GRE do this.
* A challenge is providing students appropriate study tools -- old exams are favorites of students (but maybe there are other ways -- worksheets, Jeopardy).

The conceptual portions of this lecture, particularly the interpretation of the difficulty and discrimination statistics are based in Anastasi's work [@anastasi_psychological_1997]


## And now a quiz! For serious, take it.
Let's start with some items from an early version of the exam I gave when I taught CPY7020/Statistical Methods.

Item 5     
A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of _____ scaling.

* a)	Nominal
* b)	Ordinal
* c)	Interval
* d)	Ratio


Item 11     
The term “grade inflation” has frequently been applied to describe the distribution of grades in graduate school.  Which of the following best describes this distribution.

* a)	negatively skewed 
* b)	uniform/rectangular
* c)	positively skewed and leptokurtic
* d)	uniform and platykurtic

Item 19	     
All distributions of Z-scores will have the identical

* a)	Mean
* b)	Variance
* c)	Standard deviation
* d)	All of the above

Item 21     
The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

* a)	mean
* b)	percentile rank
* c)	raw score
* d)	z-score

Item 37     
Of the following, what statement best describes $r^2$ = .49

* a)	strong positive correlation
* b)	strong positive or negative correlation
* c)	weak positive or negative correlation
* d)	weak negative correlation

Item 38     
When there are no ties among ranks, what is the relationship between the Spearman rho ($\rho$) and the Pearson ($r$)?

* a)	$\rho$ = $r$
* ii)	$\rho$ > $r$
* a)	$\rho$ < $r$
* b)	no relationship

## Item Difficulty 

### Percent passing

**Item difficulty index** is the proportion of test takers who answer an item correctly.  It is calculated by dividing the number of people who passed the item (e.g., 55) by the total number of people (e.g., 100).

* If 55% pass an item, we write $p$ = .55
* The easier the item; the larger the percentage will be. 

What is an ideal pass rate (and this "ideal" is the *statistical ideal* mostly for norm-referenced tests like the ACT, SAT, GRE)?

* The closer the difficulty of an item approaches 1.00 or 0, the less differential information about test takers it contributes. 
  + If, out of 100 people, 50 pass an item and 50 fail ($p$ = .50)…we have 50 X 50 or 2,500 paired comparisons or differential bits of information.
* How much information would we have for an item passed by:
  + 70% of the people (70 * 30 = ???)
  + 90% of the people (90 * 10 = ???)
* For maximum differentiation, one would choose all items at the .50 level (but hold up...)

### Several factors prevent .50 from being the ideal difficulty level

**Speeded tests** complicate the interpretation of item difficulty because items are usually of equivalent difficulty and there are so many that no one could complete them all.  Thus later items should be considered to be more difficult -- but item difficulty is probably not the best assessment of item/scale quality.

**Guessing** the correct answer in true/false and multiple choice contexts interferes with the goal of $p$ - .50. In a 1952 issue of *Psychometrika*, Lord provided thish guide for optimal $p$ values based on the number of choices in the objective context:

```{r Optimal P table, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE}
#install.packages("kableExtra")
#install.packages("pander")
#library(knitr)
#library(kableExtra)
library(tidyverse)
library(pander)

OptimalP <- read.csv("OptimalP.csv")
pandoc.table(OptimalP, style = "multiline", split.table = Inf, justify = "left", format = docx)
```


**The purpose** of the testing changes the ideal difficulty level.

* If the test is *norm-referenced* (ACT, SAT, GRE), .50 is very useful.
* If the test is mastery oriented, $p$ values may be be as high as 0.90 since student performance is a function of repeated attempts with feedback.

**Item intercorrelations** impacts interpretation of item difficulty.

* The more homogeneous the test, the higher these intercorrelations will be. If all items were perfectly intercorrelated and all were of the .50 difficulty level: 
  + the same 50 persons out of 100 would pass each item, that is,
  + half of the test takers would obtain perfect scores; the other half zero scores
* Best to select items with a moderate spread of difficulty, but whose AVERAGE difficulty level of .50
* The percentage of persons passing an item, expresses the item difficulty in terms of which statistical scale of measurement?  (i.e., nominal, ordinal, interval, ratio)
  + Because of this issue, we can correctly indicate the rank order or relative difficulty of the items 
  + However, we cannot infer that the difference in difficulty between Items 1 and 2 is equal to the difference between Items 2 and 3.
* We can make an *equal-interval inference* with the table of normal curve frequencies (i.e., translating the proportion to z-scores). Z-scores would be used as the units if an equal interval inference was required in the analysis. For example,
  + *p* = .84 is equal to -1 *SD*
  + *p* = .16 is equal to +1 *SD*
  
*Seem a little upside down?  Recall that we are calculating the percent passing and starting the count "from the top." So a relatively easy item where 84% passed, would have an standard deviation of -1.*

![Image of graphs where p = .84 and p = .16](p84p16.jpg){#id .class width=900 height=300px}


## Item Discrimination

The degree to which an item differentiates correctly among test takers in the behavior that the test is designed to measure.

* the *criterion* can be internal or external to the test itself
  + under some conditions, the two approaches lead to opposite results because (a) items chosen to maximize the validity of the test tend to be the ones rejected on the basis of internal consistency, and (b) rejecting items with low correlations with the total score tends to homogenize the test (we are more likely to keep items with the highest average intercorrelations)
* *internal* criteria serve to maximize internal consistency or homogeneity of the test, example:
  + achievement test, where criteria is total score itself
* *external* criteria serve to maximize the validity of an external criterion, example:
  + a different assessment of the same ability being assessed
  
  
### Index of Discrimination

* Compare the proportion of cases that pass an item in contrasting criterion groups
  + upper (U) and lower (L) criterion groups are selected from the extremes of the distribution
  + traditionally these groups are created from the 27% from each of those sides of the distribution
 
* This *index of discrimination (D)* can be expressed as a difference of raw frequencies (U - L), or (more conventionally) as the difference of percentages of those who scored it correctly in the upper 27% and lower 27% groups
  + when all members of the U group and none of the members of the L group pass, D = 100
  + when all members of the L group and none of the members of the U group pass, D = 0
  + optimum point at which these two conditions reach balance is with the upper and lower 27% 
  
```{r Index of Discrimination}
OptimalDisc <- read.csv("OptimalDisc.csv")
pandoc.table(OptimalDisc, style = "multiline", split.table = Inf, justify = "left", format = docx)
```

  
  
![Image of excerpt of 5 items illustrating item discrimination ](ULchart.jpg){#id .class width=400 height=300px}

Item 5     

A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of _____ scaling.

* a)	Nominal
* b)	Ordinal
* c)	Interval
* d)	Ratio

If we wanted to hand-calculate the index of discrimination for Item #5, we find that 3 people (100%) in the upper group selected the correct answer and 3 people (100%) in the lower group selected the correct answer:  3 - 3 = 0.  If you prefer percentages:  100% - 100% = 0%.  This means there is no discrimination in performance of the upper and lower performing groupings.

Older scoring systems (e.g., Scantron) used to provide this information.

![Scantron image of item analysis for exam item #5](Item5.jpg){#id .class width=1000 height=250}

Considering what we have learned already, Item #5 is

* too easy
* does not discriminate between upper and lower performance
* *Yes, there is more data on here, but we will save it for the next level of review...just a few moments.*


Item 11     

The term “grade inflation” has frequently been applied to describe the distribution of grades in graduate school.  Which of the following best describes this distribution.

* a)	negatively skewed 
* b)	uniform/rectangular
* c)	positively skewed and leptokurtic
* d)	uniform and platykurtic

For Item #11, 2 people (~66%) from the upper group selected the correct answer, 1 person (~33%) from the lower group selected the correct answer.  Thus, the U-L was +1 (+33%) and the item is working in the proper direction.

![Scantron image of item analysis for exam item #11](Item11.jpg){#id .class width=1000 height=250}

Considering what we have learned already, Item #11 is

* difficult (50% overall selected the correct item)
* does discriminate between upper and lower performance, with more individuals in the upper groups selecting the correct answer than in the lower group

Item 19      

All distributions of Z-scores will have the identical

* a)	Mean
* b)	Variance
* c)	Standard deviation
* d)	All of the above

Hand calculation:  Upper = 3 (100%), Lower = 3 (100%).  Difference = 0.  

![Scantron image of item analysis for exam item #19](Item19.jpg){#id .class width=1000 height=250}

Considering what we have learned already, Item #19 is

* somewhat easy (92% overall selected the correct item)
* using the U - L discrimination index, it does not discriminate between upper and lower performance



Item 21     

The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

* a)	mean
* b)	percentile rank
* c)	raw score
* d)	z-score

Hand calculation:  Upper = 2 (66%), Lower = 3 (100%).  Difference = -33%. This item is upside down.   This is different than the Scantron snip below because uppers and lowers were likely calculated on exam total that included subjectively scored items (essays; and I no longer have that data). 

![Scantron image of item analysis for exam item #21](Item21.jpg){#id .class width=1000 height=250}
Considering what we have learned already, Item21 is

* somewhat difficult (58% overall selected the correct item)
* on the basis of the hand-calculations it does not discriminate between uppers and lowers


Item 37

Of the following, what statement best describes $r^2$ = .49

* a)	strong positive correlation
* b)	strong positive or negative correlation
* c)	weak positive or negative correlation
* d)	weak negative correlation

Hand calculation:  Upper = 2 (66%), Lower = 0 (0%).  Difference = 66%.  

![Scantron image of item analysis for exam item #37](Item37.jpg){#id .class width=1000 height=250}


Considering what we have learned already, Item 37 is

* very difficult (33% overall selected the correct item)
* on the basis of the hand-calculations, this completely discriminates the uppers from the lowers)


Item 38

When there are no ties among ranks, what is the relationship between the Spearman rho ($\rho$) and the Pearson r ($r$)?

* a)	$\rho$ = $r$
* ii)	$\rho$ > $r$
* a)	$\rho$ < $r$
* b)	no relationship

Hand calculation:  Upper = 1 (33%), Lower = 1 (33%).  Difference = 0%.  

![Scantron image of item analysis for exam item #38](Item38.jpg){#id .class width=1000 height=250}

Considering what we have learned already, Item 21 is

* very difficult (25% overall selected the correct item)
* on the basis of the hand-calculations, this does not discrimniate the uppers from the lowers)

## In the *psych* Package

Using the *score.multiple.choice()* function in the *psych* package.  Documentation is pp. 85-86 in http://personality-project.org/r/overview.pdf

A multiple choice exam presumes that there is one correct response.  We start with a dataset that records the students' responses. It *appears* that the psych package requires these responses to be numerical (rather than A, B, C, D).

```{r Import data}
library(psych)
exam <- read.csv ("CPY7031ExamResults2008num.csv", head = TRUE, sep = ",")
```

We create a key of the correct answers.
```{r Item analysis for a multiple choice test, message=TRUE, warning=FALSE}
library(psych)
#data(exam)
exam.keys <- c(1,4,1,2,1,1,3,2,1,3,1,2,2,2,2,2,1,3,4,1,4,3,3,3,1,4,4,1,1,2,1,1,3,3,3,2,2,1,3,3,4)  
score.multiple.choice(exam.keys, exam, score = TRUE, skew = TRUE)
#score = TRUE retains the format as a multiple choice test, FALSE transitions the data to dichotomous (correct/incorrect) scoring which is useful in some analyses
#skew = TRUE gets us skew and kurtosis; determined on the basis of correct/incorrect
```

SECOND SCREEN OF OUTPUT:  
The first 5 rows provide a *distractor analysis*.  The key tells which answer was correct.  The next columns show the percentage of time that each distractor was chosen.  

The *miss* column tells us how many times the item was skipped.
*n* tells us how many participants completed the item (this would necessarily be the inverse of "miss").

*mean* repeats the proportion of individuals who scored correctly.  This is an indication of **item dificulty**.

*sd* gives an indication of the variability around that mean

*r* is a point-biserial correlation with a dichotomous correct/incorrect correlated with the continuously scaled total scale score.  Positively scored items let us know that item is working in the proper direction...the students who got the item correct, did better on the overall total score and vice versa. 

* one of the best indicators of an items ability to *discriminate* (hence, **item discrimination**) among the criterion assessed on the test
* it is important to investigate those with values close to zero (no relation between item performance with overall test performance) and those with negative values (meaning that those who had the correct answer on the item were those who scored lower on the exam).

Important to look at the *r* and *mean* columns, together to understand the degree of difficulty and how well each item is discriminating between performance levels.

*skew* can provide an indication of ceiling and floor effects.

![Image of two graphs illustrating positive and negative skew](skew.jpg){#id .class width=600 height=200px}

If a score has a significant negative skew (long tail to the left), then there may be a piling up of items at the upper end of the scale.  This would indicate an *insufficient ceiling* and make it more difficult to discriminate among differences among the higher performers.

If a score has a significant positive skew (long tail to the right), then there may be a piling up of items at the low end, indicating an *insufficient floor.*  That is, it lacks the ability to discriminate between poorer performers.

How do you tell what is significant?

General rule of thumb says that anything greater or less than 1.0 is significantly skewed.  A formal z-test (CITE FIELD) can be conducted this way: $z_{skewness}= \frac{S-0}{SE_{skewness}}$ 

In our exam dataset, -2.65 is the most extremely negatively skewed item and its *se* = 0.08.

```{r z-score high skew}
-2.65/0.08
```

Considering that anything greater than +/- 1.96 is statistically significant...it is safe to say that this item has an insufficient ceiling.

What about the items with -0.30 (*se* = 0.15)

```{r z-score low skew}
-.30/.15
```

Not as extreme (and recall my *N* = 12, so I should probably look up a critical *t* value), but there is still some question about whether my exam items can discriminate among high performers.

WARNING -- because these distributions are *dichotomous* (correct/incorrect) they will never be normally distributed, but, like the difficulty index, they give another glimpse of the ability to discriminate.


FIRST SCREEN OF OUTPUT:  In this context, *alpha* should tell us the consistency of getting answers right or wrong.  Alpha is directly effected by:

* *interitem correlations* among the items:  a large number of positive correlations between items increases alpha
* test length:  more items produce higher reliability (all things being equal)
* test content:  the more diverse/broad, the lower the reliability coefficient

In the context of the classroom, reliabilities above .70 are probably adequate and above .80 are good.  Reliabilities below .60 suggest that items should be investigated and additional measures (tests, homework assignments) should be included in assigning grades.


So let's return and look at each of those items, now with their statistics:

Item 5     
A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of _____ scaling.

* a)	Nominal
* b)	Ordinal
* c)	Interval
* d)	Ratio

Mean = 1.0 (much too easy), *r* = NA, Distractors:  1.00	0.00	0.00	0.00, skew = NaN 


Item 11     
The term “grade inflation” has frequently been applied to describe the distribution of grades in graduate school.  Which of the following best describes this distribution.

* a)	negatively skewed 
* b)	uniform/rectangular
* c)	positively skewed and leptokurtic
* d)	uniform and platykurtic

Mean = .50, *r* = .42, Distractors: 0.50	0.00	0.33	0.17, skew = 0.30


Item 19     
All distributions of Z-scores will have the identical

* a)	Mean
* b)	Variance
* c)	Standard deviation
* d)	All of the above

Mean = .92, *r* = .04, Distractors: 0.08	0.00	0.00	0.92	, skew = -2.65


Item 21     
The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

* a)	mean
* b)	percentile rank
* c)	raw score
* d)	z-score

Mean = .58, *r* = -.19, Distractors:  0.00	0.42	0.00	0.58, skew = -0.30


Item 37     
Of the following, what statement best describes $r^2$ = .49

* a)	strong positive correlation
* b)	strong positive or negative correlation
* c)	weak positive or negative correlation
* d)	weak negative correlation

Mean = .33, *r* = .49, Distractors:  0.25	0.33	0.42	0.00, skew = .62

Item 38     
When there are no ties among ranks, what is the relationship between the Spearman rho ($\rho$) and the Pearson r ($r$)?

* a)	$\rho$ = $r$
* ii)	$\rho$ > $r$
* a)	$\rho$ < $r$
* b)	no relationship

Mean = .27, *r* = -.07, Distractors:  0.27	0.00	0.00	0.73, skew = .68     
*Notice anything else that's funky about Item38?*


**Regarding overall test characteristics**
![Scantron image of item analysis for exam item #38](examheader.jpg){#id .class width=1000 height=250}

Having looked at the individual items, we now have a more thorough picture that informs what we see in overall exam results.

Notice the total points possible for the whole scale from the Scantron ouput (56) is different than the item-analysis in *psych*.  This is because the Scantron also had the values for the essays (and I didn't have those).  This is the source of the discrepancy for the upper-minus-lower item-total calculation.

The mean in the overall report is the average score of the student (not proportion correct, as it is for individual items).

Skew is a little negative, signifying some difficulty in discriminating between the high scorers; there is no SE to calculate it.

The overall reliability coefficient is 0.77 (adequate, but doesn't quite reach the .80 threshhold).

I do keep all my items in a test bank and between 2008 and 2009 I would have revised or replaced the problematic items.


## The Whence and Whither of Item Analysis
To recap -- at the instructional level, the combination of percent passing (mean) and point-biserial correlation (discrimination index) is status quo for evaluating/improving the items.

The *psych* package draws from its IRT (item response theory) capacity to conduct distractor analysis.  IRT models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.  

In these graphs, theta is on the X axis.  Theta is the standard unit of the IRT model that represents the level of the domain being measured.  Like a z-score, a theta unit of "1" is the SD of the calibrated sample.

The pattern of responses to multiple choice ability items can show that some items have poor distractors. This may be done by using the the irt.responses function. A good distractor is one that is negatively related to ability.

```{r Graphing distractors}
#data(exam)
results <- score.multiple.choice(exam.keys, exam, score = TRUE, short = FALSE)
#op <- par(mfrow=c(2,2))
#set this to see the output for multiple items
```



```{r graph item5}

#irt.responses(scores$scores, exam[5], breaks = 2)
irt.responses(results$scores, exam[5], breaks = 2)
```

With Item #5, 100% responded correctly.  Not much to see.

```{r graph item11}
irt.responses(results$scores, exam[11], breaks = 2)
```

With Item #11 there is a positive relationship between 1/A (correct answer) and ability (theta), no relationship between 3/C and ability, and a negative relationship between 4/D and ability (indicating that 4/D is a good distractor).  These map onto each of the point-biserial correlations associated with the distractors in the Scantron output.


```{r graph item19}
irt.responses(results$scores, exam[19], breaks = 2)
```

Item #19 shows rather flat (no relationship) relations with ability for the correct item and the lone distractor.

```{r graph item21}
irt.responses(results$scores, exam[21], breaks = 2)
```

For Item #21, a positive relationship between the WRONG answer (2/B) and ability (theta) and a negative relationship between 4/D (incorrect answer) and ability.  This makes sense as the point biserial for the overall item was 0-.13.


```{r graph item37}
irt.responses(results$scores, exam[37], breaks = 2)
```

A negative relation between endorsing 1/A and ability (a good distractor).  A slightly positive relationship with ability for endorsing 3/C.  A positive relation with ability for those endorsing 2/B 9correct answer).


```{r graph item38}
irt.responses(results$scores, exam[38], breaks = 2)
```

Flat relations with ability for endorsing 1/A (correct answer) or 4/D (incorrect answer).

## Closing thoughts on developing measures in the education/achievement contest

Item analysis tends to be an assessment of *reliability*.  However, in the context of educational assessment and achievement exams, there are also *validity* issues.  

**Content validity** is concerned with whether or not the scale adequately represents the entirety of the *domain* to be assessed.

In educational and achievement contexts, this is often accomplished with a *table of specifications.*

![Image of a table of specifications](SpecsTableNewBG.png){#id .class width=650 height=300}

Image source:  http://www.sfsu.edu/~testing/MCTEST/testconstruction.html (no longer operational)  Usage type: unknown.

This tool allows us to identify the instructional objectives (first column) to be covered on the exam and allows the instructor to map out how the item will be assessed (the next 3 columns represent the manner in which the objective will be assessed) and what proportion of item types/content are covered.  Mapping an exam out this way helps us

* ensure proportionate domain coverage (not just writing "easy" objectives)
* ensure this coverage is assessed in different ways (e.g., knowledge, comprehension, application)
  +  this model is very "Blooms taxonomy of educational objectives" from the 1950s.  Since then, there are numerous categories of educational objectives.  APA's model is KSAs (knowledge, skills, attitudes).

**Takeaway message**:  Together, mapping out  exam coverage in a table of specifications PLUS item analysis (difficulty/discrimination) can be powerful tools in educational assessment.

# Bonus Reel
![Image of film strip](film-strip-1.jpg){#id .class width = 620 height = 211} 

We quickly work through the vignette that Revelle provided.  It's an 8-option multiple choice ability assessment.

Revelle is a personality researcher, with an active research project found at his website:  https://personality-project.org/.  Taking his test (https://sapa-project.org/) may make this information more engaging because it is a sample of this data we are using in the vignette.

Data is from an online ability test at the Personality Project: Synthetic Aperture Personality Assessment (SAPA; Revelle et al., 2010, 2011).  Sprinkled throughout this lengthy personality assessment are items assessing ability.  Each item has 8 choices.

![Example of image from the SAPA Project](RevelleAbilityItem.jpg){#id .class width = 500 height = 230} 

The "iqitems" dataset is embedded in the *psych* package.  If the *psych* package is loaded, all we need to do is call it up!  We can "guess" by the systematic variable names that there are 4 items each on these "scales":  reason, letter, matrix, rotate.

```{r A glimpse at iqitems, eval = FALSE}
library(psych)
library(psychTools)
psychTools::iqitems
```

Just as we did with our earlier example, we create a key of the correct answers.  Using that key, we can use the *score.multiple.choice()* function.

```{r Score Revelle instrument, message = FALSE, warning = FALSE,}
library(psychTools)
#data(iqitems)
iq.keys <- c(4,4,4, 6,6,3,4,4, 5,2,2,4, 3,2,6,7)
score.multiple.choice(iq.keys, iqitems, short = FALSE, score = TRUE, totals = TRUE, skew = TRUE)
#missing data?  there is an impute mean or median option; it's not multiple imputation, but will do in a pinch
```

At the level of the whole scale:

* alpha = .84 (exceeding the .80 thresshold)
* average interitem correlation = .25 (positive correlation between item and total scale score)

Items seem to be easiest on the reason scale (62-70% passing) and become increasingly difficult across the scales with rotate being the most difficult (only 19-30% passing). Item total correlations, though, remain strong and positive throughout ($r$ ranging from .48 to .59).

Because the items are working in the proper direction, we might not take time to conduct a distractor analysis.  If we did, it might be for the very difficult items on the rotate scale.

The sharpest skew is positive (1.62) and is one of the rotate items. 


```{r Significance of skew}
1.62/0.01
```

It's corresponding z-score is waaaaaay past 1.96, so we can presume that this item likely has an insufficient floor (i.e., cannot distinguish between the poorest performers).

Revelle demonstrates converting the multiple choice to a dichotomous right/wrong (think "true/false").  Not sure it gets us much, but here it is.

```{r Convert to True False}
iq.tf <- score.multiple.choice(iq.keys, iqitems, score = FALSE)
describe(iq.tf)
```

Distractor analysis can be continued with this IRT-esque function.  

"reason.4" is the first item.  From the key, we know that 4 is the correct answer, and we see that answering 4 is positively related to ability.

Revelle advises that good distractors are ones that are negatively related to ability.Thus, distractors 2 and 3 seem to have these qualities.

```{r Plot distractors, message = FALSE, warning = FALSE}
data(iqitems)
scores <- score.multiple.choice(iq.keys, iqitems, score = TRUE, short = FALSE)
op <- par(mfrow = c(2,2))#set this to see the output for multiple items
irt.responses(scores$scores, iqitems[1:16], breaks = 11)
```

## References
