% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={ReCentering Psych Stats: Psychometrics},
  pdfauthor={Lynette H Bikos, PhD, ABPP},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[shorthands=off,main=english]{babel}
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{ReCentering Psych Stats: Psychometrics}
\author{Lynette H Bikos, PhD, ABPP}
\date{}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{book-cover}{%
\chapter*{BOOK COVER}\label{book-cover}}
\addcontentsline{toc}{chapter}{BOOK COVER}

\begin{figure}
\centering
\includegraphics{images/ReCenterPsychStats-Psychometrics-bookcover.png}
\caption{An image of the book cover. It includes four quadrants of non-normal distributions representing gender, race/ethnicty, sustainability/global concerns, and journal articles}
\end{figure}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\textbf{If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the purpose teaching my classes and gaining formative feedback from a host of stakeholders. The document was last updated on 19 Sep 2021}. Emerging volumes on other statistics are posted on the \href{https://lhbikos.github.io/BikosRVT/ReCenter.html}{ReCentering Psych Stats} page at my research team's website.

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c932455e-ef06-444a-bdca-acf7012d759a}{Screencasted Lecture Link}

To \emph{center} a variable in regression means to set its value at zero and interpret all other values in relation to this reference point. Regarding race and gender, researchers often center male and White at zero. Further, it is typical that research vignettes in statistics textbooks are similarly seated in a White, Western (frequently U.S.), heteronormative, framework. The purpose of this project is to create a set of open educational resources (OER) appropriate for doctoral and post-doctoral training that contribute to a socially responsive pedagogy -- that is, it contributes to justice, equity, diversity, and inclusion.

Statistics training in doctoral programs are frequently taught with fee-for-use programs (e.g., SPSS/AMOS, SAS, MPlus) that may not be readily available to the post-doctoral professional. In recent years, there has been an increase and improvement in R packages (e.g., \emph{psych}, \emph{lavaan}) used for in analyses common to psychological research. Correspondingly, many graduate programs are transitioning to statistics training in R (free and open source). This is a challenge for post-doctoral psychologists who were trained with other software. This OER will offer statistics training with R and be freely available (specifically in a GitHub respository and posted through GitHub Pages) under a Creative Commons Attribution - Non Commercial - Share Alike license {[}CC BY-NC-SA 4.0{]}.

Training models for doctoral programs in HSP are commonly scholar-practitioner, scientist-practitioner, or clinical-scientist. An emerging model, the \emph{scientist-practitioner-advocacy} training model incorporates social justice advocacy so that graduates are equipped to recognize and address the sociocultural context of oppression and unjust distribution of resources and opportunities \citep{mallinckrodt_scientist-practitioner-advocate_2014}. In statistics textbooks, the use of research vignettes engages the learner around a tangible scenario for identifying independent variables, dependent variables, covariates, and potential mechanisms of change. Many students recall examples in Field's \citeyearpar{field_discovering_2012} popular statistics text: Viagra to teach one-way ANOVA, beer goggles for two-way ANOVA, and bushtucker for repeated measures. What if the research vignettes were more socially responsive?

In this OER, research vignettes will be from recently published articles where:

\begin{itemize}
\tightlist
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC{[}low-middle income countries{]}),
\item
  the research is responsive to issues of justice, equity, inclusion, diversity,
\item
  the lesson's statistic is used in the article, and
\item
  there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s); or it is publicly available.
\end{itemize}

In training for multicultural competence, the saying, ``A fish doesn't know that it's wet'' is often used to convey the notion that we are often unaware of our own cultural characteristics. In recent months and years, there has been an increased awakening to the institutional and systemic racism that our systems are perpetuating. Queuing from the water metaphor, I am hopeful that a text that is recentered in the ways I have described can contribute to \emph{changing the water} in higher education and in the profession of psychology.

\hypertarget{copyright-with-open-access}{%
\section*{Copyright with Open Access}\label{copyright-with-open-access}}
\addcontentsline{toc}{section}{Copyright with Open Access}

This book is published under a a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA.

A \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub open-source repository} contains all of the text and source code for the book, including data and images.

\hypertarget{acknowledgements}{%
\chapter*{ACKNOWLEDGEMENTS}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{ACKNOWLEDGEMENTS}

As a doctoral student at the University of Kansas (1992-2005), I learned that ``a foreign language'' was required for graduation. \emph{Please note that as one who studies the intersections of global, vocational, and sustainable psychology, I regret that I do not have language skills beyond English.} This could have been met with credit from high school my rural, mid-Missouri high school did not offer such classes. This requirement would have typically been met with courses taken during an undergraduate program -- but my non-teaching degree in the University of Missouri's School of Education was exempt from this. The requirement could have also been met with a computer language (fortran, C++) -- I did not have any of those either. There was a tiny footnote on my doctoral degree plan that indicated that a 2-credit course, ``SPSS for Windows'' would substitute for the language requirement. Given that it was taught by my one of my favorite professors, I readily signed up. As it turns out, Samuel B. Green, PhD, was using the course to draft chapters in the textbook \citep{green_using_2014} that has been so helpful for so many. Unfortunately, Drs. Green (1947 - 2018) and Salkind (2947 - 2017) are no longer with us. I have worn out numerous versions of their text. Another favorite text of mine was Dr.~Barbara Byrne's \citeyearpar{byrne_structural_2016}, ``Structural Equation Modeling with AMOS.'' I loved the way she worked through each problem and paired it with a published journal article, so that the user could see how the statistical evaluation fit within the larger project/article. I took my tea-stained text with me to a workshop she taught at APA and was proud of the signature she added to it (a little catfur might have fallen out). Dr.~Byrne created SEM texts for a number of statistical programs (e.g., LISREL, EQS, MPlus). As I was learning R, I wrote Dr.~Byrne, asking if she had an edition teaching SEM/CFA with R. She promptly wrote back, saying that she did not have the bandwidth to learn a new statistics package. We lost Dr.~Byrne in December 2020. I am so grateful to these role models for their contributions to my statistical training. I am also grateful for the doctoral students who have taken my courses and are continuing to provide input for how to improve the materials.

The inspiration for training materials that re*center statistics and research methods came from the \href{https://www.academics4blacklives.com/}{Academics for Black Survival and Wellness Initiative}. This project, co-founded by Della V. Mosley, Ph.D., and Pearis L. Bellamy, M.S., made clear the necessity and urgency for change in higher education and the profession of psychology.

At very practical levels, I am indebted to SPU's Library, and more specifically, SPU's Education, Technology, and Media Department. Assistant Dean for Instructional Design and Emerging Technologies, R. John Robertson, MSc, MCS, has offered unlimited consultation, support, and connection. Senior Instructional Designer in Graphics \& Illustrations, Dominic Wilkinson, designed the logo and bookcover. Psychology and Scholarly Communications Librarian, Kristin Hoffman, MLIS, has provided consultation on topics ranging from OERS to citations. I am alo indebted to Associate Vice President, Teaching and Learning at Kwantlen Polytechnic University, Rajiv Jhangiani, PhD. Dr.~Jhangiani's text \citeyearpar{jhangiani_research_2019} was the first OER I ever used and I was grateful for his encouraging conversation.

Financial support for this text has been provided from the \emph{Call to Action on Equity, Inclusion, Diversity, Justice, and Social Responsivity
Request for Proposals} grant from the Association of Psychology Postdoctoral and Internship Centers (2021-2022).

\hypertarget{ReCintro}{%
\chapter{Introduction}\label{ReCintro}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=cc9b7c0d-e5c3-4e4e-a469-acf7013ee761}{Screencasted Lecture Link}

\hypertarget{what-to-expect-in-each-chapter}{%
\section{What to expect in each chapter}\label{what-to-expect-in-each-chapter}}

This textbook is intended as \emph{applied,} in that a primary goal is to help the scientist-practitioner-advocate use a variety of statistics in research problems and \emph{writing them up} for a program evaluation, dissertation, or journal article. In support of that goal, I try to provide just enough conceptual information so that the researcher can select the appropriate statistic (i.e., distinguishing between when ANOVA is appropriate and when regression is appropriate) and assign variables to their proper role (e.g., covariate, moderator, mediator).

This conceptual approach does include occasional, step-by-step, \emph{hand-calculations} (only we calculate them arithmetically in R) to provide a \emph{visceral feeling} of what is happening within the statistical algorithm that may be invisible to the researcher. Additionally, the conceptual review includes a review of the assumptions about the characteristics of the data and research design that are required for the statistic. Statistics can be daunting, so I have worked hard to establish a \emph{workflow} through each analysis. When possible, I include a flowchart that is referenced frequently in each chapter and assists the the researcher keep track of their place in the many steps and choices that accompany even the simplest of analyses.

As with many statistics texts, each chapter includes a \emph{research vignette.} Somewhat unique to this resource is that the vignettes are selected from recently published articles. Each vignette is chosen with the intent to meet as many of the following criteria as possible:

\begin{itemize}
\tightlist
\item
  the statistic that is the focus of the chapter was properly used in the article,
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC {[}low middle income countries{]}),
\item
  the research has a justice, equity, inclusion, diversity, and social responsivity focus and will contribute positively to a social justice pedagogy, and
\item
  the data is available in a repository or there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s).
\end{itemize}

In each chapter we employ \emph{R} packages that will efficiently calculate the statistic and the dashboard of metrics (e.g., effect sizes, confidence intervals) that are typically reported in psychological science.

\hypertarget{strategies-for-accessing-and-using-this-oer}{%
\section{Strategies for Accessing and Using this OER}\label{strategies-for-accessing-and-using-this-oer}}

There are a number of ways you can access this resource. You may wish to try several strategies and then select which works best for you. I demonstrate these in the screencast that accompanies this chapter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simply follow along in the .html formatted document that is available on via GitHug Pages, and then

  \begin{itemize}
  \tightlist
  \item
    open a fresh .rmd file of your own, copying (or retyping) the script and running it
  \end{itemize}
\item
  Locate the original documents at the \href{https://github.com/lhbikos/ReC_Psychometrics}{GitHub repository} . You can

  \begin{itemize}
  \tightlist
  \item
    open them to simply take note of the ``behind the scenes'' script
  \item
    copy/download individual documents that are of interest to you
  \item
    fork a copy of the entire project to your own GitHub site and further download it (in its entirety) to your personal workspace. The \href{https://desktop.github.com/}{GitHub Desktop app} makes this easy!
  \end{itemize}
\item
  Listen to the accompanying lectures (I think sound best when the speed is 1.75). The lectures are being recorded in Panopto and should include the closed captioning.
\item
  Provide feedback to me! If you fork a copy to your own GitHub repository, you can

  \begin{itemize}
  \tightlist
  \item
    open up an editing tool and mark up the document with your edits,
  \item
    start a discussion by leaving comments/questions, and then
  \item
    sending them back to me by committing and saving. I get an e-mail notiying me of this action. I can then review (accepting or rejecting) them and, if a discussion is appropriate, reply back to you.
  \end{itemize}
\end{enumerate}

\hypertarget{if-you-are-new-to-r}{%
\section{If You are New to R}\label{if-you-are-new-to-r}}

R can be oveRwhelming. Jumping right into advanced statistics might not be the easiest way to start. However, in these chapters, I provide complete code for every step of the process, starting with uploading the data. To help explain what R script is doing, I sometimes write it in the chapter text; sometimes leave hastagged-comments in the chunks; and, particularly in the accompanying screencasted lectures, try to take time to narrate what the R script is doing.

I've found that, somewhere on the internet, there's almost always a solution to what I'm trying to do. I am frequently stuck and stumped and have spent hours searching the internet for even the tiniest of things. When you watch my videos, you may notice that in my R studio, there is a ``scRiptuRe'' file. I takes notes on the solutions and scripts here -- using keywords that are meaningful to me so that when I need to repeat the task, I can hopefully search my own prior solutions and find a fix or a hint.

\hypertarget{base-r}{%
\subsection{Base R}\label{base-r}}

The base program is free and is available here: \url{https://www.r-project.org/}

Because R is already on my machine (and because the instructions are sufficient), I will not walk through the instllation, but I will point out a few things.

\begin{itemize}
\tightlist
\item
  Follow the instructions for your operating system (Mac, Windows, Linux)
\item
  The ``cran'' (I think ``cranium'') is the \emph{Comprehensive R Archive Network.} In order for R to run on your computer, you have to choose a location. Because proximity is somewhat related to processing speed, select one that is geographically ``close to you.''
\item
  You will see the results of this download on your desktop (or elsewhere if you chose to not have it appear there) but you won't ever use R through this platform.
\end{itemize}

\hypertarget{r-studio}{%
\subsection{R Studio}\label{r-studio}}

\emph{R Studio} is the desktop application I work in R. It's a separate download. Choose the free, desktop, option that is appropriate for your operating system: \url{https://www.rstudio.com/products/RStudio/}

\begin{itemize}
\tightlist
\item
  Upper right window: Includes several tabs; we frequently monitor the

  \begin{itemize}
  \tightlist
  \item
    Environment: it lists the \emph{objects} that are available to you (e.g., dataframes)
  \end{itemize}
\item
  Lower right window: has a number of helpful tabs.

  \begin{itemize}
  \tightlist
  \item
    Files: Displays the file structure in your computer's environment. Make it a practice to (a) organize your work in small folders and (b) navigating to that small folder that is holding your project when you are working on it.
  \item
    Packages: Lists the packages that have been installed. If you navigate to it, you can see if it is ``on.'' You can also access information about the package (e.g., available functions, examples of script used with the package) in this menu. This information opens in the Help window.
  \item
    Viewer and Plots are helpful, later, when we can simultaneously look at our output and still work on our script.
  \end{itemize}
\item
  Primary window

  \begin{itemize}
  \tightlist
  \item
    R Studio runs in the background(in the console). Very occasionally, I can find useful troubleshooting information here.
  \item
    More commonly, I open my R Markdown document so that it takes the whole screen and I work directly, right here.
  \end{itemize}
\item
  \emph{R Markdown} is the way that many analysts write \emph{script}, conduct analyses, and even write up results. These are saved as .rmd files.

  \begin{itemize}
  \tightlist
  \item
    In R Studio, open an R Markdown document through File/New File/R Markdown
  \item
    Specify the details of your document (title, author, desired ouput)
  \item
    In a separate step, SAVE this document (File/Save{]} into a NEW FILE FOLDER that will contain anything else you need for your project (e.g., the data).
  \item
    \emph{Packages} are at the heart of working in R. Installing and activating packages require writing script.
  \end{itemize}
\end{itemize}

\hypertarget{r-hygiene}{%
\subsection{R Hygiene}\label{r-hygiene}}

Many initial problems in R can be solved with good R hygiene. Here are some suggestions for basic practices. It can be tempting to ``skip this.'' However, in the first few weeks of class, these are the solutions I am presenting to my students.

\hypertarget{everything-is-documented-in-the-.rmd-file}{%
\subsubsection{Everything is documented in the .rmd file}\label{everything-is-documented-in-the-.rmd-file}}

Although others do it differently, everything is in my .rmd file. That is, for uploading data and opening packages I write the code in my .rmd file. Why? Because when I read about what I did hours or years later, I have a permanent record of very critical things like (a) where my data is located, (b) what version I was using, and (c) what package was associated with the functions.

\hypertarget{file-organization}{%
\subsubsection{File organization}\label{file-organization}}

File organization is a critical key to this:

\begin{itemize}
\tightlist
\item
  Create a project file folder.
\item
  Put the data file in it.
\item
  Open an R Markdown file.
\item
  Save it in the same file folder.
\item
  When your data and .rmd files are in the same folder (not your desktop, but a shared folder), they can be connected.
\end{itemize}

\hypertarget{chunks}{%
\subsubsection{Chunks}\label{chunks}}

The R Markdown document is an incredible tool for integrating text, tables, and analyses. This entire OER is written in R Markdown. A central feature of this is ``chunks.''

The easiest way to insert a chunk is to use the INSERT/R command at the top of this editor box. You can also insert a chunk with the keyboard shortcut: CTRL/ALT/i

``Chunks'' start and end with with those three tic marks and will show up in a shaded box, like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#hashtags let me write comments to remind myself what I did}
\CommentTok{#here I am simply demonstrating arithmetic (but I would normally be running code)}
\DecValTok{2021} \OperatorTok{-}\StringTok{ }\DecValTok{1966}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 55
\end{verbatim}

Each chunk must open and close. If one or more of your tic marks get deleted, your chunk won't be read as such and your script will not run. The only thing in the chunks should be script for running R; you can hashtag-out script so it won't run.

Although unnecessary, you can add a brief title for the chunk in the opening row, after the ``r.'' These create something of a table of contents of all the chunks -- making it easier to find what you did. You can access them in the ``Chunks'' tab at the bottom left of R Studio. If you wish to knit a document, you cannot have identical chunk titles.

You can put almost anything you want in the space outside of tics. Syntax for simple formatting in the text areas (e.g,. using italics, making headings, bold, etc.) is found here: \url{https://rmarkdown.rstudio.com/authoring_basics.html}

\hypertarget{packages}{%
\subsubsection{Packages}\label{packages}}

As scientist-practitioners (and not coders), we will rely on \emph{packages} to do our work for us. At first you may feel overwhelmed about the large number of packages that are available. Soon, though, you will become accustomed to the ones most applicable to our work (e.g., psych, tidyverse, lavaan, apaTables).

Researchers treat packages differently. In these lectures, I list all the packages we will use in an opening chunk that asks R to check to see if the package is installed, and if not, installs it.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(psych))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: psych
\end{verbatim}

\begin{verbatim}
## Warning: package 'psych' was built under R version 4.0.5
\end{verbatim}

To make a package operable, you need to open it through the library. This process must be repeated each time you restart R. I don't open the package (through the ``library(package\_name)'') command until it is time to use it. Especially for new users, I think it's important to connect the functions with the specific packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install.packages ("psych")}
\KeywordTok{library}\NormalTok{ (psych)}
\end{Highlighting}
\end{Shaded}

If you type in your own ``install.packages'' code, hashtag it out once it's been installed. It is problematic to continue to re-run this code .

\hypertarget{knitting}{%
\subsubsection{Knitting}\label{knitting}}

An incredible feature of R Markdown is its capacity to \emph{knit} to HTML, powerpoint, or word. If you access the .rmd files for this OER, you can use annotate or revise them to suit your purposes. If you redistribute them, though, please honor the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License with a citation.

\hypertarget{troubleshooting-in-r-markdown}{%
\subsection{tRoubleshooting in R maRkdown}\label{troubleshooting-in-r-markdown}}

Hiccups are normal. Here are some ideas that I have found useful in getting unstuck.

\begin{itemize}
\tightlist
\item
  In an R script, you must have everything in order -- Every. Single. Time.

  \begin{itemize}
  \tightlist
  \item
    All the packages have to be in your library and activated; if you restart R, you need to reload each package.
  \item
    If you open an .rmd file and want a boxplot, you cannot just scroll down to that script. You need to run any \emph{prerequisite} script (like loading the package, importing data, putting the data in the global environment, etc.)
  \item
    Do you feel lost? clear your global environment (broom) and start at the top of the R script. Frequent, fresh starts are good.
  \end{itemize}
\item
  Your .rmd file and your data need to be stored in the same file folder. These should be separate for separate projects, no matter how small.
\item
  Type any warnings you get into a search engine. Odds are, you'll get some decent hints in a manner of seconds. Especially at first, these are common errors:

  \begin{itemize}
  \tightlist
  \item
    The package isn't loaded (if you restarted R, you need to reload your packages)
  \item
    The .rmd file has been saved yet, or isn't saved in the same folder as the data
  \item
    Errors of punctuation or spelling
  \end{itemize}
\item
  Restart R (it's quick -- not like restarting your computer)
\item
  If you receive an error indicating that a function isn't working or recognized, and you have loaded the package, type the name of the package in front of the function with two colons (e.g., psych::describe(df). If multiple packages are loaded with functions that have the same name, R can get confused.
\end{itemize}

\hypertarget{strategies-for-success}{%
\subsection{stRategies for success}\label{strategies-for-success}}

\begin{itemize}
\tightlist
\item
  Engage with R, but don't let it overwhelm you.

  \begin{itemize}
  \tightlist
  \item
    The \emph{mechanical is also the conceptual}. Especially when it is \emph{simpler}, do try to retype the script into your own .rmd file and run it. Track down the errors you are making and fix them.
  \item
    If this stresses you out, move to simply copying the code into the .rmd file and running it. If you continue to have errors, you may have violated one of the best practices above (Is the package loaded? Are the data and .rmd files in the same place? Is all the prerequisite script run?).
  \item
    Still overwhelmed? Keep moving forward by downloading a copy of the .rmd file that accompanies any given chapter and just ``run it along'' with the lecture. Spend your mental power trying to understand what each piece does. Then select a practice problem that is appropriate for your next level of growth.
  \end{itemize}
\item
  Copy script that works elsewhere and replace it with your datafile, variables, etc.\\
\item
  The leaRning curve is steep, but not impossible. Gladwell\citeyearpar{gladwell_outliers_2008} reminds us that it takes about 10,000 hours to get GREAT at something (2,000 to get reasonably competent). Practice. Practice. Practice.
\item
  Updates to R, R Studio, and the packages are NECESSARY, but can also be problematic. It could very well be that updates cause programs/script to fail (e.g., ``X has been deprecated for version X.XX''). Moreover, this very well could have happened between my distribution of these resources and your attempt to use it. My personal practice is to update R, R Studio, and the packages a week or two before each academic term.
\item
  Embrace your downward dog. Also, walk away, then come back.
\end{itemize}

\hypertarget{resources-for-getting-started}{%
\subsection{Resources for getting staRted}\label{resources-for-getting-started}}

R for Data Science: \url{https://r4ds.had.co.nz/}

R Cookbook: \url{http://shop.oreilly.com/product/9780596809164.do}

R Markdown homepage with tutorials: \url{https://rmarkdown.rstudio.com/index.html}

R has cheatsheets for everything, here's one for R Markdown: \url{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}

R Markdown Reference guide: \url{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}

Using R Markdown for writing reproducible scientific papers: \url{https://libscie.github.io/rmarkdown-workshop/handout.html}

LaTeX equation editor: \url{https://www.codecogs.com/latex/eqneditor.php}

\hypertarget{QuestCon}{%
\chapter{Questionnaire Construction: The Fundamentals}\label{QuestCon}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=ec7c8795-a0af-4f23-9145-ad9a00355e3c}{Screencasted Lecture Link}

I found this lesson to be more of a struggle to prepare than I expected. Why? There is a great deal of lore about what increases response rates and participation. Yet, research over the years had both supported and contradicted these claims. One example is where to include ``sensitive items.'' Historically, textbook authors have recommended that these should come last so that respondents would be engaged in the process and be more willing to complete the survey \citep{krathwohl_methods_2009, rowley_designing_2014}. Yet, research has shown that this has not held up in employee groups \citep{roberson_questionnaire_1990} nor among members of the National Association of Social Workers \citep{robert_g._green_should_2000}.

Given these contradictions, this lecture starts with the overall structure of a survey. The core of the lecture focuses on recent, evidence-based support for item-level decisions. I briefly discuss construct-specific guidance and discuss specific considerations for the on-line environment. I then close by addressing some of the decisions that I routinely make in survey construction and provide my rationale for why. Because this lesson occurs at the beginning of a text on psychometrics -- this ``skips over and around'' the issue of reliability and validity.

\hypertarget{navigating-this-lesson}{%
\section{Navigating this Lesson}\label{navigating-this-lesson}}

There is just under one hour of lecture.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives}{%
\subsection{Learning Objectives}\label{learning-objectives}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Outline the overall structure/components of a questionnaire,
\item
  Articulate test construction myths (e.g., location of sensitive items, ``requirement'' to have reverse scored items) and their evidence-based solutions (when they have them)
\item
  List elements to consider when the questionnaire is administered online
\end{itemize}

\hypertarget{planning-for-practice}{%
\subsection{Planning for Practice}\label{planning-for-practice}}

This is a two-part lesson on questionnaire construction. After the second lesson, a detailed suggestion for practice will be provided that lists criteria for creating and piloting a survey of your own.

\hypertarget{readings-resources}{%
\subsection{Readings \& Resources}\label{readings-resources}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Chyung, S. Y., Roberts, K., Swanson, I., \& Hankinson, A. (2017). Evidence-Based Survey Design: The Use of a Midpoint on the Likert Scale. Performance Improvement, 56(10), 15--23. \url{https://doi.org/10.1002/pfi.21727}
\item
  Chyung, S. Y., Barkin, J. R., \& Shamsy, J. A. (2018a). Evidence‐Based Survey Design: The Use of Negatively Worded Items in Surveys. Performance Improvement, 57(3), 16--25. \url{https://doi.org/10.1002/pfi.21749}
\item
  Chung, S. Y., Kennedy, M., \& Campbell, I (2018b). Evidence-based survey design: The use of ascending or descending order of Likert-type response options. Performance Improvement, 57(9), 9-16. \url{https://doi.org/10.1002/pfi.21800}
\item
  Chyung, S. Y., Swanson, I., Roberts, K., \& Hankinson, A. (2018c). Evidence‐Based Survey Design: The Use of Continuous Rating Scales in Surveys. Performance Improvement, 57(5), 38--48. \url{https://doi.org/10.1002/pfi.21763}

  \begin{itemize}
  \tightlist
  \item
    Finding the Chyung et al.~series was like finding a pot of gold! They provide empirical support for guiding choices about survey construction. And they are current! If you don't have time to read them in detail, I recommend you scan them and archive them for future reference.
  \end{itemize}
\end{itemize}

\hypertarget{components-of-the-questionnaire}{%
\section{Components of the Questionnaire}\label{components-of-the-questionnaire}}

Let's start by examining the components of a questionnaire and the general guidelines for their construction\citep{colton_designing_2015, pershing_ineffective_2001}:

\textbf{Title}

\begin{itemize}
\tightlist
\item
  reflect the content of the instrument
\item
  be concisely worded
\item
  be written in language easily understood by the respondents
\item
  should not be offensive or off-putting
\item
  should be formatted clearly at the top/beginning of the document
\end{itemize}

\textbf{Introductory Statement}

\begin{itemize}
\tightlist
\item
  include a brief summary of the instrument's purpose
\item
  contain an appropriate statement concerning the confidentiality of the respondent's information (informed consent)
\item
  be motivating such that respondents are inspired/willing to complete the items
\item
  specify the approximate amount of time required to complete the instrument
\end{itemize}

\textbf{Directions}

\begin{itemize}
\tightlist
\item
  complete, unambiguous, concise
\item
  written at a language level appropriate to the respondents
\item
  tell the respondents how to return the instrument once they have completed it (surprisingly, in Qualtrics, this is also important; submission requires hitting that last little ``--\textgreater\textgreater{}'')
\end{itemize}

\textbf{Items} (discussed in a later section)

\textbf{Closing Statement}

\begin{itemize}
\tightlist
\item
  thank the participants for their participation
\item
  remind participants that their information is valuable and perhaps remind about

  \begin{itemize}
  \tightlist
  \item
    next steps or follow-up
  \item
    confidentiality
  \end{itemize}
\end{itemize}

\textbf{Overall Structure/Look}

\begin{itemize}
\tightlist
\item
  should be coherent with an easy-to-follow layout
\item
  not crowded
\item
  professional appearance

  \begin{itemize}
  \tightlist
  \item
    not crowded, plenty of white space
  \item
    avoiding a ``slick look''
  \item
    numbering and headings to provide a sense of progress
  \item
    breaks between every 4-6 questions (or shading alternate items)
  \item
    in a sense, inviting and ``easy on the eye''
  \end{itemize}
\end{itemize}

Pershing and Pershing \citeyearpar{pershing_ineffective_2001} reviewed 50 \emph{reactionnaires} that were used by training evaluators at a ``prestigious medical school.'' Their purpose was to determine the degree to which the survey design adhered to the recommendations. The results suggested that:

\begin{itemize}
\tightlist
\item
  72\% did not include an introductory statement; an additional 16\% were ``minimal''
\item
  78\% had no closing statement
\item
  30\% had no directions; another 54\% of directions were ``minimal''
\item
  8\% were professional in appearance
\end{itemize}

In summary, the formatting of the reactionnaires were not designed in a way that would maximize respondent engagement. In turn, we might expect this to threaten the psychometric reliability and validity.

\hypertarget{what-improves-or-threatens-response-rates-and-bias}{%
\section{What Improves (or Threatens) Response Rates and Bias?}\label{what-improves-or-threatens-response-rates-and-bias}}

It's not always clear, but well-designed studies and formal reviews of literature are revealing, sometimes surprising findings to old debates. When we design survey instruments based on our own preference rather than research-based evidence, we may get less than optimal data \citep{chyung_evidence-based_2018}.

Chyung et al.~(2018) reviewed the five steps \citep{schwarz_asking_2001} that survey respondents engage when answering structured, closed-ended survey items.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Interpreting the question
\item
  Retrieving information from their memory
\item
  Integrating the information
\item
  Selecting one of the given response options
\item
  Editing the answer for reasons of social desirability
\end{enumerate}

Chyung and colleagues appear to be starting such a systematic review. What follows are their evidence based evaluations regarding some of the most common questions about questionnaire construction.

\hypertarget{should-likert-type-scales-include-a-midpoint}{%
\subsection{Should Likert-type scales include a midpoint?}\label{should-likert-type-scales-include-a-midpoint}}

Chyung et al.~\citeyearpar{chyung_evidence-based_2017} reviewed the literature on whether/not to use a \textbf{midpoint} (``neutral'' or ``neither disagree nor agree''). Examining their article, we can see variants of Likert-style scaling for a scale of agreement. They look something like this:

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.16\columnwidth}\raggedright
Type\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.16\columnwidth}\raggedright
No midpoint (4 pt)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Strongly Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\emph{skipped}\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
Agree\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
Strongly Agree\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.16\columnwidth}\raggedright
Midpoint (5 pt)\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Strongly Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
Neither Disagree Nor Agree\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright
Agree\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
Strongly Agree\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Chyung and colleagues quickly suggest that the question is not ``Should I use a midpoint?'' but rather ``When should I use a midpoint?''

The article is more detailed, but essentially, a midpoint is appropriate when:

\begin{itemize}
\tightlist
\item
  the measurement scale is interval (instead of ordinal; this is a good statistical property to have)
\item
  the question content is such that the midpoint is a \emph{true} midpoint and not a point for hedging or avoiding
\end{itemize}

If a true midpoint is impossible, then consider adding an option such as ``I don't know'' or ``It depends.'' Of course these introduce scoring dilemmas (e.g., needing to be recoded as missing; then having a plan for managing missingness) that will need to be addressed in preparing the data for analysis.

\hypertarget{should-continuous-rating-scales-be-used-in-surveys}{%
\subsection{\texorpdfstring{Should \emph{continuous rating scales} be used in surveys?}{Should continuous rating scales be used in surveys?}}\label{should-continuous-rating-scales-be-used-in-surveys}}

First, let's consider the distinction between \emph{discrete}, \emph{continuous}, and \emph{numerical} scales. Figure 4 in the Chyung, Swanson, Roberts, and Hankinson \citeyearpar{chyung_evidencebased_2018-1} article illustrate the major differences and some variations.

\begin{itemize}
\tightlist
\item
  \textbf{Discrete} scales are Likert-type scales that range range between 2 and 11 \emph{discrete} options. Clasically, respondents pick \emph{words} (e.g., pain rated as \emph{no pain}, \emph{mild}, \emph{moderate}, \emph{severe}, \emph{extreme}, \emph{worst pain possible}).

  \begin{itemize}
  \tightlist
  \item
    6-point discrete rating scales result in a collection of six \emph{ordered values}
  \item
    thus they are on the ordinal measurement scale and (as discussed above)
  \item
    they should be analyzed with non-parametric statistical procedures (parametric approaches can be used if the data are normally distributed and there is a mid-point)
  \end{itemize}
\item
  \textbf{Continuous} scales allow respondents to indicate a response anywhere within a given range -- usually by marking a place on a horizontal line on a continuum of a minimum of 100 points. There are no discrete categories defined by words or numbers. In Qualtrics there is a slider option.

  \begin{itemize}
  \tightlist
  \item
    Continuous scales result in precise numbers (e.g., 26 or 26.8 if the scale is 0 to 100)
  \item
    these are on an interval-level measurement scale AND
  \item
    they can be evaluated with parametric statistical analyses
  \item
    \emph{visual analog scales (VAS; aka graphic rating scales, GRS)} are another variant of continuous rating scales if they allow the participants to make ``anywhere on the line.'' Some VAS scales have verbal descriptors to guide the marking; some have numbers (hence, \emph{numerical response scales})
  \end{itemize}
\end{itemize}

Which is better? The mixed results are summarized in Chyung et al's \citeyearpar{chyung_evidencebased_2018-1} Table 1. With a focus on the types of research I encounter in my program, here is my take-away:

\begin{itemize}
\tightlist
\item
  Continuous scales provide better data (more precise/full information, more likely to be normally distributed, better reliability) for statistical analysis.

  \begin{itemize}
  \tightlist
  \item
    \emph{Caveat:} If the response scale on a Likert scale is increased to 11, there is a better chance to have normally distributed responses.
  \item
    \emph{Caveat:} When ``simple descriptives'' are desired (e.g., histograms, frequency distributions) the discrete scale may be the best choice.
  \end{itemize}
\item
  Both are easy to use, except in the case where respondents complete the surveys on mobile devices.

  \begin{itemize}
  \tightlist
  \item
    \emph{Caveat:} there has been more missing data with sliders (compared to radio buttons)
  \item
    \emph{Caveat:} respondents are more likely to change their responses on sliders. If this means there is greater accuracy or more careful responding, this is desirable.
  \end{itemize}
\item
  In both circumstances adding ``don't know,'' ``prefer not to respond,'' or ``not applicable'' may improve the validity of the responses
\end{itemize}

\hypertarget{should-likert-type-response-options-use-an-ascending-or-descending-order}{%
\subsection{Should Likert-type response options use an ascending or descending order?}\label{should-likert-type-response-options-use-an-ascending-or-descending-order}}

Let's first look at the difference \citep{chyung_evidence-based_2018}:

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.13\columnwidth}\raggedright
Type\strut
\end{minipage} & \begin{minipage}[b]{0.16\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.13\columnwidth}\raggedright
Ascending\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Strongly Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Neither Disagree Nor Agree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Agree\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
Strongly Agree\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.13\columnwidth}\raggedright
Descending\strut
\end{minipage} & \begin{minipage}[t]{0.16\columnwidth}\raggedright
Strongly Agree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Agree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Neither Agree Nor Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright
Disagree\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
Strongly Disagree\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In the consideration of the choice between ascending/descending, we are concerned with \emph{response-order effects}. Let's first examine these conceptually/theoretically.

\textbf{Recency effect} is the tendency of survey respondents to select the options that they see at the end of the response-option list. This is expected when options are presented orally (e.g., during interviews, people tend to choose from the last-offered options)

\textbf{Primacy effect} is the survey respondents' tendency to select the options that are presented at the beginning of the
response-option list. This is expected when options are presented visually. For example, people tend to choose among the first-presented categories in self-administered written survey questionnaires.

\begin{itemize}
\tightlist
\item
  \emph{left-sided selection bias} occurs when respondents read text from left-to-right and are more inclined to select options from the left.
\item
  \emph{satisficing theory} occurs when individuals seek solutions that are ``simply satisfactory'' so as to minimize psychological costs. Thus, respondents may

  \begin{itemize}
  \tightlist
  \item
    select the first option that seems ``reasonable enough'',
  \item
    select the ``I don't know'' response, or
  \item
    randomly select one of the options.
  \end{itemize}
\item
  \emph{acquiesence bias} is the tendency for respondents to agree with the statement provided---aka yea-saying bias (e.g., being polite).

  \begin{itemize}
  \tightlist
  \item
    Closely related is \emph{social-desirability bias,} the tendency for respondents to select among the options they think are more socially acceptable or desirable (instead of true responses).
  \item
    In surveys, this generally is selecting \emph{agree} or \emph{strongly agree}.
  \end{itemize}
\end{itemize}

Considering these response biases together, Chyung et al.~suggest that when the response options are presented in descending order (\emph{Strongly agree, Agree, Neutral, Disagree, Strongly disagree}), respondents would (theoretically) see a positive option immediately on the left side of the response scale and perceive it to be socially desirable and satisfactory, resulting in their decision to select it without having to spend more time to choose a true response. However, the same effects may or may not happen when the response options are presented in ascending order (\emph{Strongly disagree, Disagree, Neutral, Agree, Strongly agree}).

After reviewing 13 studies, Chyung et al.~observed that many studies (paper and web based, with children and adults, in English and other language):

\begin{itemize}
\tightlist
\item
  revealed response-order effects in self administered surveys, especially the primacy effect, associated with left-side selection bias, acquiescence bias, and satisficing.
\item
  showed more positive average scores from descending-ordered scales
\end{itemize}

Recommendations:

\begin{itemize}
\tightlist
\item
  present response scales in ascending order

  \begin{itemize}
  \tightlist
  \item
    when a number line is used, lower and negative numbers should be on the left
  \end{itemize}
\item
  when using descended order scales

  \begin{itemize}
  \tightlist
  \item
    keep respondents motivated to complete items accurately
  \item
    present half items with descended-ordered scales and the other half with ascended-ordered scales
  \item
    assign half of participants with descended-ordered scales; half with ascended-ordered scales
  \item
    present response options vertically rather than horizontally
  \end{itemize}
\end{itemize}

\hypertarget{should-surveys-include-negatively-worded-items}{%
\subsection{Should surveys include negatively worded items?}\label{should-surveys-include-negatively-worded-items}}

In examining this question, Chyung et al.~\citep{chyung_evidencebased_2018} made a distinction between (see Table 1 in the article):

\begin{itemize}
\tightlist
\item
  \textbf{Statement format} the same response scale such as a Likert scale
\item
  \textbf{Question format} different response scales that are tailored to individual survey questions. A challenge with this format is the difficulty in calculating an average score of data obtained from multiple survey items.
\end{itemize}

The advent of negatively-worded items began with Rensis Likert in 1932. He was an American social psychologist who, in attempt to mitigate aqcuiescence/yea-saying biases, recommended designing one half of survey items to be associated with agreement and the other half with disagreement. Although Likert recommended ``straightforward statements,'' incorporating negative words can become quickly complicated. Table 2 in the Chyung paper shows that there are four ways of wording survey statements:

\textbf{Reverse-coding}, which is necessary when including negatively worded items in a scale, assumes that agreeing to a positively worded statement and disagreeing to its negatively worded counterpart are the same. Tables 3 and 4 in the ms show how this assumption may be faulty.

A review of the negatively-worded-item literature suggested the following:

\begin{itemize}
\tightlist
\item
  scales with all positively worded items yielded greater accuracy when compared with all negatively worded items or mixed worded items
\item
  scores on positively and negatively worded items are not the same (e.g., strongly disagreeing to a positively worded statement is differenf from strongly agreeing to a negatively worded statement)
\item
  positively worded items produce higher means than negatively worded items. This may be due to

  \begin{itemize}
  \tightlist
  \item
    carelessness and fatigue in reading items
  \item
    the cognitive processing of postive and negative items may be different
  \end{itemize}
\item
  a \emph{method factor} has shown itself where in factor analyses of scales with factor structures unrelated to the wording, exploratory approaches to factor analysis have produced separate factors with the negatively worded (or otherwise ambiguous) items, this results in a threat to construct validity and reliability.
\end{itemize}

Chyung, Barkin, and Ramsey \citeyearpar{chyung_evidencebased_2018} noted that resondent performance declines approximately 12 minutes after respondents start a survey. This relates to negatively worded items because it appeared that respondents increasingly failed to notice negatively worded statements even when there were efforts to draw their attention to them via bolding, underlining, or capitalizing the negated element (e.g., \emph{not}). Thus, when negatively worded items are used, they should probably be presented early in the protocol.

Chyung et al \citeyearpar{chyung_evidencebased_2018} also cautioned about a response set bias that can occur when using all positively worded items. They recommended making design choices that enhance bias-free and accurate responding based on the research design.

\begin{itemize}
\tightlist
\item
  For example, attributes to be measured in some constructs (e.g., depression, anxiety) are, themselves, negative and so a negatively worded item may be most clear and appropriate.
\item
  The inclusion (and subsequent analysis) of negatively phrased items may help \emph{detect} acquiesence bias.
\item
  Table 5 in the manuscript provides some guidelines that are more nuanced when negative items must be included. For example,

  \begin{itemize}
  \tightlist
  \item
    ensure that negatively worded items are true polar opposites and symmetrical (so they can be analyzed with the positively worded items)
  \item
    group negative items together (and forewarn/format so they are recognized as such)
  \item
    administer the survey when respondents are not fatigued
  \item
    analyze the effect of the negatively worded items
  \end{itemize}
\end{itemize}

\hypertarget{construct-specific-guidance}{%
\section{Construct-specific guidance}\label{construct-specific-guidance}}

Self-efficacy is domain-specific construct. That is, even though there are some \emph{general self-efficacy scales} Bandura's original definition suggests that it should be task specific (i.e., career decision-making self-efficacy, math self-efficacy).

This construct is an example where Bandura, himself \citeyearpar{bandura_guide_2006}, provided specific guidelines for creating these task-specific assessments. Generally they included:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  phrasing items as ``can do'' rather than ``will do,''
\item
  maintaining consistency with the self-efficacy construct definition (e.g., domain specific, a focus on capability rather than self-worth),
\item
  including items that reflect gradations of challenge,
\item
  asking individuals to rate their current (as opposed to future) operative capabilities
\item
  scaling should be on a 100 point scale
\end{enumerate}

\hypertarget{surveying-in-the-online-environment}{%
\section{Surveying in the Online Environment}\label{surveying-in-the-online-environment}}

Nearly a decade ago, a survey of human subjects review boards suggested that 94\% of the IRB applications reviewed involved online or Web-based surveys \citep{buchanan_online_2009}. Thus, it is important to give some specifical conceptual consideration to the online environment.

A first set of considerations involve data security, identity, and permission (implicit and explicit).

The \textbf{IP address} as well as \textbf{longitude/latitude} has been a contentious issue for a number of years \citep{buchanan_online_2009}. EU data protection laws consider IP addresses as personally identifiable data; in the U.S., IP addresses typically fall outside the definition of ``personal information.'' In Qualtrics, the default is to collect and download the IP address (the ``anonymize response'' option can prevent this data from being collected). On the one hand it is helpful to know geographically ``from where'' participants are responding; on the other, some consider its capture to be a violation of privacy.

Relatedly, what is considered to be \textbf{fully informed consent} \citep{conrad_survey_2007}. Is it ethical to capture paradata (e.g., typing speed, changed answers, response times) or metadata without explicitly saying so?

\textbf{The tool} being used to collect the data is concerning. Buchanan and Hvizdak \citeyearpar{buchanan_online_2009} argued that until each tool is vetted and its privacy policies and data security policies are understood, we cannot be 100\% certain how security, consent, and privacy are instantiated within the individual tools. For example, it is possible that tool creators \emph{could} gather respondent data and repurpose it for their own marketing, for sale to other researchers, and so forth

Online and web-based protocols increase our reach geographically and cross-culturally. For now, I will bracket out the issue of cultural translation, for the purpose of online the question is about \textbf{access} \citep{conrad_survey_2007}. Think about the decades of psychological research based on White, college-educated, males. Are we creating another strata of privileged research with technology that may not be accessible in terms of both internet/technology as well as capacity/fluency with the tool? On the other hand, what are the risks of not adopting new technologies before everyone has them.

When paper/pencil measures were administered in face-to-face settings (individually or in auditoriums of students) there was some degree of \textbf{standardized protocol.} This is lost when surveys are administered online and we cannot guarantee \emph{who} is taking the survey. To what degree does this matter?

When respondents are remote, what happens if they have a \textbf{negative reaction to the survey}? In a face-to-face context, debriefings can occur and referrals can be made. How is this managed remotely?

\textbf{Security of test items} might also be concerning. It is not ok to use propietary items without the permission of the testmaker. If the security of items is important (e.g., SAT/GRE, intelligence test items, inkblots) because they are central to administration, how can this be protected in the virtual environment?

Consequently, when students in our programs write doctoral dissertations they are to narrate the following in their Method section.

\begin{itemize}
\tightlist
\item
  Describe how informed consent will be obtained in the online environment.
\item
  Describe the level of identification that is collected. If the claim of ``anonymous'' or ``de-identified'' indicate whether/not this includes capturing the IP address; some researchers believe that capturing a computer's IP address threatens anonymity.
\item
  Describe the steps to be taken to ensure that respondents met the inclusion/exclusion criteria of the study.
\item
  Anticipate and describe how the online (e.g., uncontrolled, public, distractions) setting might affect responses.
\item
  Particularly if the survey contained sensitive materials, describe how respondents might access resources for debriefing or referral.
\item
  Identify the permissions (from original authors or copyright holders) granted to reformat and post (on the internet) existing surveys. If items are considered to be secure (e.g., those on the MMPI or WAIS), identify steps taken to protect them.
\end{itemize}

\hypertarget{in-my-surveys}{%
\section{In my Surveys}\label{in-my-surveys}}

Because there isn't empirical data on every decision that we make in survey construction, I thought it might be useful for me to address some of the decisions that I find myself making in the online surveys I use in my own research.

\hypertarget{demographics-and-background-information}{%
\subsection{Demographics and Background Information}\label{demographics-and-background-information}}

A core value that I hope to reflect in the \emph{ReCentering Psych Stats} series is to promote socially and culturally responsive research. Correspondingly, the information we collect from participants should ensure that they feel that their identities are authentically reflected in the survey. Naively, when I first considered how to capture race/ethnicity in my surveys, I looked to the categories used in the U.S. Census. Immediately, I learned that this is problematic. Rather than elaborating here, I invite you to take a listen to NPR's \href{https://www.npr.org/podcasts/510312/codeswitch}{Code Switch} podcast had a series of episodes that included a discussion of the evolution of the race and ethnicity question. Two of the episodes review how the race/ethnicity question is used and and why it is problematic: \href{https://www.npr.org/transcripts/607553683}{Census Watch 2020} and \href{https://www.npr.org/transcripts/540671012}{The U.S. Census and Our Sense of Us}. As made clear in the Code Switch podcasts, the race/ethnicity question in the U.S. Census \emph{erases} people when their identities are not included.

My last few surveys have captured race/ethnicity data differently. Each time, I engage in several practices that (I hope) will continue to shape the item in a socially and culturally responsive way. Systematically, I:

\begin{itemize}
\tightlist
\item
  conduct a quick internet search to see if there is an emerging best practice (even though I may have also searched weeks or months prior),
\item
  consider who the intended research population is in relationship to the topic of investigation,
\item
  look to recently published, similar, research to see what other researchers are doing, and
\item
  ask for a colleagial, formative review from individuals who hold marginalized identities, whose data will be requested in the survey.
\end{itemize}

When I engage in research, I try to balance the need to quantify (with discrete categories) who is participating in the survey and inviting respondents to state (in their own words) their identity. This is consistent with my view that variables like race, ethnicity, and gender identity as socially constructed. In addition to this particular worldview, Parent \citeyearpar{parent_handling_2013} has suggested that the worst possible kind of missing data pattern (MNAR -- missing not at random) may be caused when items are \emph{unanswerable} to particular person. Therefore, it is essential that all individuals find their place in the items that assess demographic variables.

My last survey was directed toward community members (including students, alumni, staff, faculty) of my predominantly White, liberal arts, Christian institution. After reviewing recently published research articles and consulting with a handful of individuals, I chose to include the following categories -- \emph{each with a text write-in box} so that individuals could select the category(ies) that fit best and have the opportunity to refine it(them). I am excited to review this data because such responses may inform my next survey. The categories included:

\begin{itemize}
\tightlist
\item
  Asian or Pacific Islander
\item
  Black or African American
\item
  Hispanic or Latino
\item
  Native American or Alaskan Native
\item
  White or Caucasian
\item
  Biracial or multiracial
\item
  An international/global identity that does not fit in the U.S. categorization of race/ethnicity
\item
  A race/ethnicity not listed above
\end{itemize}

Note that individuals could select multiplecategories

\begin{figure}
\centering
\includegraphics{images/QuestCon/RaceEthnicity.jpg}
\caption{Image of a survey item inquiring about race/ethnicity from a survey. Each option has an option for the respondent to clarify.}
\end{figure}

The option to select multiple boxes will result in some extra coding when we prepare the data for analysis. I am taking approach that we will \emph{listen} to the data and decide, based on the results, how to report the findings in a way that will efficiently fit into an APA style empirical paper and honor the participants.

The population of interest for this particular study are those who are engaged in protest activities regarding hiring practices and policies that result in discrimination to members of the LGBTQIA+ community. This means that questions of gender identity, pronouns, and relationship to the LGBTQIA+ community are important to the research, but needed to be asked sensitively and with great security of the data.

Regarding gender identity, I used a similar approach to the race/ethnicity question, allowing individuals to select multiple categories and offering write-in boxes for each. The categories included:

\begin{itemize}
\tightlist
\item
  female
\item
  male
\item
  nonbinary
\item
  transwoman
\item
  transman
\item
  something else
\item
  prefer not to say
\end{itemize}

Additionally, I invited individuals to identify their pronouns. Again, write-in boxes were offered with each option.

\begin{itemize}
\tightlist
\item
  they/them/theirs
\item
  she/her/hers
\item
  he/him/his
\item
  they/she/he
\item
  neo pronouns (e.g., xe/xem/xyr, ze/hir/hirs, ey/em/eir)
\item
  something else
\end{itemize}

Finally, we wanted individuals to indicate their relationship to the LGBTQIA+ community. We asked them to select all that apply. Only the ``something else'' box had a write-in option:

\begin{itemize}
\tightlist
\item
  Member
\item
  Exploring/Questioning
\item
  Ally
\item
  Not related
\item
  Something else (with a write-in box)
\end{itemize}

I expect that my future surveys may inquire about these variables differently. If you have found a different way to ask, please fconsider e-mailing me. I would love to provide different options and give credit to contributors.

\hypertarget{survey-order}{%
\subsection{Survey Order}\label{survey-order}}

Historically, demographic information has been first or last in the survey. Although some research has reported no differences in response rates when demographic and sensitive data are at the beginning or end \citep{krathwohl_methods_2009, rowley_designing_2014}, I am inclined to open the survey with questionnaire items that are closely related to the topic listed on the recruitment materials and end the survey with the demographic information. Why? It makes sense to me that if someone has responded positively to the survey topic, they expect to answer questions (right away) about that topic.

In between that opening survey and closing demographic items, I consider if there are any \emph{order effects} that would engage in undue \emph{priming} of responses. If there are such concerns, I think through the order to minimize them. If there are no such concerns, I put my surveys in blocks and then ask my survey program to randomly present the blocks. This serves two purposes:

\begin{itemize}
\tightlist
\item
  counterbalancing possible order effects, and
\item
  distributing missingness for individuals who do not complete the survey.
\end{itemize}

\hypertarget{forced-responses}{%
\subsection{Forced Responses}\label{forced-responses}}

Programs like Qualtrics are able to engage in a variety of \emph{content validation} procedures. If these are in place, they may require the person to enter a properly formatted response (e.g., phone number, e-mail address, numerical response between 0 and 100) before responding. These are extremely helpful tools in collecting data that will be closest-to-being-ready-for-analysis. These same procedures can \emph{force} or \emph{request} a response.

\emph{Requiring} a response is tempting. However, doing so violate IRB requirements that allow a person to skip or ``quit at any time without penalty.'' They may also anger a person such that they stop responding. Some researchers get around this by \emph{requiring} the response, but including a ``Not applicable'' or ``Prefer to not answer'' column. Because I worry that (a) the respondent may confuse that option with one extreme of the scale and/or (b) my research team and I will forget to code it as missing data, I prefer the \emph{request} response alternative.

In Qualtrics in particular, I turn on the ``Request response'' feature for each of the questions. If an item is skipped, a simple warning is displayed that invites the respondent to review the page of answers to see if they would like to answer the question. If not, they can simply move forward.

\hypertarget{practice-problems}{%
\section{Practice Problems}\label{practice-problems}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to:

This is a two-part lesson on questionnaire construction. After the \protect\hyperlink{qualTRIX}{second lesson}, a detailed suggestion for practice will be provided that lists criteria for creating and piloting a survey of your own.

\hypertarget{qualTRIX}{%
\chapter{Be a QualTRIXter}\label{qualTRIX}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=422daef0-a158-48b3-a69d-ad9a00202939}{Screencasted Lecture Link}

The focus of this lecture is on the technical and mechanical tools available in Qualtrics (and likely other survey platforms) to increase the effectiveness of your survey.

\hypertarget{navigating-this-lesson-1}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-1}}

This lecture is just under one hour. Plan for another 30 minutes for \emph{intRavenous qualtRics} practice.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-1}{%
\subsection{Learning Objectives}\label{learning-objectives-1}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Utilize basic Qualtrics tools (e.g,. question type, use of headers) so that surveys are present materials clearly to the respondent.
\item
  Incorporate more advanced tools (e.g., display logic, randomization) that may increase the respondent's ability to complete the survey and provide accurate responses.
\item
  Provide a rationale for survey options that protect (or possibly reveal) an individual's identity.
\end{itemize}

\hypertarget{planning-for-practice-1}{%
\subsection{Planning for Practice}\label{planning-for-practice-1}}

This is the second of a two-part lesson on questionnaire construction. At the end of this lesson is a detailed suggestion for practice that lists criteria for creating and piloting a survey of your own. There are four essential criteria for your survey:

\begin{itemize}
\tightlist
\item
  Adhere to the evidence-based practices identified in the lesson on \protect\hyperlink{QuestCon}{questionnaire construction}.
\item
  Utilize four techniques (in the context of Qualtrics, I term these \emph{qualTRIXter skills}) that increase the flow, effectiveness, and appearance of your survey.
\item
  Pilot and consider feedback provided by those who took the survey.
\item
  Import the data into the R environment.
\end{itemize}

\hypertarget{readings-resources-1}{%
\subsection{Readings \& Resources}\label{readings-resources-1}}

In preparing this chapter, I drew heavily from the tutorials available at the \href{https://www.qualtrics.com/support/}{Qualtrics support site}. I have tried to link them throughout the presentation. It is likely they could change at any time and/or they might not work on your particular browser (as I write this, half of them will not work on FireFox, but they do on Chrome and Edge).

\hypertarget{packages-1}{%
\subsection{Packages}\label{packages-1}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(qualtRics))\{install.packages("qualtRics")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette}{%
\section{Research Vignette}\label{research-vignette}}

I will demonstrate the qual``TRIX'' by using a Qualtrics account at my institution, Seattle Pacific University. The only surveys in this account are for the \emph{Recentering Psych Stats} chapters and lessons. All surveys are designed to not capture personally identifying information and not collecting IP addresses nor longitude/latitude. I use this survey in several lessons in this OER. If you haven't taken the survey yet, \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_b2cClqAlLGQ6nLU}{I invite you to do so, now}.

As a teaching activity for the ReCentering Psych Stats OER, the topic of the survey was selected to be consistent with the overall theme of OER. Specifically, the purpose of this study is to understand the campus climate for students whose identities make them vulnerable to bias and discrimination. These include students who are Black, non-Black students of color, LGBTQ+ students, international students, and students with disabilities.

After consulting with a diverse group of stakeholders and subject matter experts (and revising the response options numerous times) I have attempted to center anti-Black racism in the U.S. \citep{mosley_critical_2021, mosley_radical_2020, singh_building_2020}. In fact, the display logic does not present the race items when the course is offered outside the U.S. There are only five options for race: \emph{biracial/multiracial}, \emph{Black}, \emph{non-Black person(s) of color}, \emph{White}, and \emph{I did not notice} (intended to capture a color-blind response). One unintended negative consequence of this design is that the response options could contribute to \emph{colorism} \citep{adames_fallacy_2021, capielo_rosario_acculturation_2019}. Another possibility is that the limited options may erase, or make invisible, other identities. At the time that I wrote up the first description of this survey, the murder of six Asian American women in Atlanta had just occurred. The Center for the Study of Hate and Extremeism has documented that while overall hate drimes dropped by 7\% in 2020, anti-Asian hate crimes reported to the police in America's largest cities increased by 149\% \citep{noauthor_fact_nodate}. These incidents have occurred not only in cities, but in our neighborhoods and on our campusus \citep{kim_guest_2021, kim_yes_2021, noauthor_stop_nodate}. While this survey is intended to assess campus climate as a function of race, it unfortunately does not distinguish between many identities that experience marginalization.

Although the dataset should provide the opportunity to test a number of statistical models, one working hypothesis that framed the study is that the there will be a greater sense of belonging and less bias and discrimination when there is similar representation (of identities that are often marginalized) in the instructional faculty and student body. Termed, ``structural diversity'' \citep{lewis_black_2019} this is likely an oversimplification. In fact, an increase in diverse representation without attention to interacting factors can increase hostility on campus \citep{hurtado_linking_2007}. Thus, the task of rating of a single course relates to the larger campus along the dimensions of belonging and bias/discrimination. For example, if a single class has higher ratings on issues of inclusivity, diversity, and respect, we would expect that sentiment to be echoed in the broader institution.

The survey design has notable limitations You will likely notice that we ask about demographic characteristics of the instructional staff and classmates in the course rated, but we do not ask about the demographic characteristics of the respondent. In making this decision, we likely lose important information. For example, Iacovino and James \citeyearpar{iacovino_retaining_2016} have noted that White students perceive campus more favorably than Black student counterparts.

The decision to not collect demographic details about the respondent was about protecting their (your) identity. As you will see, you have the opportunity to download and analyze the data. If a faculty member asked an entire class to take the survey, the datestamp and a handful of demographic identifiers could very likely identify a student. In certain circumstances, this might be risky in that private information (i.e., gender nonconformity, disclosure of a disability) along with course evaluation data and a datestamp could be related back to the student.

Further, the items that ask respondents to \emph{guess} the identities of the instructional staff and classmates are limited, and contrary to best practices in survey construction that recommend providing the option of a ``write-in'' a response.

In parallel, the items asking respondents to identity characteristics of the instructional staff along dimensions of gender, international status, and disability are ``large buckets'' and do not include ``write-in'' options. Similarly, there was no intent to cause harm by erasing or making invisible individuals whose identities are better defined by different descriptors. Further, no write-in items were allowed. This was also intentional to prevent potential harm caused by people who could leave inappropriate, racist, or otherwise harmful comments.

As I review Qualtrics essentials and trix, I will their use (if used) in the ReCentering Psych Stats survey.

\hypertarget{qualtrics-essentials}{%
\section{Qualtrics Essentials}\label{qualtrics-essentials}}

Qualtrics is a powerful program and I find that many of the surveys we distribute don't capitalize on the features Qualtrics has to offer. Qualtrics has detailed tutorials and instructions that are well worth the investment of a weekend to review them.

In this lecture I will point you to the elements that I think are critical to constructing online surveys. Because Qualtrics tutorials are (a) clear and thorough and (b) frequently updated, I will (a) point you to the tutorials that are available at the time of this lecture prep, (b) tell you why I think they are appropriate, and (c) show you how we have used them in some of our own surveys.

Even if you think you know what you are doing, start here (and then always take the time to ``look around'' at all the options on each window):

\textbf{Survey Basic Overview}: The link below will give you an overview. From there, you can follow all kinds of leads, looking for things you want to do with your survey -- and getting ideas for what will improve it.
\url{https://www.qualtrics.com/support/survey-platform/survey-module/survey-module-overview/}

\href{https://www.qualtrics.com/support/survey-platform/survey-module/block-options/block-options-overview/}{\textbf{Blocks}} are the basic organizational tool in Qualtrics surveys. Blocks have two purposes: (a) grouping items shown on ``one page,'' and (b) using the block for easy ordering and/or random selection/presentation.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/question-types-overview/}{\textbf{Question types}}: Take a time to look at ALL the options. You might be surprised to learn that there is a better choice than you might have imagined.

Let's take a look at super basic/helpful question types:

\begin{itemize}
\tightlist
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/static-content/descriptive-text-and-graphic/}{\textbf{Text/graphic}}: These are the types you should use for providing information (e.g., informed consent) to the participants.\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/matrix-table/}{\textbf{Matrix table}} : A more efficient way to use the Likert-style items (than multiple choice). There is some controversy about whether not to use matrix tables vs.~multiple choice dropdowns\ldots{}

  \begin{itemize}
  \tightlist
  \item
    Make sure to select a reasonable amount of header repetition. This allows the respondent the maximum opportunity to see the column descriptors while they are responding.
  \end{itemize}
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/slider/}{\textbf{Slider}} : This gets you continuous data on that 1 to 100 scale. If the scale you are using is already published, and has not been psychometrically evaluated for slider use, you should probably stick with the format recommended in the publication. But if YOU are designing a survey, think about this option.
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/text-entry/}{\textbf{Text Entry Questions}} have multiple options for length of answer. Don't miss the options that include forms and content validation\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/validation/}{\textbf{Validation}}: Allows the user to allow certain types of information and specify their formats (e.g., numbers, e-mail addresses, dates). There is a balancing between being overly restricting and ensuring that the data is entered in the most clear and consistent way possible. A validation option I frequently use is one that asks individuals if they intended to leave something blank. This is tool that helps prevent missingness without forcing an individual to respond to an item that (a) might not be clear to them, (b) might not be appropriate or them, and/or (c) might result in an answer that is untrue for their unique circumstance.
\end{itemize}

\hypertarget{qual-trix}{%
\section{Qual-TRIX}\label{qual-trix}}

\href{https://www.qualtrics.com/support/survey-platform/my-projects/sharing-a-project/}{\textbf{Collaborating}} with other Qualtrics users in your institution is easy!
Scroll down to ``Collaborating Inside Your Organization'' and follow the instructions for adding individuals to your survey (you must ``own'' the survey\ldots your collaborators will not be able to add others).

The ability to \textbf{schedule survey distributions} is like having your very own GA! If you have a roster(contact list) you can schedule distributions, reminders, and thank yous. Qualtrics will keep track of who responds and send reminders to the non-responders. Here are resources for

\begin{itemize}
\tightlist
\item
  \href{https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/emails-overview/}{E-mail overview}\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/distributions-module/email-distribution/emails/email-distribution-management/}{E-mail distribution management}\\
\item
  \href{https://www.qualtrics.com/support/survey-platform/contacts/creating-a-contact-list/}{Contact lists}
\end{itemize}

\textbf{Personalizing} invitations and surveys. \href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/piped-text/piped-text-overview/}{Piped text} is a way to personalize invitations and/or ``carry forward'' prior responses into new questions.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/randomizer/}{\textbf{Randomization} of blocks} (or a subset of blocks) can be use for several purposes such as: (a) using random selection to display one or more blocks to respondents -- as in a random clinical trial, (b) to randomly display a percentage of blocks or items to shorten the survey in a planned missing design, and (c) randomly display some or all of the blocks of the survey to all respondents so that when respondents experience test fatigue, when they quit responding, ``the last items/surveys'' aren't always the same ones -- thus distributing the missingness across surveys.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/block-options/question-randomization/}{\textbf{Randomization} of items} within a block can be used for similar purposes. You can also use this to display only some of the items (e.g., planned missingness).

\href{https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/advanced/file-upload/}{\textbf{File upload} from respondents} is an additional package that requires the institution to pay a higher fee. If available, this allows respondents to upload some sort of file (photo, powerpoint, .pdf). We use it for poster contests at professional contests (where students upload their poster for online judging in advance of the conference). A colleague of mine uses this function to collect application elements (i.e., resumes, cover letters, reference letters) to a fellowship program.

\begin{itemize}
\tightlist
\item
  As researchers, we can also upload files (e.g., hardcopy of informed consent, documents to be reviewed) for use by the respondent.
\end{itemize}

\href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/display-logic/}{\textbf{Display, Skip, and/or Branch Logic}} can be used to help display to respondents \emph{only} the items that pertain to them. There are multiple approaches to doing this. Using a display logic approach may feel a bit \emph{backward} where the logic is applied \emph{from} the landing spot. We did this extensively in as study that involved two language versions and three age options.

Two other approaches for these issues are \href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/skip-logic/}{skip logic} and \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-flow/standard-elements/branch-logic/}{branch logic}

\hypertarget{even-more-particularly-relevant-to-irb}{%
\section{Even moRe, particularly relevant to iRb}\label{even-more-particularly-relevant-to-irb}}

We can use Qualtrics tools for purposes beyond collecting and downloading data. These tools are especially useful when I think about IRB applications and ethics related to data collection.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/}{\textbf{Exporting to Word}} Helpful for your IRB application (and perhaps in a cloud so that a team can use track changes to edit), it is super simple to export the survey to Microsoft Word. PLUS! You have options for including question numbers, recode values, logic, etc., so that it is essentially a codebook.

\href{https://www.qualtrics.com/support/survey-platform/distributions-module/web-distribution/anonymous-link/}{\textbf{Anonymizing responses}} Another step toward an anonymous response is to withhold the IP address. This is accomplished in the Survey Options menu.

\href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/survey-protection/\#PreventingRespondentsFromTakingYourSurveyMoreThanOnce}{\textbf{Prevent ballot box stuffing}} Want to make sure that respondents only answer once? In the same Survey Options window, you can prevent ballot box stuffing.

Other security options include

\begin{itemize}
\tightlist
\item
  Password protection
\item
  HTTP Referer verification
\end{itemize}

Look also at:

\begin{itemize}
\tightlist
\item
  \textbf{Progress bar} to provide particpants hope (or despair) for ``how much longer.''
\item
  \textbf{Survey termination} to connect cutom endings and thank-you notes.
\item
  \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-options/partial-completion/}{\textbf{Partial completion}} to specify how long the respondent has to complete the survey (after opening it) and whether it is recorded or deleted if it is not completed.

  \begin{itemize}
  \tightlist
  \item
    Related to this, back on the Data \& Analysis tab, you can see both \#s of \href{https://www.qualtrics.com/support/survey-platform/data-and-analysis-module/data/responses-in-progress/}{recorded responses and responses in progress}. You also have options to manually determine how you want to include/exclude the response in progress.
  \item
    ARGHGHGHGHGH!!!! That grubby little ``--\textgreater{}'' submit and progress symbol is often the reason that surveys that are \textgreater{} 90\% complete aren't counted as ``complete.'' What to do? Options: (a) don't say ``Thanks and goodbye'' on a page that has any items, and (b) provide instructions to look for the ``--\textgreater{}'' symbol to continue.
  \end{itemize}
\end{itemize}

Finally, \textbf{PREVIEW PREVIEW PREVIEW}! There is no better way check your work than with previews.

\hypertarget{intravenous-qualtrics}{%
\section{intRavenous Qualtrics}\label{intravenous-qualtrics}}

Access credentials for the institutional account, individual user's account, and survey are essential for getting the survey items and/or results to export into R. The Qualtrics website provides a tutorial for \href{https://www.qualtrics.com/support/integrations/api-integration/overview/\#GeneratingAnAPIToken}{generating an API token}.

We need two pieces of information: the \textbf{root\_url} and an \textbf{API token}.

\begin{itemize}
\tightlist
\item
  Log into your respective qualtrics.com account.
\item
  Select Account Settings
\item
  Choose ``Qualtrics IDs'' from the user name dropdown
\end{itemize}

We need the \textbf{root\_url}. This is the first part of the web address for the Qualtrics account. For our institution it is: spupsych.az1.qualtrics.com

The API token is in the box labeled, ``API.'' If it is empty, select, ``Generate Token.'' If you do not have this option, locate the \emph{brand administrator} for your Qualtrics account. They will need to set up your account so that you have API privileges.

\emph{BE CAREFUL WITH THE API TOKEN} This is the key to your Qualtrics accounts. If you leave it in an .rmd file that you forward to someone else, this key and the base URL gives access to every survey in your account. If you share it, you could be releasing survey data to others that would violate confidentiality promises in an IRB application.

If you mistakenly give out your API token you can generate a new one within your Qualtrics account and re-protect all its contents.

You do need to change the API key/token if you want to download data from a different Qualtrics account. If your list of surveys generates the wrong set of surveys, restart R, make sure you have the correct API token and try again.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#only have to run this ONCE to draw from the same Qualtrics account...but will need to get different token if you are changing between accounts }
\KeywordTok{library}\NormalTok{(qualtRics)}
\CommentTok{#qualtRics::qualtrics_api_credentials(api_key = "mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg",}
              \CommentTok{#base_url = "spupsych.az1.qualtrics.com", overwrite = TRUE, install = TRUE)}
\end{Highlighting}
\end{Shaded}

\emph{all\_surveys()} generates a dataframe containing information about all the surveys stored on your Qualtrics account.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{surveys <-}\StringTok{ }\KeywordTok{all_surveys}\NormalTok{() }
\CommentTok{#View this as an object (found in the right: Environment).  }
\CommentTok{#Get survey id # for the next command}
\CommentTok{#If this is showing you the WRONG list of surveys, you are pulling from the wrong Qualtrics account (i.e., maybe this one instead of your own). Go back and change your API token (it saves your old one). Changing the API likely requires a restart of R.}
\NormalTok{surveys}
\end{Highlighting}
\end{Shaded}

To retrieve the survey, use the \emph{fetch\_survey()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#obtained with the survey ID }
\CommentTok{#"surveyID" should be the ID from above}
\CommentTok{#"verbose" prints messages to the R console}
\CommentTok{#"label", when TRUE, imports data as text responses; if FALSE prints the data as numerical responses}
\CommentTok{#"convert", when TRUE, attempts to convert certain question types to the "proper" data type in R; because I don't like guessing, I want to set up my own factors.}
\CommentTok{#"force_request", when TRUE, always downloads the survey from the API instead of from a temporary directory (i.e., it always goes to the primary source)}
\CommentTok{# "import_id", when TRUE includes the unique Qualtrics-assigned ID; since I have provided labels, I want false}

\CommentTok{#Out of the blue, I started getting an error, that R couldn't find function "fetch_survey."  After trying a million things, adding qualtRics:: to the front of it solved the problem}
\NormalTok{QTRX_df <-qualtRics}\OperatorTok{::}\KeywordTok{fetch_survey}\NormalTok{(}\DataTypeTok{surveyID =} \StringTok{"SV_b2cClqAlLGQ6nLU"}\NormalTok{, }\DataTypeTok{time_zone =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{label=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{convert=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{force_request =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{import_id =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

-- Column specification --------------------------------------------------------
cols(
  .default = col_double(),
  StartDate = col_datetime(format = ""),
  EndDate = col_datetime(format = ""),
  RecordedDate = col_datetime(format = ""),
  ResponseId = col_character(),
  DistributionChannel = col_character(),
  UserLanguage = col_character(),
  Virtual_6 = col_logical(),
  `5_iPronouns` = col_logical(),
  `5_iGenderConf` = col_logical(),
  `5_iRace` = col_logical(),
  `5_iUS` = col_logical(),
  `5_iDis` = col_logical(),
  `6_iPronouns` = col_logical(),
  `6_iGenderConf` = col_logical(),
  `6_iRace` = col_logical(),
  `6_iUS` = col_logical(),
  `6_iDis` = col_logical(),
  `7_iPronouns` = col_logical(),
  `7_iGenderConf` = col_logical(),
  `7_iRace` = col_logical()
  # ... with 17 more columns
)
i Use `spec()` for the full column specifications.
\end{verbatim}

\begin{verbatim}
Warning in file.remove(survey.fpath): cannot remove file 'C:/Users/lhbikos/
AppData/Local/Temp/RtmpmSx9br/Recent Course Eval.csv', reason 'Permission
denied'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#useLocalTime = TRUE,}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(QTRX_df, file="QTRX_df.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#QTRX_df <- read.csv ("QTRX_df.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(QTRX_df, "QTRX_df.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#QTRX_df <- readRDS("QTRX_df.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-codebook}{%
\subsection{The Codebook}\label{the-codebook}}

In order to prepare data from a survey, it is critical to know about its content, scoring directions for scales/subscales, and its design. As I demonstrated above, we can export a \href{./Rate-a-Course_Codebook.pdf}{codebook}, that is, a Word (or PDF) version of the survey with all the coding. In Qualtrics the protocol is: Survey/Tools/ImportExport/Export Survey to Word. Then select all the options you want (especially ``Show Coded Values''). A tutorial provided by Qualtrics can be found \href{https://www.qualtrics.com/support/survey-platform/survey-module/survey-tools/import-and-export-surveys/}{here}. This same process can be used to print the PDF example I used above.

I recommend providing custom variable names and recode values directly in Qualtrics before exporting them into R. A Qualtrics tutorial for this is provided \href{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/}{here}. In general, consider these qualities when creating variable names:

\begin{itemize}
\tightlist
\item
  Brevity: historically, SPSS variable names could be a maximum of 8 characters.
\item
  Intuitive: although variables can be renamed in R (e.g., for use in charts and tables), it is helpful when the name imported from Qualtrics provides some indication of what the variable is.
\item
  Systematic: start items in a scale with the same stem, followed by the item number -- ITEM1, ITEM2, ITEM3.
\end{itemize}

More complete information about data preparation is covered in chapters in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{ReCentering Psych Stats: Multivariate Modeling} text.

\hypertarget{using-data-from-an-exported-qualtrics-.csv-file}{%
\subsection{Using data from an exported Qualtrics .csv file}\label{using-data-from-an-exported-qualtrics-.csv-file}}

The lecture focused on the ``intRavenous'' import. It is is also possible to download the Qualtrics data in a variety of formats (e.g., CSV, Excel, SPSS). Since I got started using files with the CSV extension (think ``Excel'' lite), that is my preference.

In Qualtrics, these are the steps to download the data: Projects/YOURsurvey/Data \& Analysis/Export \& Import/Export data/CSV/Use numeric values

I think that it is critical that to save this file in the same folder as the .rmd file that you will use with the data.

R is sensitive to characters used filenames As downloaded, my Qualtrics .csv file had a long name with spaces and symbols that are not allowed. Therore, I gave it a simple, sensible, filename, ``ReC\_Download210319.csv''. An idiosyncracy of mine is to datestamp filenames. I use two-digit representations of the year, month, and date so that if the letters preceding the date are the same, the files would alphabetize automatically.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library(qualtRics)}
\CommentTok{#QTRX_csv <- read_survey("ReC_Download210319.csv", strip_html = TRUE, import_id = FALSE, time_zone=NULL, legacy = FALSE)}
\end{Highlighting}
\end{Shaded}

Although minor tweaking may be required, the same script above should be applicable to this version of the data.

\hypertarget{tweaking-data-format}{%
\subsection{Tweaking Data Format}\label{tweaking-data-format}}

Two general approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Inside Qualtrics: Use the recode values option (found in the item's gearbox, to the left of the block) to specify variable names and recode values. These should be preserved on the download. \url{https://www.qualtrics.com/support/survey-platform/survey-module/question-options/recode-values/}
\item
  In the R script: In another lecture I demonstrate how to change the formats of data (character, string), selecting only the variables in which we are interested (e.g., excluding the meta-data), and renaming variables sensibly.
\end{enumerate}

Both work! Just a preference -- and probably have an explicit process approach within your research team/collaborators.

\hypertarget{practice-problems-1}{%
\section{Practice Problems}\label{practice-problems-1}}

The suggestion for practice is to develop a questionnaire, format it, pilot it, and download it. Essentially you will be

\begin{itemize}
\tightlist
\item
  Formatting a survey on Qualtrics using all the best practices identified in the lecture

  \begin{itemize}
  \tightlist
  \item
    these include having an introductory statement (to include statement of confidentiality), directions for each sub-survey (if more than one), and closing statement.
  \item
    selecting the most appropriate question type for the items. For example, matrix instead of multiple choice.
  \item
    within question type, using the appropriate options for proper formatting (e.g., the anchors in a matrix should be topically consistent and equal-interval)
  \end{itemize}
\item
  The survey should include minimum of 3 of the qualTRIXter skills (identified in lecture); choose from

  \begin{itemize}
  \tightlist
  \item
    establishing collaboration
  \item
    scheduling e-mail distribution and follow-up
  \item
    personalizing the survey in some way
  \item
    randomization of blocks or items
  \item
    integrating display, skip, or branch logic (e.g., having males and females take a different route)
  \item
    exporting the survey to Word
  \item
    recoding variables in the item controls
  \item
    anonymize the responses
  \item
    prevent ballot box stuffing
  \item
    include a progress bar
  \item
    create a custom ending, e-mail, or thank-you note
  \item
    something else that YOU discovered that isn't in the lecture
  \end{itemize}
\item
  Piloting it, getting their feedback, and identifying what problems are (and how you might fix them)

  \begin{itemize}
  \tightlist
  \item
    with 3 folks from your RVT, cohort, or this class
  \item
    with 3 additional folks who aren't quite as ``research savvy''
  \item
    collect their feedback (ideally in a text-item directly on the survey itself) and write a brief summary (3 paragraphs max) of their impressions and how you might improve the survey
  \end{itemize}
\item
  Import the Qualtrics data directly R

  \begin{itemize}
  \tightlist
  \item
    preferably, directly from Qualtrics with the API token, base URL, and survey ID
  \item
    alternatively (for the same \# of points) from the exported CSV file \emph{via the qualtRics package} (required)
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}lcc@{}}
\toprule
Assignment Component & Points Possible & Points Earned\tabularnewline
\midrule
\endhead
1. Qualtrics survey best practices & 5 &\tabularnewline
2. QualTRIXter skills (at least 3) & 5 &\tabularnewline
3. Minimum of 6 pilot respondents & 5 &\tabularnewline
4. Summary of pilot feedback & 5 &\tabularnewline
5. Import of Qualtrics data into R & 5 &\tabularnewline
6. Explanation to grader & 5 &\tabularnewline
\textbf{Totals} & 20 &\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{rxy}{%
\chapter{Psychometric Validity: Basic Concepts}\label{rxy}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=d3b821d7-c182-435d-80ba-ad9e003dbc97}{Screencasted Lecture Link}

The focus of this lecture is to provide an introduction to validity. This includes understanding some of the concerns of validity, different aspects of validity, and factors as they affect validity coefficients.

\hypertarget{navigating-this-lesson-2}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-2}}

There is just over one hour of lecture. If you work through the materials with me it would be plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-2}{%
\subsection{Learning Objectives}\label{learning-objectives-2}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Distinguish between different types of validity based on short descriptions.
\item
  Compute and interpret validity coefficients.
\item
  Evaluate the incremental validity of an instrument-of-interest.
\item
  Define and interpret the standard error of estimate.
\item
  Develop a rationale that defends importance of establishing the validity of a measuring instrument.
\end{itemize}

\hypertarget{planning-for-practice-2}{%
\subsection{Planning for Practice}\label{planning-for-practice-2}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to interpret examine aspects of the construct validity through the creation and interpretation of validity coefficients. Ideally, you will examine both convergent/discriminant as well as incremental validity.

\hypertarget{readings-resources-2}{%
\subsection{Readings \& Resources}\label{readings-resources-2}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Jhangiani, R. S., Chiang, I.-C. A., Cuttler, C., \& Leighton, D. C. (2019). Reliability and Validity. In \emph{Research Methods in Psychology}. \url{https://doi.org/10.17605/OSF.IO/HF7DQ}
\item
  Clark, L. A. \& Watson, D. (1995). Constructing validity: Basic issues in objective scale development. Psychological Assessment, 7, 309-319.

  \begin{itemize}
  \tightlist
  \item
    In this manuscript, Clark and Watson (1995) create a beautiful blend of theoretical issues and practical suggestions for creating measures that evidence construct validity. From the practical perspective, the authors first guide potential scale constructors through the literature review and creating an item pool (including tips on writing items). The authors address structural validity by first beginning with strategies for constructing the test. In this section, the authors revisit the issue of dimensionality (i.e., alpha vs.~factor analysis). Finally, the authors look at initial data collection (addressing sample size) and psychometric evaluation.
  \end{itemize}
\end{itemize}

\hypertarget{packages-2}{%
\subsection{Packages}\label{packages-2}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{#if(!require(MASS))\{install.packages("MASS")\}}
\CommentTok{#if(!require(psych))\{install.packages("psych")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette-1}{%
\section{Research Vignette}\label{research-vignette-1}}

Explorations of validity are frequently correlational in nature. This lesson provides descriptions of numerous pathways for establishing an instrument's validity. Best practices involving numerous demonstrations of validity. Across several lessons, we will rework several several of the analyses reported in the research vignette. For this lesson in particular, the research vignette allows both convergent/discriminant validity as well as incremental validity.

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate Szymanski and Bissonette have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales composed of the following items:

\begin{itemize}
\tightlist
\item
  College response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students.
  \item
    My university/college is unresponsive to the needs of LGBTQ students.
  \item
    My university/colleg provides a supportive environment for LGBTQ students.
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus.
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus.
  \item
    LGBTQ students are harassed on my university/college campus.
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate.

In the \protect\hyperlink{rxx}{lesson} on reliability, I simulate item-level data. However, this lesson we will be interested in the correlations between the total and subscale scores with five additional scales:

\begin{itemize}
\tightlist
\item
  LGBTQ victimization
\item
  Satisfaction with college
\item
  Intention to persist in college
\item
  Generalized anxiety disorder symptoms
\item
  Symptoms of depression
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Szy_mu <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.16}\NormalTok{, }\FloatTok{2.71}\NormalTok{, }\FloatTok{3.61}\NormalTok{, }\FloatTok{.11}\NormalTok{, }\FloatTok{5.61}\NormalTok{, }\FloatTok{4.41}\NormalTok{, }\FloatTok{1.45}\NormalTok{, }\FloatTok{1.29}\NormalTok{)}
\NormalTok{Szy_sd <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.26}\NormalTok{, }\FloatTok{1.33}\NormalTok{, }\FloatTok{1.51}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{1.15}\NormalTok{, }\FloatTok{.53}\NormalTok{, }\FloatTok{.80}\NormalTok{, }\FloatTok{.78}\NormalTok{)}
\NormalTok{Szy_r_mat <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,   }\FloatTok{.88}\NormalTok{, }\FloatTok{.90}\NormalTok{, }\FloatTok{.35}\NormalTok{, }\FloatTok{-.56}\NormalTok{, }\FloatTok{-.27}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.24}\NormalTok{,}
                  \FloatTok{.88}\NormalTok{,  }\DecValTok{1}\NormalTok{,  }\FloatTok{.58}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{-.59}\NormalTok{, }\FloatTok{-.29}\NormalTok{, }\FloatTok{.17}\NormalTok{, }\FloatTok{.18}\NormalTok{,}
                  \FloatTok{.90}\NormalTok{, }\FloatTok{.58}\NormalTok{,  }\DecValTok{1}\NormalTok{,  }\FloatTok{.37}\NormalTok{, }\FloatTok{-.41}\NormalTok{, }\FloatTok{-.19}\NormalTok{, }\FloatTok{.27}\NormalTok{, }\FloatTok{.24}\NormalTok{,}
                  \FloatTok{.35}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.37}\NormalTok{,  }\DecValTok{1}\NormalTok{,  }\FloatTok{-.22}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{.21}\NormalTok{,}
                 \FloatTok{-.56}\NormalTok{,}\OperatorTok{-}\NormalTok{.}\DecValTok{59}\NormalTok{, }\FloatTok{-.41}\NormalTok{, }\FloatTok{-.22}\NormalTok{, }\DecValTok{1}\NormalTok{,   }\FloatTok{.53}\NormalTok{, }\FloatTok{-.29}\NormalTok{, }\FloatTok{-.32}\NormalTok{,}
                 \FloatTok{-.27}\NormalTok{, }\FloatTok{-.29}\NormalTok{, }\FloatTok{-.19}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.53}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{-.22}\NormalTok{, }\FloatTok{-.26}\NormalTok{,}
                  \FloatTok{.25}\NormalTok{, }\FloatTok{.17}\NormalTok{,  }\FloatTok{.27}\NormalTok{,  }\FloatTok{.23}\NormalTok{,  }\FloatTok{-.29}\NormalTok{, }\FloatTok{-.22}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{.76}\NormalTok{,}
                  \FloatTok{.24}\NormalTok{,  }\FloatTok{.18}\NormalTok{, }\FloatTok{.24}\NormalTok{, }\FloatTok{.21}\NormalTok{, }\FloatTok{-.32}\NormalTok{,  }\FloatTok{-.26}\NormalTok{, }\FloatTok{.76}\NormalTok{,  }\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{8}\NormalTok{)}
\NormalTok{Szy_cov_mat <-}\StringTok{ }\NormalTok{Szy_sd }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(Szy_sd) }\OperatorTok{*}\StringTok{ }\NormalTok{Szy_r_mat}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210907}\NormalTok{)}
\NormalTok{SzyDF <-}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{as.data.frame}\NormalTok{(MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{646}\NormalTok{, }\DataTypeTok{mu=}\NormalTok{Szy_mu, }\DataTypeTok{Sigma=}\NormalTok{Szy_cov_mat, }\DataTypeTok{tol=}\FloatTok{1e-3}\NormalTok{, }\DataTypeTok{empirical=}\OtherTok{TRUE}\NormalTok{)),}\DecValTok{2}\NormalTok{) }\CommentTok{#adding "tol=1e-3" fixed the not positive matrix error}
\NormalTok{SzyDF <-}\StringTok{ }\KeywordTok{round}\NormalTok{(dplyr}\OperatorTok{::}\KeywordTok{rename}\NormalTok{(SzyDF, }\DataTypeTok{CClimate =}\NormalTok{ V1, }\DataTypeTok{CResponse =}\NormalTok{ V2, }\DataTypeTok{Stigma =}\NormalTok{ V3, }\DataTypeTok{Victimization =}\NormalTok{ V4, }\DataTypeTok{CollSat =}\NormalTok{ V5, }\DataTypeTok{Persistence =}\NormalTok{ V6, }\DataTypeTok{Anxiety =}\NormalTok{ V7, }\DataTypeTok{Depression =}\NormalTok{ V8),}\DecValTok{2}\NormalTok{)}
\CommentTok{#round(cor(SzyDF),2)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(SzyDF, file="SzyDF.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#SzyDF <- read.csv ("SzyDF.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(SzyDF, "SzyDF.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#SzyDF <- readRDS("SzyDF.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{fundamentals-of-validity}{%
\section{Fundamentals of Validity}\label{fundamentals-of-validity}}

\textbf{Validity} (the classic definition) is the ability of a test to measure what it purports to measure. Supporting that definition are these notions:

\begin{itemize}
\tightlist
\item
  Validity is extent of matching, congruence, or ``goodness of fit'' between the operational definition and concept it is supposed to measure.
\item
  An instrument is said to be valid if it taps the concept it claims to measure.
\item
  Validity is the appropriateness of the interpretation of the results of an assessment procedure for a given group of individuals, not to the procedure itself.
\item
  Validity is a matter of degree; it does not exist on an all-or-none basis.
\item
  Validity is always specific to some particular use or interpretation.
\item
  Validity is a unitary concept.
\item
  Validity involves an overall evaluative judgment.
\end{itemize}

Over the years (and, perhaps within each construct), validity has somewhat of an \emph{evolutionary} path from a focus on content, to prediction, to theory and hypothesis testing.

When the focus is on \textbf{content}, we are concerned with the:

\begin{itemize}
\tightlist
\item
  assessment of what individuals had learned in specific content areas
\item
  relevance of its content (i.e., we compare the content to the content domain)
\end{itemize}

When the focus is on \textbf{prediction}, we are concerned with:

\begin{itemize}
\tightlist
\item
  how different persons respond in a given situation (now or later).
\item
  the correlation coefficient between test scores (predictor) and the assessment of a criterion (performance in a situation)
\end{itemize}

A focus on \textbf{theory and hypothesis testing} adds:

\begin{itemize}
\tightlist
\item
  a strengthened theoretical orientation
\item
  a close linkage between psychological theory and verification through empirical and experimental hypothesis testing
\item
  an emphasis on constructs in describing and understanding human behavior.
\end{itemize}

\textbf{Constructs} are broad categories, derived from the common features shared by directly observable behavioral variables. They are theoretical entities and not directly observable. \textbf{Construct validity} is at the heart of psychometric evaluation. We define \textbf{construct validity} as the fundamental and all-inclusive validity concept, insofar as it specifies what the test measures. Content and predictive validation procedures are among the many sources of information that contribute to the understanding of the constructs assessed by a test.

\hypertarget{validity-criteria}{%
\section{Validity Criteria}\label{validity-criteria}}

We have just stated that validity is an overall, evaluative judgment. Within that umbrella are different criteria by which we judge the validity of a measure. We casually refer to them as \emph{types}, but each speaks to that unitary concept.

\hypertarget{content-validity}{%
\subsection{Content Validity}\label{content-validity}}

Content validity is concerned with the representativeness of the domain being assessed. Content validation procedures may differ depending on whether the test is in the educational/achievement context or if it is more of an attitude/behavioral survey.

In the educational/achievement context, content validation seeks to ensure the items on an exam are appropriate for the content domain being assessed.

A \textbf{table of specifications} is a two-way chart which indicates the instructionally relevant learning tasks to be measured. Percentages in the table indicate the relative degree of emphasis that each content area

Let's imagine that I was creating a table of specifications for items on a quiz for this very chapter. The columns represent the types of outcomes we might expect. The American Psychological Association often talks about \emph{KSAs} (knowledge, skills, attitudes), so I will utilize those as a framework. You'll notice that the number of items and percentages do not align mathematically. Because, in the exam, I would likely weight application items (e.g., ``work the problem'') more highly than knowledge items (e.g., multiple choice), the relative weighting may differ.

\textbf{Table of Specifications}

\begin{longtable}[]{@{}lcccc@{}}
\toprule
\begin{minipage}[b]{0.38\columnwidth}\raggedright
Learning Objectives\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Knowledge\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Skills\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Attitudes\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
\% of test\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Distinguish between different types of validity based on short descriptions.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
6 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
30\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Compute and interpret validity coefficients.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
2 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
15\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Evaluate the incremental validity of an instrument-of-interest.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
20\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Define and interpret the standard error of estimate.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
15\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Develop a rationale that defends importance of establishing the validity of a measuring instrument.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
20\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
TOTALS\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
7 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
3 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
100\%\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Subject matter experts} (SMEs)s are individuals chosen to evaluate items based on their degree of knowledge of the subject being assessed. If used:

\begin{itemize}
\tightlist
\item
  report how many and professional qualifications;
\item
  to classify items, report the directions they were given and the extent of agreement among judges.
\end{itemize}

Empirical procedures for enhancing content validity of educational assessments may include

\begin{itemize}
\tightlist
\item
  comparing item-level and total scores with grades; lower grades should get lower scores;
\item
  analyzing individual errors;
\item
  observing student work methods (have the students ``think aloud'' in front of an examiner);
\item
  evaluating the role of speed, noting how many do not complete the test in the time allowed;
\item
  correlating the scores with a reading comprehension test (if the exam is highly correlated, then it may be a test of reading and not another subject). Alternatively, if it is a reading comprehension test; give the student the questions (without the passage) to see how well they answered the questions on the basis of prior knowledge.
\end{itemize}

For surveys and tests outside of educational settings, content validation procedures ask, ``Does the test cover a representative sample of the specified skills and knowledge?'' and "Is test performance reasonably free from the influence of irrelevant variables? Naturally, SMEs might be used.

An example of content validation from Industrial-Organizational Psychology is the job analysis which precedes the development of test for employee selection and classification.
Not all tests require content analysis. In aptitude and personality tests we are probably more interested in other types of validity evaluation.

\hypertarget{face-validity-the-unvalidity}{%
\subsection{Face Validity: The ``un''validity}\label{face-validity-the-unvalidity}}

Face validity is concerned with the question, ``How does an assessment look on the `face of it'?'' Let's imagine that on a qualification exam for electricians, a math items asks the electrician to estimate the amount of yarn needed to complete a project. The item may be more \emph{face valid} if the calculation was with wire. Thus, face validity can often be improved by reformulating test items in terms that appear relevant and plausible for the context.

Face validity should never be regarded as a substitute for objectively determined validity. In contrast, it should not be assumed that when a (valid and reliable) test has been modified to increase its face validity, that its objective validity and reliability is unaltered. That is, it must be reevaluated.

\hypertarget{criterion-related-validity}{%
\subsection{Criterion Related Validity}\label{criterion-related-validity}}

Criterion related validity has to do with the test's ability to \emph{predict} an outcome (the criterion). If the criterion is something that occurs simultaneously, it is an assessment of \textbf{concurrent validity}; if it is in the future then it is an assessment of \textbf{predictive validity.}

A \textbf{criterion} is ``the thing'' that the test should, theoretically, be able to \emph{predict}. That prediction could be occurring at the same time (\emph{concurrent validity}) or at a future time (\emph{predictive validity}). Regardless, the estimate of the criteria must be independent of the survey/assessment being evaluated. The table below provides examples of types of tests and concurrent and predictive validity criteria.

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.32\columnwidth}\raggedright
Type of Test\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Concurrent Criteria Example\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Predictive Criteria Example\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.32\columnwidth}\raggedright
A shorter (or cheaper) standardized achievement test\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
school grades, existing standardized tests\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
subsequent graduation/college admissions, cumulative GPA\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Employee selection tests\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
decision made by a search committee\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
subsequent retention or promotion of the selected employee\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.32\columnwidth}\raggedright
Assessment of depression severity (shorter or cheaper)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
diagnosis from a mental health professional; correlation with an established measure\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
inpatient hospitalization or act of self-harm\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Contrasted groups} is a specific type of criterion-related validity. Clearly differentiated groups (e.g., sales clerks versus excutives; engineers versus musicians) are chosen to see if exam performance or profiles differ in predictable ways.

\textbf{Criterion contamination} occurs when test scores, themselves, are used to make decisions about the criteria. To prevent this,

\begin{itemize}
\tightlist
\item
  No person who participates in the assignment of criterion ratings can have any knowledge of the examinee's test scores.\\
\item
  the test scores must be kept strictly confidentially.
\end{itemize}

There are a number of issues related to criterion-related validity.

\begin{itemize}
\tightlist
\item
  Is the criterion choice appropriate?

  \begin{itemize}
  \tightlist
  \item
    Criterion validity is only as good as the validity of the criterion to which one is making a comparison.
    -- In the 1980s and 1990s there was more attention in this area. That is critics questioned the quality of the criterion being used.
  \end{itemize}
\item
  To what degree can the results of criterion-related validity be generalized?
  -- Most tests are developed (intentionally) for a local context, setting, or audience. Consequently, in the local context, the criterion-prediction sample is usually too small (i.e., 50 cases or less).
  -- Those who want to generalize the test to a broader population should evaluate the test in relationship to the new purpose.
\item
  Is there a role for meta-analysis?
  -- Repeated validation studies of our tests, on different samples, results in a number of small-scale studies, each with their own validity coefficients.
  -- We can use meta-analytic procedures in reporting the results of validity coefficients when they are used for establishing criterion validity.
\end{itemize}

\hypertarget{construct-validity}{%
\subsection{Construct Validity}\label{construct-validity}}

\textbf{Construct validity} was introduced in 1954 in the first edition of APA's testing standards and is defined as the extent to which the test may be said to measure a theoretical construct or trait. The overarching focus is on the role of \emph{psychological theory} in test construction and the ability to formulate hypotheses that can supported (or not) in the evaluation process. Construct validity is established by the accumulation of information from a variety of sources.

There are a number of sources that can be used to support construct validity.

\hypertarget{internal-consistency}{%
\subsection{Internal Consistency}\label{internal-consistency}}

In the next \protect\hyperlink{rxx}{chapter}, you will learn that \textbf{internal consistency} is generally considered to be an index of reliability. In the context of criterion-related validity, a goal is to ensure that the criterion is the total score on the test, itself. To that end, some of the following could also support this aspect of validity:

\begin{itemize}
\tightlist
\item
  Comparing high and low scorers. Items that fail to show a significantly greater proportion of ``passes'' in the upper than the lower group are considered invalid, and are modified or eliminated.
\item
  Computing a biserial correlation between the item and total score.\\
\item
  Correlating the subtest score with the total score. Any subtest whose correlation with the total score is too low, is eliminated.
\end{itemize}

Although some take issue with this notion, the degree of \emph{homogeneity} (the degree to which items assess the same thing) has some bearing on construct validity. There is a tradeoff between items that measure a narrow slice of the construct definition (internal consistency estimates are likely to be higher) and those that sample the construct definition more broadly (internal consistency estimates are likely to be lower).

Admittedly, the contribution of internal consistency data is limited. In absence of external data, it tells us little about WHAT the test measures.

\hypertarget{structural-validity}{%
\subsection{Structural Validity}\label{structural-validity}}

\hypertarget{exploratory-factor-analysis}{%
\subsubsection{Exploratory Factor Analysis}\label{exploratory-factor-analysis}}

\textbf{Exploratory factor analysis} (EFA) is used to simplify the description of behavior by reducing the number of categories (factors or dimensions) to as many as the numbers of the items to fewer. In instrument development, techniques like \emph{principal components analysis} or \emph{principal axis factoring} are used to identify clusters (latent factors) among items. We frequently treat these as scales and subscales.

Imagine the use of 20 tests to 300 people. There would be 190 correlations.

\begin{itemize}
\tightlist
\item
  Irrespective of content, we can probably summarize the intercorrelations of tests with 5-6 factors.
\item
  When the clustering of tests includes vocabulary, analogies, opposites, and sentence completions, we might suggest a ``verbal comprehension factor.''
\item
  Factorial validity is the correlation of the test with whatever is common to a group of tests or other indices of behavior. If our single test has a correlation of .66 with the factor on which it loads, then the ``factorial validity of the new test as a measure of the common trait is .66.''
\end{itemize}

When EFA is utilized the items are ``fed'' into an iterative process that analyzes the relations and ``reveals'' (or suggests -- we are the ones who interpret the data) how many factors (think scales/subscales) andwhich items comprise them.

\hypertarget{confirmatory-factor-analysis}{%
\subsubsection{Confirmatory Factor Analysis}\label{confirmatory-factor-analysis}}

\textbf{Confirmatory factor analysis} (CFA) involves specifying, a priori, a proposed relationship of items, scales, and subscales and then testing its \emph{goodness of fit.} In CFA (a form of structural equation modeling {[}SEM{]}), the latent variables (usually the higher order scales and total scale score) are positioned to cause the responses on the indicators/items.

Subsequent lessons provide examples of both EFA and CFA approaches to psychometrics.

\hypertarget{experimental-interventions}{%
\subsection{Experimental Interventions}\label{experimental-interventions}}

Construct validity is also supported by hypothesis testing and experimentation. If we expect that the construct assessed by the instrument is malleable (e.g., depression) and that an intervention could change it, then a random clinical trial that evaluated the effectiveness of an intervention (and it worked -- depression scores declined) would simultaneously provide support for the intervention as well as the instrument.

\hypertarget{convergent-and-discriminant-validity}{%
\subsection{Convergent and Discriminant Validity}\label{convergent-and-discriminant-validity}}

In a psychometric evaluation, we will often administer our instrument-of-interest along with a battery of instruments that are more-and-less related. \textbf{Convergent validity} is supported when there are \emph{moderately high} correlations between our tests and the instruments with which we expect moderately high correlations. In contrast, \textbf{discriminant validity} is established by low and/or non-significant correlations between our instrument-of-interest and instruments that should be unrelated. For example, we want a low and non-significant correlation between a quantitative reasoning test and scores on a reading comprehension test. Why? Because if the correlation is too high, the test cannot discriminate between reading comprehension and math.

There are no strict cut-offs to establish convergence or discrimination. We can even ask, ``Could a correlation intended to support convergence be too high?'' It is possible! Unless the instrument-of-interest offers advantages such as brevity or cost, then correlations that fall into the ranges of multicollinearity or singularity can indicate unnecessary duplication or redundancy.

In our research vignette, Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} conduct a correlation matrix that reports the bivariate relations between the LGBTQ Campus Climate full-scale as well as the College Response and Stigma subscales with measures that assess (a) LGBTQ victimization, (b) satisfaction with college, (c) persistence attitudes, and (d) anxiety, and (e) depression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\OperatorTok{::}\KeywordTok{apa.cor.table}\NormalTok{(SzyDF, }\DataTypeTok{filename =} \StringTok{"SzyCor.doc"}\NormalTok{, }\DataTypeTok{table.number =} \DecValTok{1}\NormalTok{, }\DataTypeTok{show.sig.stars=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{landscape=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable         M    SD   1            2            3           
  1. CClimate      3.16 1.26                                       
                                                                   
  2. CResponse     2.71 1.33 .88**                                 
                             [.86, .90]                            
                                                                   
  3. Stigma        3.61 1.51 .90**        .58**                    
                             [.88, .91]   [.53, .63]               
                                                                   
  4. Victimization 0.11 0.23 .35**        .25**        .37**       
                             [.28, .42]   [.18, .32]   [.30, .44]  
                                                                   
  5. CollSat       5.61 1.15 -.56**       -.59**       -.41**      
                             [-.61, -.50] [-.64, -.54] [-.47, -.34]
                                                                   
  6. Persistence   4.41 0.53 -.27**       -.29**       -.19**      
                             [-.34, -.20] [-.36, -.22] [-.26, -.11]
                                                                   
  7. Anxiety       1.45 0.80 .25**        .17**        .27**       
                             [.18, .32]   [.09, .24]   [.20, .34]  
                                                                   
  8. Depression    1.29 0.78 .24**        .18**        .24**       
                             [.17, .31]   [.10, .25]   [.17, .31]  
                                                                   
  4            5            6            7         
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
                                                   
  -.22**                                           
  [-.29, -.15]                                     
                                                   
  -.04         .53**                               
  [-.12, .04]  [.47, .58]                          
                                                   
  .23**        -.29**       -.22**                 
  [.16, .30]   [-.36, -.22] [-.29, -.15]           
                                                   
  .21**        -.32**       -.26**       .76**     
  [.14, .28]   [-.39, -.25] [-.33, -.19] [.73, .79]
                                                   

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

Examination of these values (which align well with the results in the table) follow the expected pattern. That is, higher scores on the overall Campus Climate, College Response, and Stigma scales result in higher levels of victimization, anxiety and depression. Conversely, they are associated with lower college satisfaction and persistence.

The College Response and Stigma subscales' relations with satisfaction with college (-.59, -.41, respectively) and persistence attitudes (-.29*, -.19, respectively) are examples of the convergent and discriminant patterns (College Response has higher relations with these than Stigma) that support construct validity.

The \textbf{multitrait-multimethod matrix} is a systematic experimental design for the dual approach of convergent and discriminant validation, which requires the assessment of two or more traits (classically math, English, and reading scores) by two more methods (self, parent, and teacher). Conducting a web-based image search on this term will show a matrix of alpha coefficients and correlation coefficients that are interpreted in relationship to each other. Roughly:

\begin{itemize}
\tightlist
\item
  alpha (internal consistency) coefficients should be the highest,
\item
  validity coefficients (correlations of the same trait assessed by different methods) should be higher than correlations between different traits measured by different methods,
\item
  validity coefficients (correlations of the same trait assessed by different methods) should be higher than different traits measured by the same method.
\end{itemize}

\hypertarget{incremental-validity}{%
\subsection{Incremental Validity}\label{incremental-validity}}

\textbf{Incremental validity} is the increase in predictive validity attributable to the test. It indicates the contribution the test makes to the selection of individuals who will meet the minimum standards in criterion performance. There are different ways to assess this, one of the most common is to first enter known predictors and then see if the instrument-of-interest continues to account variance over-and-above those that are entered.

In the Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} psychometric evaluation, the negative relations with satisfaction with college and intention to persist in college as well as positive relations with both anxiety and depression persisted even after controlling for LGBTQ victimization experiences.

I will demonstrate this procedure, predicting the contribution that the LGBTQ Campus Climate total scale scrore has on predicting intention to persist in college, over and above LGBTQ victimization.

The process is to use hierarchical linear regression. Two models are built. In the first mode (``PfV'' stands {[}in my mind{]} for ``Persistence from Victimization''), persistence is predicted from victimization. The second model adds the LGBTQ Campus Climate Scale. I asked for summaries of each model. Then the \emph{anova()} function compares the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PfV <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Persistence }\OperatorTok{~}\StringTok{ }\NormalTok{Victimization, }\DataTypeTok{data =}\NormalTok{ SzyDF)}
\NormalTok{PfVC <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Persistence }\OperatorTok{~}\StringTok{ }\NormalTok{Victimization }\OperatorTok{+}\StringTok{ }\NormalTok{CClimate, }\DataTypeTok{data =}\NormalTok{ SzyDF)}
\KeywordTok{summary}\NormalTok{(PfV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Persistence ~ Victimization, data = SzyDF)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.59249 -0.34872 -0.01686  0.35461  1.65781 

Coefficients:
              Estimate Std. Error t value            Pr(>|t|)    
(Intercept)    4.42029    0.02312 191.183 <0.0000000000000002 ***
Victimization -0.09367    0.09079  -1.032               0.303    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5302 on 644 degrees of freedom
Multiple R-squared:  0.00165,   Adjusted R-squared:  0.0001002 
F-statistic: 1.065 on 1 and 644 DF,  p-value: 0.3025
\end{verbatim}

From the PfVCmodel we learn that victimation has a non-significant effect on intentions to persist in college Further, the \(R^2\) is quite small (0.002).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(PfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Persistence ~ Victimization + CClimate, data = SzyDF)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.54270 -0.33725 -0.01329  0.34526  1.65393 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    4.78193    0.05489  87.119 < 0.0000000000000002 ***
Victimization  0.14182    0.09330   1.520                0.129    
CClimate      -0.12263    0.01701  -7.208      0.0000000000016 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5104 on 643 degrees of freedom
Multiple R-squared:  0.07628,   Adjusted R-squared:  0.07341 
F-statistic: 26.55 on 2 and 643 DF,  p-value: 0.000000000008341
\end{verbatim}

In the PfVC model, we see that the LGBTQ Campus Climate full scale score has a significant impact on intentions to persist. Specifically, foreach additional point higher on the Campus Climate Score, intentions to persist decrease by .13 points. Together, the model accounts for 7\% of the variance (this is also an \(R^2\) change of 7\%).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(PfV, PfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Persistence ~ Victimization
Model 2: Persistence ~ Victimization + CClimate
  Res.Df    RSS Df Sum of Sq     F            Pr(>F)    
1    644 181.01                                         
2    643 167.48  1    13.531 51.95 0.000000000001602 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that there is a statistically significant difference between the models.

Let's try another model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AfV <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Anxiety }\OperatorTok{~}\StringTok{ }\NormalTok{Victimization, }\DataTypeTok{data =}\NormalTok{ SzyDF)}
\NormalTok{AfVC <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Anxiety }\OperatorTok{~}\StringTok{ }\NormalTok{Victimization }\OperatorTok{+}\StringTok{ }\NormalTok{CClimate, }\DataTypeTok{data =}\NormalTok{ SzyDF)}
\KeywordTok{summary}\NormalTok{(AfV)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Anxiety ~ Victimization, data = SzyDF)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.40178 -0.51888  0.04366  0.54301  2.51019 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    1.36192    0.03398  40.081 < 0.0000000000000002 ***
Victimization  0.80273    0.13342   6.016        0.00000000299 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7791 on 644 degrees of freedom
Multiple R-squared:  0.05322,   Adjusted R-squared:  0.05175 
F-statistic:  36.2 on 1 and 644 DF,  p-value: 0.000000002991
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(AfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Anxiety ~ Victimization + CClimate, data = SzyDF)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.47857 -0.48764  0.01947  0.52990  2.50069 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    1.00143    0.08241  12.152 < 0.0000000000000002 ***
Victimization  0.56799    0.14008   4.055           0.00005635 ***
CClimate       0.12224    0.02554   4.786           0.00000212 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7662 on 643 degrees of freedom
Multiple R-squared:  0.08578,   Adjusted R-squared:  0.08293 
F-statistic: 30.16 on 2 and 643 DF,  p-value: 0.0000000000003007
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(AfV, AfVC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: Anxiety ~ Victimization
Model 2: Anxiety ~ Victimization + CClimate
  Res.Df    RSS Df Sum of Sq      F      Pr(>F)    
1    644 390.94                                    
2    643 377.50  1    13.445 22.901 0.000002117 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This model is a little more exciting in that our first model (AfV) is statistically significant. That is, victimization has a statistically significant effect on anxiety, accounting for 5\% of the variance. Even so, when added, the LGBTQ Campus Climate total scale score is also significant, and accounts for an additional 4\% of variance (\(\Delta{R^2}\)),\(R^2\) = 0.85. There is a statistically significant difference between models (\emph{F}{[}1, 643{]} = 22.98, \emph{p} \textless{} .001).

\hypertarget{considering-the-individual-and-social-consequences-of-testing}{%
\subsection{Considering the Individual and Social Consequences of Testing}\label{considering-the-individual-and-social-consequences-of-testing}}

Messick \citep{messick_consequences_2000} and others recommend that the ``consequences of testing'' be included in the concept of test validity. Messick's point was to consider the the unintended consequences of specific uses. That is, their use may be detrimental to individuals or to members of certain ethnic or other populations with diverse experiential backgrounds. Examples of inappropriate use have included:

\begin{itemize}
\tightlist
\item
  The California Psychological Inventory (CPI) being used as a screening tool for employment as a security job. Two of its items inquired about same-sex activities and the employer was using this to screen out gay men. Applicants were able to demonstrate, in court, a consistent rejection of gay applicants.
\item
  While this is not a psychological test, urine samples are often collected as drug screening tools. In reality, urine can reveal a number of things -- such as pregnancy.
\end{itemize}

The issue begs ``conflicting goals.'' In this case, the problem was not caused by the test. Rather, by its misuse. Studying the ``consequences'' of testing is one that is not necessarily answerable by empirical data/statistical analysis. It requires critical observation, human judgment, and systematic debate.

\hypertarget{factors-affecting-validity-coefficients}{%
\section{Factors Affecting Validity Coefficients}\label{factors-affecting-validity-coefficients}}

Keeping in mind that a \emph{validity coefficient} is merely the correlation between the test and some criteria, the same elements that impact the magnitude and significance of a correlation coefficient will similarly effect a validity coefficient.

\textbf{Nature of the group} A test that has high validity in predicting a particular criterion in one population, may have little or no validity in predicting the same criterion in another population. If a test is designed for use in diverse populations, information about the population generalizability should be reported in the technical manuals.

\textbf{Sample heterogeneity} Other things being equal, if there is a linear relationship between X and Y, it will have a greater magnitude when the sample is heterogeneous.

\textbf{Pre-selection} Just like internal and external validity in a research design can be threatened by selection issues, pre-selection can also impact the validity coefficients of a measure. For example, if we are evaluating a new test for job selection, we may select a group of newly hired employees. We plan to collect some measure of job performance at a later date. Our results may be limited by the criteria used to select the employees. Were they volunteers? Were they only those hired? Were they ALL of the applicants.

\textbf{Validity coefficients may change over time}. Consider the relationship between the college boards and grade point average at Yale University. Fifty years ago \(r_{xy} = .72\); today \(r_{xy} = .52\). Why? The nature of the student body has become more diverse (50 years ago, the student body was predominantly White, high SES, and male).

The \textbf{form of the relationship} matters. The Pearson R assumes the relationship between the predictor and criterion variables is linear, uniform, and homoschedastistic (equal variability throughout the range of a bivariate distribution). When the variability is unequal throughout the range of the distribution the relationship is heteroscedastic.

\begin{figure}
\centering
\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-20-1.pdf}
\caption{\label{fig:unnamed-chunk-20}Illustration of heteroschedasticity}
\end{figure}

There could also be other factors involved in the relationship between the instrument and the criterion:

\begin{itemize}
\tightlist
\item
  curvilinearity
\item
  an undetected mechanism such as a moderator
\end{itemize}

Finally, what is our thresshold for acceptability?

\begin{itemize}
\tightlist
\item
  Consider statistical signifance -- but also its limitations (e.g., power, Type I error, Type II error)
\item
  Consider the magnitude of the correlation; and also \(R^2\) (the proportion of variance accounted for)
\item
  Consider error:

  \begin{itemize}
  \tightlist
  \item
    The standard error of the estimate shows the margin of error to be expected in the individuals predicted criterion score as the result of the imperfect validity of the instrument.
  \end{itemize}
\end{itemize}

\[SE_{est} = SD_{y}\sqrt{1 - r_{xy}^{2}}\]
Where
\(r_{xy}^{2}\) is the square of the validity coefficient
\(SD_{y}\) is the standard deviation of the criterion scores

If the validity were perfect (\(r_{xy}^{2}\) = 1.00), the error of estimate would be 0.00.
If the validity were zero, the error of estimate would equal \(SD_{y}\).

Interpreting \(SE_{est}\)

If \(r_{xy}\) = .80, then \(\sqrt{1 - r_{xy}^{2}} = .60\)\\
Error is 60\% as large as it would be by chance.Stated another way, predicting an individual's criterion performance has a margin of error that is 40\% smaller than it would be by chance.

To obtain the \(SE_{est}\), we merely multiply by the \(SD_{y}\). This puts error in the metric of the criterion variable.

Your Turn
If \(r_{xy}\) = .25, then \(\sqrt{1 - r_{xy}^{2}} =\) ??

Make a statement about chance.
Make a statement about margin of error.

\hypertarget{practice-problems-2}{%
\section{Practice Problems}\label{practice-problems-2}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to interpret examine aspects of the construct validity through the creation and interpretation of validity coefficients. Ideally, you will examine both convergent/discriminant validity as well as incremental validity.

\hypertarget{problem-1-play-around-with-this-simulation.}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

If calculating is new to you, perhaps you just change the number in ``set.seed(210907)'' from 210907 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Interpret the validity coefficients\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
25\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-2-conduct-the-reliability-analysis-selecting-different-variables.}{%
\subsection{Problem \#2: Conduct the reliability analysis selecting different variables.}\label{problem-2-conduct-the-reliability-analysis-selecting-different-variables.}}

The Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} article conducted a handful of incremental validity assessments. Select different outcome variables (e.g., depression) and/or use the subscales as the instrument-of-interest.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Interpret the validity coefficients\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
25\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
25\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-3-try-something-entirely-new.}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), create validity coefficients and use three variables to estimate the incremental validity of the instrument-of-interest.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Create a correlation matrix that includes the instrument-of-interest and the variables that will have varying degrees of relation\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Interpret the validity coefficients\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. With at least three variables, evaluate the degree to which the instrument demonstrates incremental validity (this should involve two regression equations and their statistical comparison)\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\centering
25\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{rxx}{%
\chapter{Reliability}\label{rxx}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=b53e4e6f-9c60-47cb-bae9-ad9e00393754}{Screencasted Lecture Link}

The focus of this lecture is the assessment of reliability. We start by defining \emph{classical test theory} and examing several forms of reliability. While the majority of our time is spent considering estimates of internal consistency, we also examine retest reliability and interrater reliability.

\hypertarget{navigating-this-lesson-3}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-3}}

There is one hour and twenty minutes of lecture. If you work through the materials with me it would be plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-3}{%
\subsection{Learning Objectives}\label{learning-objectives-3}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Define ``reliability''
\item
  Identify broad classes of reliability
\item
  Interpret reliability coefficients
\item
  Describe the strengths and limitations of the alpha coefficient
\end{itemize}

\hypertarget{planning-for-practice-3}{%
\subsection{Planning for Practice}\label{planning-for-practice-3}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The practice problems are the start of a larger project that spans multiple lessons. Therefore,if possible, please use a dataset that has item-level data for which there is a theorized total scale score as well as two or more subscales. With each of these options I encourage you to:

\begin{itemize}
\tightlist
\item
  Format (i.e., rescore if necessary) a dataset so that it is possible to calculates estimates of internal consistency
\item
  Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)
\item
  Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.
\item
  Calculate total and subscale scores.
\item
  Describe other reliability estimates that would be appropriate for the measure you are evaluating.
\end{itemize}

I encourage you to use data that allows for the possibility of a total scale score as well as two or more subscales. This will allow you to continue using it in some of the lessons that follow.

\hypertarget{readings-resources-3}{%
\subsection{Readings \& Resources}\label{readings-resources-3}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Jhangiani, R. S., Chiang, I.-C. A., Cuttler, C., \& Leighton, D. C. (2019). Reliability and Validity. In \emph{Research Methods in Psychology}. \url{https://doi.org/10.17605/OSF.IO/HF7DQ}
\item
  Revelle, W., \& Condon, D. M. (2019a). Reliability from α to ω: A tutorial. Psychological Assessment. \url{https://doi.org/10.1037/pas0000754}

  \begin{itemize}
  \tightlist
  \item
    A full-text preprint is available \href{https://personality-project.org/revelle/publications/rc.pa.19.pdf}{here}.
  \end{itemize}
\item
  Revelle, W., \& Condon, D. M. (2019b). Reliability from α to ω: A tutorial. Online supplement. Psychological Assessment. \url{https://doi.org/10.1037/pas0000754}
\item
  Revelle, William. (n.d.). Reliability. In An introduction to psychometric theory with applications in R. Retrieved from \url{http://www.personality-project.org/dev/r/book/\#chapter7}

  \begin{itemize}
  \tightlist
  \item
    All three documents provide a practical integration of conceptual and mechanical.
  \end{itemize}
\item
  Szymanski, D. M., \& Bissonette, D. (2020). Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation. \emph{Journal of Homosexuality}, 67(10), 1412--1428. \url{https://doi.org/10.1080/00918369.2019.1591788}

  \begin{itemize}
  \tightlist
  \item
    The research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-3}{%
\subsection{Packages}\label{packages-3}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(psych))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"psych"}\NormalTok{)\}}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(tidyverse))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: tidyverse
\end{verbatim}

\begin{verbatim}
Warning: package 'tidyverse' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
-- Attaching packages --------------------------------------- tidyverse 1.3.1 --
\end{verbatim}

\begin{verbatim}
v ggplot2 3.3.5     v purrr   0.3.4
v tibble  3.1.4     v dplyr   1.0.7
v tidyr   1.1.3     v stringr 1.4.0
v readr   2.0.1     v forcats 0.5.1
\end{verbatim}

\begin{verbatim}
Warning: package 'ggplot2' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'tibble' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'tidyr' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'readr' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'purrr' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'dplyr' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'stringr' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Warning: package 'forcats' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x ggplot2::%+%()   masks psych::%+%()
x ggplot2::alpha() masks psych::alpha()
x dplyr::filter()  masks stats::filter()
x dplyr::lag()     masks stats::lag()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(MASS))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"MASS"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: MASS
\end{verbatim}

\begin{verbatim}

Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:dplyr':

    select
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(sjstats))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"sjstats"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: sjstats
\end{verbatim}

\begin{verbatim}
Warning: package 'sjstats' was built under R version 4.0.5
\end{verbatim}

\begin{verbatim}
Registered S3 methods overwritten by 'parameters':
  method                           from      
  as.double.parameters_kurtosis    datawizard
  as.double.parameters_skewness    datawizard
  as.double.parameters_smoothness  datawizard
  as.numeric.parameters_kurtosis   datawizard
  as.numeric.parameters_skewness   datawizard
  as.numeric.parameters_smoothness datawizard
  print.parameters_distribution    datawizard
  print.parameters_kurtosis        datawizard
  print.parameters_skewness        datawizard
  summary.parameters_kurtosis      datawizard
  summary.parameters_skewness      datawizard
\end{verbatim}

\begin{verbatim}

Attaching package: 'sjstats'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:psych':

    phi
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(apaTables))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"apaTables"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: apaTables
\end{verbatim}

\begin{verbatim}
Warning: package 'apaTables' was built under R version 4.0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(qualtRics))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"qualtRics"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: qualtRics
\end{verbatim}

\begin{verbatim}
Warning: package 'qualtRics' was built under R version 4.0.5
\end{verbatim}

\hypertarget{defining-reliability}{%
\section{Defining Reliability}\label{defining-reliability}}

\hypertarget{begins-with-classical-test-theory-ctt}{%
\subsection{Begins with Classical Test Theory (CTT)}\label{begins-with-classical-test-theory-ctt}}

CTT is based on Spearman's (1904) \emph{true-score model}, where

\begin{itemize}
\tightlist
\item
  an observed score is conceived of as consisting of two components -- a true component and an error component
\item
  X = T + E

  \begin{itemize}
  \tightlist
  \item
    X = the fallible, observed/manifest score, obtained under ideal or perfect conditions of measurement (these conditions never exist);
  \item
    T = the true/latent score (that will likely remain unknown); and
  \item
    E = random error
  \end{itemize}
\item
  In CTT, we assume that the traits measured are constant and the errors random.

  \begin{itemize}
  \tightlist
  \item
    Therefore, the mean of measurement errors for any individual (upon numerous repeated testings) would be ????.
  \end{itemize}
\item
  That said, in CTT, the true score would be equal to the mean of the observed scores over an indefinite number of repeated measures.

  \begin{itemize}
  \tightlist
  \item
    Caveat: this is based on the assumption that when individuals are repeatedly measured, their true scores remain unchanged.
  \end{itemize}
\item
  In classic test theory, true score can be estimated over multiple trials. However, if errors are systematically biased, the true score will remain unknown.
\end{itemize}

\hypertarget{why-are-we-concerned-with-reliability-error}{%
\subsection{Why are we concerned with reliability? Error}\label{why-are-we-concerned-with-reliability-error}}

\begin{itemize}
\tightlist
\item
  Measurements are imperfect and every observation has some unknown amount of error associated with it. Two components in error:

  \begin{itemize}
  \tightlist
  \item
    \textbf{random/unsystematic}: varies in unpredictable and inconsistent ways upon repeated measurements; sources are unknown
  \item
    \textbf{systematic}: recurs upon repeated measurements reflecting situational or individual effects that, theoretically, could be specified.
  \end{itemize}
\item
  Correlations are attenuated from the true correlation if the observations contain error.
\item
  Knowing the reliability of an instruments allows us to

  \begin{itemize}
  \tightlist
  \item
    estimate the degree to which measured at one time and place with one instrument predict scores at another time and/or place and perhaps measured with a different instrument
  \item
    estimate the consistency of scores
  \item
    estimate ``\ldots the degree to which test scores are free from errors of measurement'' (APA, 1985, p.~19)
  \end{itemize}
\end{itemize}

Figure 7.1a in Revelle's chapter illustrates the \emph{attentuation} of the correlation between the variables \emph{p} and \emph{q} as a function of reliabilty.

\begin{itemize}
\tightlist
\item
  circles (latent variables) represent the \emph{true score}
\item
  observed/measured/manifest variables are represented by squares and each has an associated error; not illustrated are the \emph{random} and \emph{systematic} components of error
\item
  a true score, is composed of a measured variable and its error
\item
  the relationship between the true scores would be stronger than the one between the measured variables
\item
  moving to 7.1b, the correlation between LV p and the observed '' can be estimated from the correlation of p' with a parallel test (this is the reliability piece)
\end{itemize}

Figure 7.2 in Revelle's Chapter 7 \citeyearpar{revelle_introduction_nodate} illustrates the conceptual effect of reliability on the estimation of a true score.

\hypertarget{the-reliability-coefficient}{%
\subsection{The Reliability Coefficient}\label{the-reliability-coefficient}}

The symbol for reliability, \(r_{xx}\), sums up the big-picture definition: that reliability is the correlation of a measure with itself. There are a number of ways to think about it:

\begin{itemize}
\tightlist
\item
  a ``theoretical validity'' of a measure because it refers to a relationship between observed scores and scores on a latent variable or construct,
\item
  represents the fraction of an observed score variance that is not error,
\item
  ranges from 0-1

  \begin{itemize}
  \tightlist
  \item
    1, when all observed variance is due to true-score variance; there are no random errors,
  \item
    0, when all observed variance is due to random errors of measurement,
  \end{itemize}
\item
  represents the squared correlation between observed scores and true scores,
\item
  the ratio between true-score variance and observed score variance (for a formulaic rendition see \citep{pedhazur_measurement_1991}),
\end{itemize}

\[r_{xt}^{2}=r_{xx} =\frac{\sigma_{2}^{t}}{\sigma_{2}^{x}}\]
where
\(r_{xt}^{2}\) is the proportion of variance between observed scores (\emph{t} + \emph{e}) and true scores (\emph{t}); its square root is the correlation

\(r_{xx}\) is the reliability of a measure

\({\sigma_{2}^{t}}\) is the variance of true scores

\({\sigma_{2}^{x}}\) is the variance of observed scores

\begin{itemize}
\tightlist
\item
  The reliability coefficient is interpreted as the proportion of systematic variance in the observed score.

  \begin{itemize}
  \tightlist
  \item
    .8 means that 80\% of the variance of the observed scores is systematic;
  \item
    .2 (e.g., 1.00 - .8)is the proportion of variance due to random errors;
  \item
    the reliability coefficient is population specific.
  \end{itemize}
\end{itemize}

To restate the first portion of the formula: although reliability is expressed as a correlation between observed scores, it is also the ratio of reliable variance to total variance.

\hypertarget{research-vignette-2}{%
\section{Research Vignette}\label{research-vignette-2}}

The research vignette for this lesson is the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}. The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate. Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales composed of the following items:

\begin{itemize}
\tightlist
\item
  College response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students. (cold)
  \item
    My university/college is unresponsive to the needs of LGBTQ students. (unresponsive)
  \item
    My university/college provides a supportive environment for LGBTQ students. {[}un{]}supportive; must be reverse-scored
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus. (negative)
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus. (heterosexism)
  \item
    LGBTQ students are harassed on my university/college campus. (harassed)
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate.Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210827}\NormalTok{)}
\NormalTok{SzyT1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{88}\NormalTok{, }\FloatTok{.73}\NormalTok{, }\FloatTok{.73}\NormalTok{, }\FloatTok{-.07}\NormalTok{,}\OperatorTok{-}\NormalTok{.}\DecValTok{02}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{.10}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.86}\NormalTok{, }\FloatTok{.76}\NormalTok{, }\FloatTok{.71}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{) }\CommentTok{#primary factor loadings for the two factors}
\KeywordTok{rownames}\NormalTok{(SzyT1) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"supportiveNR"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{) }\CommentTok{#variable names for the six items}
\CommentTok{#rownames(Szyf2) <- paste("V", seq(1:6), sep=" ") #prior code I replaced with above}
\KeywordTok{colnames}\NormalTok{(SzyT1) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"F1"}\NormalTok{, }\StringTok{"F2"}\NormalTok{)}
\NormalTok{SzyCorMat <-}\StringTok{ }\NormalTok{SzyT1 }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(SzyT1) }\CommentTok{#create the correlation matrix}
\KeywordTok{diag}\NormalTok{(SzyCorMat) <-}\StringTok{ }\DecValTok{1}
\CommentTok{#SzyCorMat #prints the correlation matrix}
\NormalTok{SzyM <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{2.31}\NormalTok{, }\FloatTok{3.11}\NormalTok{, }\FloatTok{2.40}\NormalTok{, }\FloatTok{3.18}\NormalTok{, }\FloatTok{4.44}\NormalTok{, }\FloatTok{3.02}\NormalTok{) }\CommentTok{#item means}
\NormalTok{SzySD <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.35}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.26}\NormalTok{, }\FloatTok{1.60}\NormalTok{, }\FloatTok{1.75}\NormalTok{, }\FloatTok{1.50}\NormalTok{) }\CommentTok{#item standard deviations; turns out we won't need these since we have a covariance matrix}
\NormalTok{SzyCovMat <-}\StringTok{ }\NormalTok{SzySD }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(SzySD) }\OperatorTok{*}\StringTok{ }\NormalTok{SzyCorMat }\CommentTok{#creates a covariance matrix from the correlation matrix}
\CommentTok{#SzyCovMat #displays the covariance matrix}

\NormalTok{dfSzyT1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{646}\NormalTok{, }\DataTypeTok{mu =}\NormalTok{ SzyM, }\DataTypeTok{Sigma =}\NormalTok{ SzyCovMat, }\DataTypeTok{empirical =} \OtherTok{TRUE}\NormalTok{),}\DecValTok{0}\NormalTok{)) }\CommentTok{#creates the item level data from the sample size, mean, and covariance matrix}
\NormalTok{dfSzyT1[dfSzyT1}\OperatorTok{>}\DecValTok{7}\NormalTok{]<-}\DecValTok{7} \CommentTok{#restricts the upperbound of all variables to be 7 or less}
\NormalTok{dfSzyT1[dfSzyT1}\OperatorTok{<}\DecValTok{1}\NormalTok{]<-}\DecValTok{1} \CommentTok{#resticts the lowerbound of all variable to be 1 or greater}
\CommentTok{#colMeans(dfSzyT1) #displays column means}

\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{row_number}\NormalTok{()) }\CommentTok{#add ID to each row}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1}\OperatorTok{%>%}\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(ID, }\KeywordTok{everything}\NormalTok{())}\CommentTok{#moving the ID number to the first column; requires}
\NormalTok{dfSzyT1<-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{supportive =} \DecValTok{8} \OperatorTok{-}\StringTok{ }\NormalTok{supportiveNR) }\CommentTok{#because the original scale had 1 reversed item, I reversed it so that we can re-reverse it for practice. Remember in reversals we subtract from a number 1 greater than our scaling}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1}\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{supportiveNR)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(dfSzyT1, file="dfSzyT1.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#dfSzyT1 <- read.csv ("dfSzyT1.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(dfSzyT1, "dfSzyT1.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#sdfSzyT1 <- readRDS("dfSzyT1.rds")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfSzyT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             vars   n   mean     sd median trimmed    mad min max range  skew
ID              1 646 323.50 186.63  323.5  323.50 239.44   1 646   645  0.00
cold            2 646   2.42   1.17    2.0    2.34   1.48   1   6     5  0.49
unresponsive    3 646   3.17   1.43    3.0    3.13   1.48   1   7     6  0.20
negative        4 646   3.22   1.52    3.0    3.16   1.48   1   7     6  0.33
heterosexism    5 646   4.41   1.61    4.0    4.45   1.48   1   7     6 -0.20
harassed        6 646   3.07   1.42    3.0    3.02   1.48   1   7     6  0.33
supportive      7 646   5.52   1.14    6.0    5.57   1.48   2   7     5 -0.30
             kurtosis   se
ID              -1.21 7.34
cold            -0.52 0.05
unresponsive    -0.65 0.06
negative        -0.47 0.06
heterosexism    -0.60 0.06
harassed        -0.36 0.06
supportive      -0.76 0.04
\end{verbatim}

If we look at the information about this particular scale, we recognize that the \emph{supportive} item is scaled in the opposite direction of the rest of the items. That is, a higher score on \emph{supportive} would indicate a positive perception of the campus climate for LGBTQ individuals, whereas higher scores on the remaining items indicate a more negative perception. Before moving forward, we must reverse score this item.

In doing this, I will briefly note that in this case I have given my variables one-word names that represent each item. Many researchers (including myself) will often give variable names that are alpha numerical: LGBTQ1, LGBTQ2, LGBTQ\emph{n}. Either is acceptable. In the psychometric case, the one-word names may be useful shortcuts as one begins to understand the inter-item relations.

In reverse-scoring the \emph{supportive} item, I will rename it ``unsupportive'' as an indication of its reversed direction.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{dfSzyT1<-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{unsupportive =} \DecValTok{8} \OperatorTok{-}\StringTok{ }\NormalTok{supportive)}\CommentTok{#when reverse-coding, subtract the variable from one number higher than the scaling}

\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfSzyT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             vars   n   mean     sd median trimmed    mad min max range  skew
ID              1 646 323.50 186.63  323.5  323.50 239.44   1 646   645  0.00
cold            2 646   2.42   1.17    2.0    2.34   1.48   1   6     5  0.49
unresponsive    3 646   3.17   1.43    3.0    3.13   1.48   1   7     6  0.20
negative        4 646   3.22   1.52    3.0    3.16   1.48   1   7     6  0.33
heterosexism    5 646   4.41   1.61    4.0    4.45   1.48   1   7     6 -0.20
harassed        6 646   3.07   1.42    3.0    3.02   1.48   1   7     6  0.33
supportive      7 646   5.52   1.14    6.0    5.57   1.48   2   7     5 -0.30
unsupportive    8 646   2.48   1.14    2.0    2.43   1.48   1   6     5  0.30
             kurtosis   se
ID              -1.21 7.34
cold            -0.52 0.05
unresponsive    -0.65 0.06
negative        -0.47 0.06
heterosexism    -0.60 0.06
harassed        -0.36 0.06
supportive      -0.76 0.04
unsupportive    -0.76 0.04
\end{verbatim}

Next, I will create dfs that each contain the items of the total and subscales. These will be useful in the reliability estimates that follow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, cold, unresponsive, unsupportive, negative, heterosexism, harassed)}
\NormalTok{ResponseT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, cold, unresponsive, unsupportive)}
\NormalTok{StigmaT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, negative, heterosexism, harassed)}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-big-parade-of-reliability-coefficients}{%
\section{The Big Parade of Reliability Coefficients}\label{the-big-parade-of-reliability-coefficients}}

While I cluster the reliability coefficients into large groups, please understand that these are somewhat overlapping.

Table 1 in Revelle and Condon's \citeyearpar{revelle_reliability_2019-1} article provides a summary of of the type of reliability tested, the findings, and the function used in the \emph{psych} package.

\hypertarget{reliability-options-for-a-single-administration}{%
\section{Reliability Options for a Single Administration}\label{reliability-options-for-a-single-administration}}

If reliability is defined as the correlation between a test and a test just like it, how do we estimate the reliability of a single test, given only one time \citep{revelle_william_reliability_nodate}? It may help to keep in mind that reliability is the ratio of true score variance to test score variance (or 1 - the ratio of error variance). Thus, the goal is to estimate the amount of error variance in the test. In this case we can investigate:

\begin{itemize}
\tightlist
\item
  a correlation between two random parts of the test
\item
  internal consistency
\item
  the internal structure of the test
\end{itemize}

\hypertarget{split-half-reliability}{%
\subsection{Split half reliability}\label{split-half-reliability}}

\emph{Split half reliability} is splitting a test into two random halves, correlating the two halves, and adjusting the correlation with the \emph{Spearman-Brown} prophecy formula. Abundant formulaic detail in Revelle's Chapter 7/Reliability \citeyearpar{revelle_william_personality_nodate}.

An important question to split half is ``How to split?'' Revelle terms it a ``combinatorially difficult problem.'' There are 126 possible splits for a 10 item scale, 6,345 possible splits for a 16 item scale, and over 4.5 billion for a 36 item scale! The \emph{psych} package's \emph{splitHalf()} function will try all possible splits for scales of up to 16 items, then sample 10,000 splits for scales longer than that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{split <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{splitHalf}\NormalTok{ (LGBTQT1, }\DataTypeTok{raw =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{brute =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{split }\CommentTok{#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = LGBTQT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.78
Guttman lambda 6                          =  0.73
Average split half reliability            =  0.64
Guttman lambda 3 (alpha)                  =  0.64
Guttman lambda 2                          =  0.7
Minimum split half reliability  (beta)    =  0.04
Average interitem r =  0.23  with median =  0.09
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.17 0.71 0.78
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(split}\OperatorTok{$}\NormalTok{raw,}\DataTypeTok{breaks =} \DecValTok{101}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Split half reliability"}\NormalTok{,}
\DataTypeTok{main =} \StringTok{"Split half reliabilities of 6 LGBTQ items"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-30-1.pdf}

Results of the split-half can provide some indication of whether not the scale is unidimensional.

In this case the maximum reliability coefficient is .78, the average .64, and the lowest is .04. Similarly we can look at the quantiles: .17, .71, .78.

The split-half output also includes the classic Cronbach's (1951) alpha coefficient (.64; aka Guttman lambda 3) and average interitem correlations (.24). The figure plots the frequencies of the reliability coefficient values.

While I did not find guidelines on what constitutes a ``high enough lowerbound'' to establish homogeneity, Revelle suggested that a scale with .85, 80, and .65 had ``strong evidence for a relatively homogeneous scale.'' When the values were .81, .73, .42, Revelle indicated that there was ``strong evidence for non-homogeneity'' \citep[p.~11]{revelle_reliability_2019}. In making this declaration Revelle was also looking at the strength of the inter-item correlation and for a rather tight, bell-shaped distribution, at the higher (\textgreater{} .73) end of the figure. \emph{We don't quite have that.}

What happens when we examine the split-half estimates of the subscales? With only three items, there's not much of a split and so the associated histogram will not be helpful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splitRx <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{splitHalf}\NormalTok{ (ResponseT1, }\DataTypeTok{raw =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{brute =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{splitRx }\CommentTok{#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = ResponseT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.75
Guttman lambda 6                          =  0.72
Average split half reliability            =  0.96
Guttman lambda 3 (alpha)                  =  0.79
Guttman lambda 2                          =  0.79
Minimum split half reliability  (beta)    =  0.69
Average interitem r =  0.56  with median =  0.58
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.69 0.72 0.75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(splitRx}\OperatorTok{$}\NormalTok{raw,}\DataTypeTok{breaks =} \DecValTok{101}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Split half reliability"}\NormalTok{,}
\DataTypeTok{main =} \StringTok{"Split half reliabilities of 3 items of the College Response subscale"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-31-1.pdf}

The alpha is higher -- .79 The range of splits for max, ave, and low are .75, .96, and .69 and the quantiles are 0.69 0.72 0.75. The inter-item correlations have an average of .57.

Let's look at the split half reliabilities for the Stigma subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{splitSt <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{splitHalf}\NormalTok{ (StigmaT1, }\DataTypeTok{raw =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{brute =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{splitSt }\CommentTok{#show the results of the analysis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Split half reliabilities  
Call: psych::splitHalf(r = StigmaT1, raw = TRUE, brute = TRUE)

Maximum split half reliability (lambda 4) =  0.75
Guttman lambda 6                          =  0.72
Average split half reliability            =  0.96
Guttman lambda 3 (alpha)                  =  0.79
Guttman lambda 2                          =  0.79
Minimum split half reliability  (beta)    =  0.7
Average interitem r =  0.56  with median =  0.57
                                             2.5% 50% 97.5%
 Quantiles of split half reliability      =  0.7 0.72 0.75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(splitRx}\OperatorTok{$}\NormalTok{raw,}\DataTypeTok{breaks =} \DecValTok{101}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"Split half reliability"}\NormalTok{,}
\DataTypeTok{main =} \StringTok{"Split half reliabilities of 3 items of the Stigma subscale"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-32-1.pdf}
The maximum, average, and minimum split half reliabilities were .74, .96, and .70; quantiles were at .70, .72, and .74. The average interitem correlation was .56.

Because the alpha coefficient can be defined as the ``average of all possible split-half coefficients'' for the groups tested, it is common for researchers to not provide split half results in their papers. This is true for our research vignette. I continue to teach the split half because it can be a stepping stone in the conceptualization of internal consistency as an estimate of reliability.

\hypertarget{from-alpha}{%
\subsection{From alpha}\label{from-alpha}}

The most common methods to assess internal consistency are the \emph{KR20} (for dichotomous items) and \(\alpha\) (for Likert scaling); alpha has an alias, \(\lambda _{3}\) (the Guttman lambda 3).

Alpha and the Guttman 3 (used for scales with Likert-type scaling) may be thought of as:

\begin{itemize}
\tightlist
\item
  a function of the number of items and the average correlation between the items
\item
  the correlation of a test with a non-existent test just like it
\item
  average of all possible split-half coefficients for the groups tested
\end{itemize}

Although the \emph{psych} package has an incredible and thorough \emph{alpha()} function, Revelle is not a fan of alpha. In fact, his alpha function reports a 95\% CI around alpha as well as bootstrapped alpha results.

Let's grab alpha coefficients for our total and subscales.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{ (LGBTQT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = LGBTQT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.64      0.64    0.73      0.23 1.8 0.023  3.1 0.83    0.089

 lower alpha upper     95% confidence boundaries
0.6 0.64 0.69 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.62      0.60    0.66      0.23 1.5    0.024 0.075 0.111
unresponsive      0.60      0.58    0.67      0.22 1.4    0.024 0.094 0.076
unsupportive      0.63      0.62    0.70      0.25 1.6    0.023 0.083 0.124
negative          0.59      0.62    0.67      0.24 1.6    0.026 0.070 0.124
heterosexism      0.60      0.61    0.69      0.24 1.6    0.026 0.081 0.124
harassed          0.55      0.57    0.68      0.21 1.3    0.030 0.100 0.033

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
cold         646  0.52  0.60  0.53   0.32  2.4 1.2
unresponsive 646  0.60  0.64  0.56   0.37  3.2 1.4
unsupportive 646  0.48  0.56  0.45   0.28  2.5 1.1
negative     646  0.63  0.57  0.50   0.39  3.2 1.5
heterosexism 646  0.64  0.58  0.48   0.39  4.4 1.6
harassed     646  0.70  0.66  0.57   0.50  3.1 1.4

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.26 0.29 0.25 0.14 0.04 0.00 0.00    0
unresponsive 0.15 0.20 0.24 0.23 0.13 0.04 0.01    0
unsupportive 0.24 0.28 0.28 0.16 0.04 0.00 0.00    0
negative     0.15 0.19 0.24 0.22 0.12 0.05 0.02    0
heterosexism 0.05 0.08 0.14 0.25 0.22 0.14 0.12    0
harassed     0.16 0.21 0.24 0.24 0.11 0.02 0.02    0
\end{verbatim}

The second screen of output shows the information we are interested in:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .64 is based on the covariances
\item
  \textbf{std.apha}, .64 is based on correlations
\item
  \textbf{average\_r}, .24 is the average inter-item correlation (i.e., all possible pairwise combinations of items)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(ResponseT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = ResponseT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r
      0.78      0.79    0.72      0.56 3.8 0.015  2.7  1     0.58

 lower alpha upper     95% confidence boundaries
0.75 0.78 0.81 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.64      0.65    0.48      0.48 1.8    0.028    NA  0.48
unresponsive      0.74      0.74    0.58      0.58 2.8    0.021    NA  0.58
unsupportive      0.75      0.76    0.61      0.61 3.1    0.019    NA  0.61

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
cold         646  0.86  0.87  0.78   0.69  2.4 1.2
unresponsive 646  0.86  0.83  0.69   0.61  3.2 1.4
unsupportive 646  0.80  0.82  0.67   0.59  2.5 1.1

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.26 0.29 0.25 0.14 0.04 0.00 0.00    0
unresponsive 0.15 0.20 0.24 0.23 0.13 0.04 0.01    0
unsupportive 0.24 0.28 0.28 0.16 0.04 0.00 0.00    0
\end{verbatim}

In the case of the College Response subscale:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .79 is based on the covariances
\item
  \textbf{std.apha}, .80 is based on correlations
\item
  \textbf{average\_r}, .57 is the average interitem correlation
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(StigmaT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = StigmaT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
      0.79      0.79    0.72      0.56 3.8 0.014  3.6 1.3     0.57

 lower alpha upper     95% confidence boundaries
0.76 0.79 0.82 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
negative          0.66      0.66    0.49      0.49 1.9    0.027    NA  0.49
heterosexism      0.72      0.72    0.57      0.57 2.6    0.022    NA  0.57
harassed          0.76      0.76    0.62      0.62 3.2    0.019    NA  0.62

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
negative     646  0.87  0.87  0.77   0.69  3.2 1.5
heterosexism 646  0.85  0.84  0.71   0.63  4.4 1.6
harassed     646  0.80  0.82  0.66   0.59  3.1 1.4

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
negative     0.15 0.19 0.24 0.22 0.12 0.05 0.02    0
heterosexism 0.05 0.08 0.14 0.25 0.22 0.14 0.12    0
harassed     0.16 0.21 0.24 0.24 0.11 0.02 0.02    0
\end{verbatim}

In the case of the Stigma subscale:

\begin{itemize}
\tightlist
\item
  \textbf{raw\_alpha}, .79 is based on the covariances
\item
  \textbf{std.apha}, .79 is based on correlations
\item
  \textbf{average\_r}, .56 is the average interitem correlation
\end{itemize}

The documentation for this package is incredible. Scroll to near the bottom of the \emph{alpha()} function to learn what these are.

Especially useful are item-level statistics:

\begin{itemize}
\tightlist
\item
  \textbf{r.drop} is the corrected item-total correlation (\protect\hyperlink{ItemAnalSurvey}{in the next lesson}) for this item against the scale without this item
  *,\textbf{mean} and \textbf{sd} are the mean and standard deviation of each item, across all individuals.
\end{itemize}

\textbf{But don't get too excited} the popularity of alpha emerged when tools available for calculation were less sophisticated. Alpha can be misleading:

\begin{itemize}
\tightlist
\item
  alpha inflates, somewhat artificially, even when inter-item correlations are low.

  \begin{itemize}
  \tightlist
  \item
    a 14-item scale will have an alpha of at least .70 even if it has two orthogonal (i.e., unrelated) scales \citep{cortina_what_1993}
  \end{itemize}
\item
  alpha assumes a unidimensional factor structure,
\item
  the same alpha can be obtained for dramatically different underlying factor structures (see graphs in Revelle's Chapter 7)
\end{itemize}

The proper use of alpha requires the following:

\begin{itemize}
\tightlist
\item
  \emph{tau equivalence}, that is, equal covariances with the latent score represented by the test, and
\item
  \emph{unidimensionality}, equal factor loadings on the single factor of the test
\end{itemize}

When either of these is violated, alpha underestimates reliability and overestimates the fraction of test variance that is associated with the general variance in the test.

It is curious that the subscale estimates are stronger than the total scale estimates. This early evidence supports the two-scale solution.

Alpha and the split halves are \emph{internal consistency} estimates. Moving to \emph{model based} techniques allows us to take into consideration the factor structure of the scale. In the original article \citep{szymanski_perceptions_2020}, results were as follows. Note that the alphas are stronger than in our simulation.:

\begin{longtable}[]{@{}lccc@{}}
\toprule
Scale (\emph{n}) & Alpha & Inter-item correlation range & Average inter-item correlation\tabularnewline
\midrule
\endhead
Total (6) & .85 & .27 to .66 & .49\tabularnewline
College Response (3) & .82 & .56 to .67 & .61\tabularnewline
Stigma (3) & .83 & .60 to .66 & .63\tabularnewline
\bottomrule
\end{longtable}

In the article, we can see the boost that alpha gets (.85) when the number of items is double, even though the average inter-item correlation is lower (.49)

\hypertarget{to-omega}{%
\subsection{to Omega}\label{to-omega}}

Assessing reliability with the \emph{omega} (\(\omega\)) statistics falls into a larger realm of \emph{composite reliability} where reliability is assessed from a ratio of the variability explained by the items compared with the total variance of the entire scale \citep{mcneish_thanks_2018}. Members of the omega family of reliability estimates come from factor exploratory (EFA) and confirmatory (CFA; structural equation modeling (SEM) factor analytic approaches. This lesson precedes the lessons on CFA and SEM. Therefore, my explanations and demonstrations will be somewhat brief. I intend to revisit omega output in the CFA and SEM lessons and encourage you to review this section now, then return to this section again after learning more about CFA and SEM.

In the context of \emph{psychometrics} it may be useful (albeit an oversimplification) to think of factors as scales/subscales where \emph{g} refers to the amount of variance in the \emph{general} factor (or total scale score) and subcales to be items that have something in common that is separate from what is \emph{g}.

Model based estimates examine the correlations or covariances of the items and decompose the test variance into that which is

\begin{itemize}
\tightlist
\item
  common to all items (\textbf{g}, a general factor),
\item
  specific to some items (\textbf{f}, orthogonal group factors), and
\item
  unique to each item (confounding \textbf{s} specific, and \textbf{e} error variance)
\end{itemize}

\(\omega\) is something of a shapeshifter. In the \emph{psych} package

\begin{itemize}
\tightlist
\item
  \(\omega_{t}\) represents the total reliability of the test (\(\omega_{t}\))

  \begin{itemize}
  \tightlist
  \item
    In the \emph{psych} package, this is calculated from a bifactor model where there is one general \emph{g} factor (i.e., each item loads on the single general factor), one or more group factors (\emph{f}), and an item-specific factor (\emph{s}).
  \end{itemize}
\item
  \(\omega_{h}\) extracts a higher order factor from the correlation matrix of lower level factors, then applies the Schmid and Leiman (1957) transformation to find the general loadings on the original items. Stated another way, it is a measure o f the general factor saturation (\emph{g}; the amount of variance attributable to one comon factor). The subscript ``h'' acknowledges the hierarchical nature of the approach.

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{h}\) approach is exploratory and defined if there are three or more group factors (with only two group factors, the default is to assume they are equally important, hence the factor loadings of those subscales will be equal)
  \item
    Najera Catalan \citep{najera_catalan_reliability_2019} suggests that \(\omega_{h}\) is the best measure of reliability when dealing with multiple dimensions.
  \end{itemize}
\item
  \(\omega_{g}\) is an estimate that uses a bifactor solution via the SEM package \emph{lavaan} and tends to be a larger (because it forces all the cross loadings of lower level factors to be 0)

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{g}\) is confirmatory, requiring the specification of which variables load on each group factor
  \end{itemize}
\end{itemize}

Two commands in \emph{psych} get us the results:

\begin{itemize}
\tightlist
\item
  \emph{omega()} reports only the EFA solution
\item
  \emph{omegaSem()} reports both EFA and CFA solutions

  \begin{itemize}
  \tightlist
  \item
    We will use the \emph{omegaSem()} function
  \end{itemize}
\end{itemize}

Note that in our specification, we indicate there are two factors. We do not tell it (anywhere!) what items belong to what factors (think, \emph{subscales}). One test will be to see if the items align with their respective factors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{omegaSem}\NormalTok{(LGBTQT1, }\DataTypeTok{nfactors=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required namespace: lavaan
\end{verbatim}

\begin{verbatim}
Loading required namespace: GPArotation
\end{verbatim}

\begin{verbatim}

Three factors are required for identification -- general factor loadings set to be equal. 
Proceed with caution. 
Think about redoing the analysis with alternative values of the 'option' setting.
\end{verbatim}

\begin{verbatim}
Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:
    Could not compute standard errors! The information matrix could
    not be inverted. This may be a symptom that the model is not
    identified.
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-36-1.pdf} \includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-36-2.pdf}

\begin{verbatim}
 
Call: psych::omegaSem(m = LGBTQT1, nfactors = 2)
Omega 
Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, 
    digits = digits, title = title, sl = sl, labels = labels, 
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, 
    covar = covar)
Alpha:                 0.62 
G.6:                   0.72 
Omega Hierarchical:    0.01 
Omega H asymptotic:    0.01 
Omega Total            0.79 

Schmid Leiman Factor loadings greater than  0.2 
                  g   F1*   F2*   h2   u2   p2
cold                 0.85       0.73 0.27 0.01
unresponsive         0.71       0.52 0.48 0.01
unsupportive         0.68       0.46 0.54 0.01
negative-                 -0.84 0.72 0.28 0.01
heterosexism-             -0.73 0.53 0.47 0.01
harassed-                 -0.68 0.48 0.52 0.00

With eigenvalues of:
   g  F1*  F2* 
0.02 1.71 1.71 

general/max  0.01   max/min =   1
mean percent general =  0.01    with sd =  0 and cv of  0.49 
Explained Common Variance of the general factor =  0.01 

The degrees of freedom are 4  and the fit is  0 
The number of observations was  646  with Chi Square =  2.59  with prob <  0.63
The root mean square of the residuals is  0.01 
The df corrected root mean square of the residuals is  0.01
RMSEA index =  0  and the 10 % confidence intervals are  0 0.049
BIC =  -23.29

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 9  and the fit is  1.87 
The number of observations was  646  with Chi Square =  1198.08  with prob <  0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000032
The root mean square of the residuals is  0.36 
The df corrected root mean square of the residuals is  0.46 

RMSEA index =  0.452  and the 10 % confidence intervals are  0.431 0.474
BIC =  1139.84 

Measures of factor score adequacy             
                                                  g  F1*  F2*
Correlation of scores with factors             0.10 0.91 0.90
Multiple R square of scores with factors       0.01 0.82 0.82
Minimum correlation of factor score estimates -0.98 0.64 0.63

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*
Omega total for total scores and subscales    0.79 0.80 0.80
Omega general for total scores and subscales  0.01 0.01 0.01
Omega group for total scores and subscales    0.81 0.79 0.79

 The following analyses were done using the  lavaan  package 

 Omega Hierarchical from a confirmatory model using sem =  0.37
 Omega Total  from a confirmatory model using sem =  0.73 
With loadings of 
                  g   F1*   F2*   h2   u2   p2
cold           0.57  0.65       0.74 0.26 0.44
unresponsive   0.58  0.43       0.52 0.48 0.65
unsupportive   0.46  0.50       0.46 0.54 0.46
negative-                  0.85 0.73 0.27 0.01
heterosexism-              0.72 0.52 0.48 0.00
harassed       0.21       -0.68 0.51 0.49 0.09

With sum of squared loadings of:
   g  F1*  F2* 
0.91 0.86 1.71 

The degrees of freedom of the confirmatory model are  3  and the fit is  12.09965  with p =  0.007049473
general/max  0.53   max/min =   2
mean percent general =  0.27    with sd =  0.28 and cv of  1.01 
Explained Common Variance of the general factor =  0.26 

Measures of factor score adequacy             
                                                  g   F1*  F2*
Correlation of scores with factors             0.67  0.68 0.92
Multiple R square of scores with factors       0.45  0.46 0.84
Minimum correlation of factor score estimates -0.10 -0.08 0.68

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*
Omega total for total scores and subscales    0.73 0.80 0.42
Omega general for total scores and subscales  0.37 0.40 0.04
Omega group for total scores and subscales    0.35 0.39 0.38

To get the standard sem fit statistics, ask for summary on the fitted object
\end{verbatim}

There's a ton of output! How do we make sense of it?

First, our items aligned perfectly with their respective factors (subscales). That is, it would be problematic if the items switched factors.

Second, we can interpret our results. Like alpha, the omegas range from 0 to 1, where values closer to 1 represent good reliability \citep{najera_catalan_reliability_2019}. For unidimensional measures, * \(\omega_{t}\) values above 0.80 seem to be an indicator of good reliability. For multidimensional measures with well-defined dimensions we strive for \(\omega_{h}\) values above 0.65 (and \(\omega_{t}\) \textgreater{} 0.8). These recommendations are based on a Monte Carlo study that examined a host of reliability indicators and how their values corresponded with accurate predictions of poverty status. With this in mind, let's examine the output related to our simulated research vignette.

Let's examine the output in the lower portion where the values are ``from a confirmatory model using sem.''

Omega is a reliability estimate for factor analysis that represents the proportion of variance in the LGBTQ scale attributable to common variance, rather than error. The omega for the total reliability of the test (\(\omega_{t}\); which included the general factors and the subscale factors) was .72, meaning that 72\% of the variance in the total scale is due to the factors and 28\% (100\% - 72\%) is attributable to error.

Omega hierarchical (\(\omega_{h}\)) estimates are the proportion of variance in the LGBTQ score attributable to the general factor, which in effect treats the subscales as error. \(\omega_{h}\) for the the LGBTQ total scale was .40 A quick calculation with \(\omega_{h}\) (.37) and \(\omega_{t}\) (.72; .40/.72 = .56) lets us know that that 56\% of the reliable variance in the LGBTQ total scale is attributable to the general factor.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{.4}\OperatorTok{/}\NormalTok{.}\DecValTok{72}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5555556
\end{verbatim}

Amongst the output is the Cronbach's alpha coefficient (.66). Szymanski and Bissonette \citep{szymanski_perceptions_2020} did not report omega results; this may be because there were only two subfactors and/or they did not feel like a bifactor analysis would be appropriate.

\hypertarget{some-summary-statements-about-reliability-from-single-administrations}{%
\subsection{Some summary statements about reliability from single administrations}\label{some-summary-statements-about-reliability-from-single-administrations}}

\begin{itemize}
\tightlist
\item
  With the exception of the worst split half-reliability and \(\omega_{g}\) or \(\omega_{h}\), all of the reliability estimates are functions of test length and will tend asymptotically towards 1 as the number of items increases
\item
  the omega output provides a great deal more information about reliability than a simple alpha

  \begin{itemize}
  \tightlist
  \item
    Figure 7.5 in Revelle's chapter shows four different structural representations of measures that have equal alphas (all .72)
  \end{itemize}
\item
  \(\omega_{(h)}\), \(\beta\), and the worst split half reliability are estimates of the amount of general factor variance in the test scores
\item
  in the case of low general factor saturation, the EFA based \(\omega_{(h)}\) is positively biased, so the CFA-based estimate, \(\omega_{(g)}\), should be used
\item
  \(\omega_{(t)}\) is the model based estimate of the greatest lower bound of the total reliability of the test; so is the best split half reliability
\end{itemize}

Revelle and Condon's \citep{revelle_reliability_2019} recommendations to researchers:

\begin{itemize}
\tightlist
\item
  report at least two coefficients (e.g., \(\omega_{(h)}\) and \(\omega_{(t)}\)) and discuss why each is appropriate for the inference that is being made,
\item
  report more than ``just alpha'' unless you can demonstrate that the measure is tau equivalent and unidimensional
\end{itemize}

\hypertarget{reliability-options-for-two-or-more-administrations}{%
\section{Reliability Options for Two or more Administrations}\label{reliability-options-for-two-or-more-administrations}}

\hypertarget{test-retest-of-total-scores}{%
\subsection{Test-retest of total scores}\label{test-retest-of-total-scores}}

The purpose of test-retest reliability is to understand the stability of the measure over time. With two time points, T1 and T2, the test-retest correlation is an unknown mixture of trait, state, and specific variance, and is a function of the length of time between two measures.

\begin{itemize}
\tightlist
\item
  With two time points we cannot distinguish between trait and state effects, that said

  \begin{itemize}
  \tightlist
  \item
    we would expect a high degree of stability if the retest is (relatively) immediate
  \end{itemize}
\item
  With three time points we can leverage some SEM tools to distinguish between trait and state components
\item
  A large test-retest correlation over a long period of time indicates temporal stability;

  \begin{itemize}
  \tightlist
  \item
    expected if we are assessing something trait like (e.g., cognitive ability, personality trait)
  \item
    not expected if we are assessing something state like (e.g., emotional state, mood)
  \item
    not expected if there was an intervention (or condition) and the T1 and T2 administrations are part of a pre- and post-test design.
  \end{itemize}
\end{itemize}

There are some \emph{methodological} concerns about test-retest reliability. For example, owing to memory and learning effects, the average response time to a second administration of identical items is about 80\% the time of the first administration.

Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} did not assess retest reliability. We can, though imagine how this might work. Let's imagine that both waves were taken in the same academic term, approximately two weeks apart.

With both sets of data we need to create scores for the total scale score and the two subscales. We would also need to join the two datasets into a single dataframe. We could do either, first. I think I would create the scale scores in each df, separately.

In preparing this lesson, I considered several options. While I could (and actually did, but then deleted it) simulate item-level T2 data, I don't have an easy way to correlate it with the T1 data. The resulting test-retest is absurdly low. So, I will quickly demonstrate how you would score the item-level data for the total and subscale scores, then resimulate scale-level data that is correlated to demonstrate the retest reliability.

The code below presumes that you would have missing data in your raw dataset. Using an available information approach (AIA; \citep{parent_handling_2013}) where is common to allow 20-25\% missingness, we might allow the total scale score to calculate if there is 1 variable missing; but none for the subscale scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQvars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'cold'}\NormalTok{, }\StringTok{'unresponsive'}\NormalTok{, }\StringTok{'negative'}\NormalTok{, }\StringTok{'heterosexism'}\NormalTok{, }\StringTok{'harassed'}\NormalTok{, }\StringTok{'unsupportive'}\NormalTok{)}
\NormalTok{ResponseVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'cold'}\NormalTok{, }\StringTok{'unresponsive'}\NormalTok{, }\StringTok{'unsupportive'}\NormalTok{)}
\NormalTok{Stigmavars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'negative'}\NormalTok{, }\StringTok{'heterosexism'}\NormalTok{, }\StringTok{'harassed'}\NormalTok{)}

\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{TotalT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,LGBTQvars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (this means there must be at least 5 of 6)}
\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{ResponseT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,ResponseVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{StigmaT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,Stigmavars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\end{Highlighting}
\end{Shaded}

We would need to repeat this process with our retest (T2) data, save baby dfs with our scale and total scale scores and then join them.

To demonstrate the retest reliability, I have taken a different path. In order for us to get sensible answers, I went ahead and simulated a new dataset with total and subscale scores for our variables for both waves. This next script is simply that simulation (i.e., you can skip over it).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SimCor_mu <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.13}\NormalTok{, }\FloatTok{2.68}\NormalTok{, }\FloatTok{3.58}\NormalTok{, }\FloatTok{3.16}\NormalTok{, }\FloatTok{2.66}\NormalTok{, }\FloatTok{2.76}\NormalTok{)}
\NormalTok{SimCor_sd <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.82}\NormalTok{, }\FloatTok{1.04}\NormalTok{, }\FloatTok{1.26}\NormalTok{, }\FloatTok{0.83}\NormalTok{, }\FloatTok{1.05}\NormalTok{, }\FloatTok{.99}\NormalTok{)}
\NormalTok{simCor <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{ (}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,  }\FloatTok{0.64}\NormalTok{,   }\FloatTok{0.77}\NormalTok{,   }\FloatTok{0.44}\NormalTok{,   }\FloatTok{0.33}\NormalTok{,   }\FloatTok{0.29}\NormalTok{,}
                    \FloatTok{0.64}\NormalTok{,   }\DecValTok{1}\NormalTok{,  }\FloatTok{0.53}\NormalTok{,   }\FloatTok{0.35}\NormalTok{,   }\FloatTok{0.46}\NormalTok{,   }\FloatTok{0.34}\NormalTok{,}
                    \FloatTok{0.77}\NormalTok{,   }\FloatTok{0.53}\NormalTok{,   }\DecValTok{1}\NormalTok{,  }\FloatTok{0.27}\NormalTok{,   }\FloatTok{0.4}\NormalTok{,    }\FloatTok{0.47}\NormalTok{,}
                    \FloatTok{0.44}\NormalTok{,   }\FloatTok{0.35}\NormalTok{,   }\FloatTok{0.27}\NormalTok{,   }\DecValTok{1}\NormalTok{,  }\FloatTok{0.63}\NormalTok{,   }\FloatTok{0.62}\NormalTok{,}
                    \FloatTok{0.33}\NormalTok{,   }\FloatTok{0.46}\NormalTok{,   }\FloatTok{0.4}\NormalTok{,    }\FloatTok{0.63}\NormalTok{,   }\DecValTok{1}\NormalTok{,  }\FloatTok{0.57}\NormalTok{,}
                    \FloatTok{0.29}\NormalTok{,   }\FloatTok{0.34}\NormalTok{,   }\FloatTok{0.47}\NormalTok{,   }\FloatTok{0.62}\NormalTok{,   }\FloatTok{0.57}\NormalTok{,   }\DecValTok{1}\NormalTok{),}
                  \DataTypeTok{ncol =} \DecValTok{6}\NormalTok{)}
\NormalTok{scovMat <-}\StringTok{ }\NormalTok{SimCor_sd }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(SimCor_sd)}\OperatorTok{*}\NormalTok{simCor}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210829}\NormalTok{)}
\NormalTok{retest_df <-}\StringTok{ }\NormalTok{MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{646}\NormalTok{, }\DataTypeTok{mu =}\NormalTok{ SimCor_mu, }\DataTypeTok{Sigma =}\NormalTok{ scovMat, }\DataTypeTok{empirical =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(retest_df) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"TotalT1"}\NormalTok{, }\StringTok{"ResponseT1"}\NormalTok{, }\StringTok{"StigmaT1"}\NormalTok{, }\StringTok{"TotalT2"}\NormalTok{, }\StringTok{"ResponseT2"}\NormalTok{, }\StringTok{"StigmaT2"}\NormalTok{)}
\NormalTok{retest_df  <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(retest_df) }\CommentTok{#converts to a df so we can use in R}
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{retest_df <-}\StringTok{ }\NormalTok{retest_df }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{row_number}\NormalTok{()) }\CommentTok{#add ID to each row}
\NormalTok{retest_df <-}\StringTok{ }\NormalTok{retest_df }\OperatorTok{%>%}\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(ID, }\KeywordTok{everything}\NormalTok{())}\CommentTok{#moving the ID number to the first column; requires}
\end{Highlighting}
\end{Shaded}

Examing our df, we can see the ID variable and the three sets of scores for each wave of analysis. Now we simply ask for their correlations. There are a number of ways to do this, the \emph{apaTables} package can do the calculations and pop it into a manuscript-ready table.

We won't want the ID variable to be in the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{retest_df2 <-}\StringTok{ }\NormalTok{retest_df }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{ (}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\NormalTok{ID))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\OperatorTok{::}\KeywordTok{apa.cor.table}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ retest_df2, }\DataTypeTok{landscape=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{table.number =} \DecValTok{1}\NormalTok{, }\DataTypeTok{filename=}\StringTok{"Table_1_Retest.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable      M    SD   1          2          3          4         
  1. TotalT1    3.13 0.82                                            
                                                                     
  2. ResponseT1 2.68 1.04 .64**                                      
                          [.59, .68]                                 
                                                                     
  3. StigmaT1   3.58 1.26 .77**      .53**                           
                          [.74, .80] [.47, .58]                      
                                                                     
  4. TotalT2    3.16 0.83 .44**      .35**      .27**                
                          [.38, .50] [.28, .42] [.20, .34]           
                                                                     
  5. ResponseT2 2.66 1.05 .33**      .46**      .40**      .63**     
                          [.26, .40] [.40, .52] [.33, .46] [.58, .67]
                                                                     
  6. StigmaT2   2.76 0.99 .29**      .34**      .47**      .62**     
                          [.22, .36] [.27, .41] [.41, .53] [.57, .67]
                                                                     
  5         
            
            
            
            
            
            
            
            
            
            
            
            
            
            
  .57**     
  [.52, .62]
            

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

As expected in this simulation,

\begin{itemize}
\tightlist
\item
  the strongest correlations are within each scale at their respective time, that is t

  \begin{itemize}
  \tightlist
  \item
    the T1 variables correlate with each other;
  \item
    the T2 variables correlate with each other).
  \end{itemize}
\item
  the next strongest correlations are with the same scale/subscale configuration across time, for example

  \begin{itemize}
  \tightlist
  \item
    TotalT1 with TotalT2
  \item
    ResponseT1 with ResponseT2
  \item
    StigmaT1 with StigmaT2
  \end{itemize}
\item
  the lowest correlations are different scales at T1 and T2

  \begin{itemize}
  \tightlist
  \item
    ResponseT1 with StigmaT2
  \end{itemize}
\end{itemize}

\hypertarget{test-retest-recap}{%
\subsection{Test Retest Recap}\label{test-retest-recap}}

Here are some summary notions for retest reliability.

\begin{itemize}
\tightlist
\item
  increases in the interval will lower the reliability coefficient
\item
  an experimental intervention that is designed to impact the retest assessment will lower the reliability coefficient
\item
  state measures will have lower retest coefficients than trait measures
\item
  and those all interact with each other
\end{itemize}

Note: there are numerous demonstrations in the Revelle and Condon \citetext{\citeyear{revelle_reliability_2019}; \citeyear{revelle_reliability_2019-1}} materials (Table 1). In addition to the myriad of vignettes used to illustrate foci on state, trait, items, whole scale, etc., there were demos on duplicated items, assessing for consistency, and parallel/alternate forms.

If you are asking, ``Hey, is parallel/alternate forms really a variant of test retest?'' Great question! In fact, split-half could be seen as test-retest\ldots{} once you get in the weeds, the distinctions become less clear.

\hypertarget{interrater-reliability}{%
\section{Interrater Reliability}\label{interrater-reliability}}

\hypertarget{cohens-kappa}{%
\subsection{Cohen's kappa}\label{cohens-kappa}}

Cohen's kappa coefficient is used to calculate proportions of agreement corrected for chance. This type of analysis occurs in research designs where there is some kind of (usually) categorical designation of a response. I don't have a research vignette for this. In the past, I was involved in research where members of the research team coded counselor utterances according to Hill's \emph{helping skills} system designed by Clara Hill \citep{hill_helping_2020}. In the helping skills system, 15 different helping skills are divided into three larger groups that generally reflect the counseling trajectory: \emph{exploration}, \emph{insight}, \emph{action.} One of our analyses divided counselor utterances into these categories. Let's look at a fabricated (not based on any real data) simulation where four raters each evaluated 12 counselor utterances (that represent the arch of a nonsensically speedy counseling session).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater3 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{ratings <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Rater1, Rater2, Rater3, Rater4)}
\end{Highlighting}
\end{Shaded}

Historically, kappa could only be calculated for 2 raters at a time. Presently, though, it appears there can be any number of raters and the average agreement is reported.

Let's take a look at the data, then run the analysis, and interpret the results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{cohen.kappa}\NormalTok{(ratings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.
\end{verbatim}

\begin{verbatim}

Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) 
For confidence intervals and detail print with all=TRUE
       Rater1 Rater2 Rater3 Rater4
Rater1   1.00   0.40   0.21   0.62
Rater2   0.14   1.00   0.00   0.57
Rater3   0.48   0.00   1.00   0.30
Rater4   0.54   0.45   0.43   1.00

Average Cohen kappa for all raters  0.34
Average weighted kappa for all raters  0.35
\end{verbatim}

Kappa can range from -1.00 to 1.00.

\begin{itemize}
\tightlist
\item
  K = .00 indicates that the observed agreement is exactly equal to the agreement that could be observed by chance.
\item
  Negative kappa indicates that observed kappa is less than the expected chance agreement.
\item
  K = 1.00 equals perfect agreement between judges.
\end{itemize}

On using kappa:

\begin{itemize}
\tightlist
\item
  research teams set a standard (maybe .85) and ``train up'' until kappa is achieved

  \begin{itemize}
  \tightlist
  \item
    then periodically reassess and retrain
  \end{itemize}
\item
  really difficult to obtain an adequate kappa level when the number of categories achieve

  \begin{itemize}
  \tightlist
  \item
    example is Hill's \emph{Helping Skills System} when all 15 categories (not just the big three) are used
  \end{itemize}
\item
  really difficult to obtain an adequate kappa when \emph{infrequent} categories (e.g., ``insight'') exist
\end{itemize}

Our kappa of .35 indicates that this rating team has a 35\% chance of agreement, corrected for by chance. This is substantially below the standard. Let's imagine that the team spends time with their dictionaries, examines common errors, and makes some decision rules.

Here's the resimulation\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1b <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater2b <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater3b <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{Rater4b <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"exploration"}\NormalTok{,}\StringTok{"insight"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{,}\StringTok{"action"}\NormalTok{ )}
\NormalTok{after_training <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Rater1b, Rater2b, Rater3b, Rater4b)}
\end{Highlighting}
\end{Shaded}

Now run it again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{cohen.kappa}\NormalTok{(after_training)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.
\end{verbatim}

\begin{verbatim}
Warning in psych::cohen.kappa(after_training): No variance detected in cells 2 1
\end{verbatim}

\begin{verbatim}
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.
\end{verbatim}

\begin{verbatim}
Warning in psych::cohen.kappa(after_training): No variance detected in cells 4 1
\end{verbatim}

\begin{verbatim}
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.

Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels =
levels): upper or lower confidence interval exceed abs(1) and set to +/- 1.
\end{verbatim}

\begin{verbatim}
At least one item had no variance.  Try describe(your.data) to find the problem.
\end{verbatim}

\begin{verbatim}

Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) 
For confidence intervals and detail print with all=TRUE
        Rater1b Rater2b Rater3b Rater4b
Rater1b    1.00    0.83    0.55    0.80
Rater2b    0.73    1.00    0.36    0.60
Rater3b    0.72    0.45    1.00    0.46
Rater4b    0.71    0.43    0.70    1.00

Average Cohen kappa for all raters  0.62
Average weighted kappa for all raters  0.6
\end{verbatim}

Hmmm. There was improvement, but this team needs more training!

\hypertarget{intraclass-correlation-icc}{%
\subsection{Intraclass correlation (ICC)}\label{intraclass-correlation-icc}}

Yes! This is the same ICC we used in multilevel modeling! The ICC is used when we have numerical ratings.

In our fabricated vignette below, five raters are evaluating the campus climate for LGBTQIA+ individuals for 10 units/departments on a college campus. Using the ICC can help us determine the degree of leniency and variability within judges.

Here's the resimulation (you can ignore this)\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rater1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater3 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{Rater5 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{ICC_df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Rater1, Rater2, Rater3, Rater4, Rater5)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{ICC}\NormalTok{(ICC_df [}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{], }\DataTypeTok{lmer =} \OtherTok{TRUE}\NormalTok{) }\CommentTok{#find the ICCs for the 10 campus units and 5 judges}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call: psych::ICC(x = ICC_df[1:10, 1:5], lmer = TRUE)

Intraclass correlation coefficients 
                         type  ICC   F df1 df2       p lower bound upper bound
Single_raters_absolute   ICC1 0.34 3.5   9  40 0.00259        0.12        0.64
Single_random_raters     ICC2 0.37 5.4   9  36 0.00011        0.15        0.66
Single_fixed_raters      ICC3 0.47 5.4   9  36 0.00011        0.23        0.74
Average_raters_absolute ICC1k 0.72 3.5   9  40 0.00259        0.40        0.90
Average_random_raters   ICC2k 0.74 5.4   9  36 0.00011        0.47        0.91
Average_fixed_raters    ICC3k 0.81 5.4   9  36 0.00011        0.60        0.93

 Number of subjects = 10     Number of Judges =  5
\end{verbatim}

In the output, reliability for a single judge \(ICC_1\) is the ratio of person variance to total variance. Reliability for multiple judges \(ICC_1k\) adjusts the residual variance by the number of judges.

The ICC function reports six reliability coefficients: 3 for the case of single judges, 3 for the case of multiple judges. It also reports the results in terms of a traditional ANOVA as well as a mixed effects linear model, and CIs for each coefficient.

Like most correlation coefficients, the ICC ranges from 0 to 1.

\begin{itemize}
\tightlist
\item
  An ICC close to 1 indicates high similarity between values from the same group.
\item
  An ICC close to zero means that values from the same group are not similar.
\end{itemize}

\hypertarget{what-do-we-do-with-these-coefficients}{%
\section{What do we do with these coefficients?}\label{what-do-we-do-with-these-coefficients}}

\hypertarget{corrections-for-attenuation}{%
\subsection{Corrections for attenuation}\label{corrections-for-attenuation}}

Circa 1904, Spearman created the reliability coeffient out of a need to adjust observed correlations between related constructs for the error of measurement in each construct. This is only appropriate if the measure is seen as the expected value of a single underlying construct. However, ``under the hood,'' SEM programs model the pattern of observed correlations in terms of a measurement (reliability) model as well as a structural (validity) model.

\hypertarget{predicting-true-scores-and-their-cis}{%
\subsection{Predicting true scores (and their CIs)}\label{predicting-true-scores-and-their-cis}}

True scores remain unknown and so the reliability coefficient is used in a couple of ways to estimate the true score (and the CI around that true score).

Take a quick look at the formula for predicting a true score and observe that the reliability coefficient is used within. It generally serves to nudge the observed score a bit closer to the mean: \(T'=(1-r_{xx})\bar{X}+r_{xx}X\)

The CI around that true score includes some estimate of standard error: \(CI_{95}=T'+/-z_{cv}(s_{e})\)

Whether that term is the standard error of estimate
\(s_{e}=s_{x}\sqrt{r_{xx}(1-r_{xx})}\); standard deviation of predicted true scores for a given observed score),

OR, the standard error of measurement (\(s_{m}=s_{x}\sqrt{(1-r_{xx})}\); an estimate of the amount of variation to be expected in test scores; aka, the standard deviation of the errors of measurement),

the reliability coefficient is also a player.

\emph{I can hear you asking} What is the difference between \(s_{e}\) and \(s_{m}\)?

\begin{itemize}
\tightlist
\item
  Because \(r_{xx}\) is almost always a fraction, \(s_{e}\) is smaller than \(s_{m}\)
\item
  When the reliability is high, the two standard errors are fairly similar to each other.
\item
  Using \(s_{m}\) will result in wider confidence intervals.
\end{itemize}

\hypertarget{how-do-i-keep-it-all-straight}{%
\subsection{How do I keep it all straight?}\label{how-do-i-keep-it-all-straight}}

Table 1 in Revelle and Condon's \citep{revelle_reliability_2019} article helps us connect the the type of reliability we are seeking with the statistic(s) and the R function within the \emph{psych} package.

\hypertarget{practice-problems-3}{%
\section{Practice Problems}\label{practice-problems-3}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The practice problems are the start of a larger project that spans multiple lessons. Therefore,if possible, please use a dataset that has item-level data for which there is a theorized total scale score as well as two or more subscales. With each of these options I encourage you to:

\begin{itemize}
\tightlist
\item
  Format (i.e., rescore if necessary) a dataset so that it is possible to calculates estimates of internal consistency
\item
  Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)
\item
  Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.
\item
  Calculate total and subscale scores.
\item
  Describe other reliability estimates that would be appropriate for the measure you are evaluating.
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-1}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-1}}

If evaluating internal consistency is new to you, copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. Perhaps you just change the number in ``set.seed(210827)'' from 210827 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3.Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Calculate total and subscale scores.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5.Describe other reliability estimates that would be appropriate for the measure you are evaluating.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-2-use-the-data-from-the-live-recentering-psych-stats-survey.}{%
\subsection{Problem \#2: Use the data from the live ReCentering Psych Stats survey.}\label{problem-2-use-the-data-from-the-live-recentering-psych-stats-survey.}}

The script below pulls live data directly from the ReCentering Psych Stats survey on Qualtrics. As described in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Scrubbing and Scoring chapters} of the ReCentering Psych Stats Multivariate Modeling volume, the Perceptions o the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020} was included (LGBTQ) and further adapted to assess perceptions of campus climate for Black students (BLst), non-Black students of color (nBSoC), international students (INTst), and students disabilities (wDIS). Consider conducting the analyses on one of these scales or merging them together.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3.Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Calculate total and subscale scores.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5.Describe other reliability estimates that would be appropriate for the measure you are evaluating.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#only have to run this ONCE to draw from the same Qualtrics account...but will need to get different token if you are changing between accounts }
\KeywordTok{library}\NormalTok{(qualtRics)}
\CommentTok{#qualtrics_api_credentials(api_key = "mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg",}
              \CommentTok{#base_url = "spupsych.az1.qualtrics.com", overwrite = TRUE, install = TRUE)}
\NormalTok{QTRX_df <-qualtRics}\OperatorTok{::}\KeywordTok{fetch_survey}\NormalTok{(}\DataTypeTok{surveyID =} \StringTok{"SV_b2cClqAlLGQ6nLU"}\NormalTok{, }\DataTypeTok{time_zone =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{label=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{convert=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{force_request =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{import_id =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{climate_df <-}\StringTok{ }\NormalTok{QTRX_df}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\StringTok{'Blst_1'}\NormalTok{, }\StringTok{'Blst_2'}\NormalTok{,}\StringTok{'Blst_3'}\NormalTok{,}\StringTok{'Blst_4'}\NormalTok{,}\StringTok{'Blst_5'}\NormalTok{,}\StringTok{'Blst_6'}\NormalTok{,}
         \StringTok{'nBSoC_1'}\NormalTok{, }\StringTok{'nBSoC_2'}\NormalTok{,}\StringTok{'nBSoC_3'}\NormalTok{,}\StringTok{'nBSoC_4'}\NormalTok{,}\StringTok{'nBSoC_5'}\NormalTok{,}\StringTok{'nBSoC_6'}\NormalTok{,}
         \StringTok{'INTst_1'}\NormalTok{, }\StringTok{'INTst_2'}\NormalTok{,}\StringTok{'INTst_3'}\NormalTok{,}\StringTok{'INTst_4'}\NormalTok{,}\StringTok{'INTst_5'}\NormalTok{,}\StringTok{'INTst_6'}\NormalTok{,}
         \StringTok{'wDIS_1'}\NormalTok{, }\StringTok{'wDIS_2'}\NormalTok{,}\StringTok{'wDIS_3'}\NormalTok{,}\StringTok{'wDIS_4'}\NormalTok{,}\StringTok{'wDIS_5'}\NormalTok{,}\StringTok{'wDIS_6'}\NormalTok{,}
         \StringTok{'LGBTQ_1'}\NormalTok{, }\StringTok{'LGBTQ_2'}\NormalTok{,}\StringTok{'LGBTQ_3'}\NormalTok{,}\StringTok{'LGBTQ_4'}\NormalTok{,}\StringTok{'LGBTQ_5'}\NormalTok{,}\StringTok{'LGBTQ_6'}\NormalTok{)}
\CommentTok{#Item numbers are supported with the following items:}
\CommentTok{#_1 "My campus unit provides a supportive environment for ___ students"}
\CommentTok{#_2 "________ is visible in my campus unit"}
\CommentTok{#_3 "Negative attitudes toward persons who are ____ are openly expressed in my campus unit."}
\CommentTok{#_4 "My campus unit is unresponsive to the needs of ____ students."}
\CommentTok{#_5 "Students who are_____ are harassed in my campus unit."}
\CommentTok{#_6 "My campus unit is cold and uncaring toward ____ students."}

\CommentTok{#Item 1 on each subscale should be reverse coded.}
\CommentTok{#The College Response scale is composed of items 1, 4, 6, }
\CommentTok{#The Stigma scale is composed of items 2,3, 5}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(climate_df, file="climate_df.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#climate_df <- read.csv ("climate_df.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(climate_df, "climate_df.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#climate_df <- readRDS("climate_df.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-3-try-something-entirely-new.-1}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-1}}

Complete the same steps using data for which you have permission and access. This might be data of your own, from your lab, simulated from an article, or located on an open repository.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Calculate and report the alpha coefficient for a total scale scores and subscales (if the scale has them)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3.Calculate and report \(\omega_{t}\) and \(\omega_{h}\). With these two determine what proportion of the variance is due to all the factors, error, and \emph{g}.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Calculate total and subscale scores.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5.Describe other reliability estimates that would be appropriate for the measure you are evaluating.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{ItemAnalExam}{%
\chapter{Item Analysis for Educational Achievement Tests (Exams)}\label{ItemAnalExam}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=e7edfd08-439c-4170-be8a-ad9e0167af17}{Screencasted Lecture Link}

In this lecture I walk through some procedures for analyzing the quality of multiple choice (including true/false) exam items. We look at item difficulty and item discrimination. We also look at item coverage as it relates to the learning objectives for an educational endeavor.

\hypertarget{navigating-this-lesson-4}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-4}}

There is about one hour of lecture. If you work through the materials with me it would be plan for an additional 30 minutes.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-4}{%
\subsection{Learning Objectives}\label{learning-objectives-4}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Provide a rationale for why having a \emph{test bank} might be a good idea.
\item
  Describe the effects of skewness on the interpretation of exam results.
\item
  Evaluate the the quality of a multiple choice item on the basis of item difficulty, correlation, and discrimination.
\item
  Discuss the challenges of identifying an \emph{ideal} difficulty level for test items. Further elaborate how guessing, speeded tests, interitem correlations, and the purposes of the test influence the \emph{ideal difficulty.}
\end{itemize}

\hypertarget{planning-for-practice-4}{%
\subsection{Planning for Practice}\label{planning-for-practice-4}}

Practice suggestions for this lesson encourage you to think about the exams in your life: those you might be taking; those you might be writing or proctoring.

\hypertarget{readings-resources-4}{%
\subsection{Readings \& Resources}\label{readings-resources-4}}

Classic psychometric texts tend to not cover item analysis for achievement tests. And/or, they skip over these fundamentals and move straight to item response theory/Rasch modeling (IRT). After scouring the internet, I landed on these two resources as concise, accessible, summaries.

\begin{itemize}
\tightlist
\item
  Understanding item analysis. Office of Educational Assessment, University of Washington. Retrieved September 20, 2019. Retrieved from \url{https://www.washington.edu/assessment/scanning-scoring/scoring/reports/item-analysis/}

  \begin{itemize}
  \tightlist
  \item
    It is common for excellent instructions/descriptions to accompany the scoring software used by institutions. UW appears to use ScorePak and this resource provides both conceptual and interpretive information.
  \end{itemize}
\item
  Revelle, W. (2017). An overview of the psych package. Retrieved from \url{http://personality-project.org/r/overview.pdf}

  \begin{itemize}
  \tightlist
  \item
    Pages 85-85 provide a vignette for conducting item analysis on multiple choice items.
  \end{itemize}
\end{itemize}

\hypertarget{packages-4}{%
\subsection{Packages}\label{packages-4}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(psych))\{install.packages("psych")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{research-vignette-3}{%
\section{Research Vignette}\label{research-vignette-3}}

This lesson's research vignette is from my own class. Especially in the early years of my teaching, I gave high(er) stakes mid-term and final exams. There were usually 40 (or so) multiple choice or true/false items, 2-3 applied problems or short essays, and 1 longer essay. Today's vignette are an array of exam items from a statistics exam that demonstrate the desirable and undesirable elements we want in objective items.

\hypertarget{item-analysis-in-the-educationalachievement-context}{%
\section{Item Analysis in the Educational/Achievement Context}\label{item-analysis-in-the-educationalachievement-context}}

Multiple choice, true/false, and other \emph{objectively} formatted/scored items are part-n-parcel to educational/achievement assessment. But how do we know if the items are performing the way they should? This lecture focuses on item analysis in the context of multiple choice and true/false items. Using these practices can help you identify what selection of items you'd like for your exams. These can be critical tools in helping you improve your ability to assess student performance.\\
In-so-doing, we walk through a bit of ``what we used to do,'' to current common practices, to a glimpse of our future. We owe much of this to rapid advances in technology.

\emph{Test banks} are instructor-created resources for developing/storing/protecting items for use in future exams. We create test banks when we carefully distribute/collect/protect items ``that work'' (from statistical perspective). Why would we want to do this?

\begin{itemize}
\tightlist
\item
  Once a test is ``out'' it's out. Instructors can presume that resourceful students are using it to study; yet all students won't have equal access to it.
\item
  Developing ``good'' items takes a good deal of time; does the instructor want to redo this each term?
\item
  Should we be piloting \emph{all new items} on students each term and then having the debates about whether the item should be rescored?

  \begin{itemize}
  \tightlist
  \item
    Better is to introduce a proportion of new items each year and evaluate them for inclusion in the test bank; EPPP, SAT, GRE do this.
  \end{itemize}
\item
  A challenge is providing students appropriate study tools -- old exams are favorites of students (but maybe there are other ways -- worksheets, Jeopardy).
\end{itemize}

The conceptual portions of this lecture, particularly the interpretation of the difficulty and discrimination statistics are based in Anastasi's work \citep{anastasi_psychological_1997}

\hypertarget{and-now-a-quiz-please-take-it.}{%
\subsection{And now a quiz! Please take it.}\label{and-now-a-quiz-please-take-it.}}

Let's start with some items from an early version of the exam I gave when I taught CPY7020/Statistical Methods.

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

\textbf{Item 37 } Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

\hypertarget{item-difficulty}{%
\section{Item Difficulty}\label{item-difficulty}}

\hypertarget{percent-passing}{%
\subsection{Percent passing}\label{percent-passing}}

\textbf{Item difficulty index} is the proportion of test takers who answer an item correctly. It is calculated by dividing the number of people who passed the item (e.g., 55) by the total number of people (e.g., 100).

\begin{itemize}
\tightlist
\item
  If 55\% pass an item, we write \(p\) = .55
\item
  The easier the item; the larger the percentage will be.
\end{itemize}

What is an ideal pass rate (and this ``ideal'' is the \emph{statistical ideal} mostly for norm-referenced tests like the ACT, SAT, GRE)?

\begin{itemize}
\tightlist
\item
  The closer the difficulty of an item approaches 1.00 or 0, the less differential information about test takers it contributes.

  \begin{itemize}
  \tightlist
  \item
    If, out of 100 people, 50 pass an item and 50 fail (\(p\) = .50)\ldots we have 50 X 50 or 2,500 paired comparisons or differential bits of information.
  \end{itemize}
\item
  How much information would we have for an item passed by:

  \begin{itemize}
  \tightlist
  \item
    70\% of the people (70 * 30 = ???)
  \item
    90\% of the people (90 * 10 = ???)
  \end{itemize}
\item
  For maximum differentiation, one would choose all items at the .50 level (but hold up\ldots)
\end{itemize}

\hypertarget{several-factors-prevent-.50-from-being-the-ideal-difficulty-level}{%
\subsection{Several factors prevent .50 from being the ideal difficulty level}\label{several-factors-prevent-.50-from-being-the-ideal-difficulty-level}}

\textbf{Speeded tests} complicate the interpretation of item difficulty because items are usually of equivalent difficulty and there are so many that no one could complete them all. Thus later items should be considered to be more difficult -- but item difficulty is probably not the best assessment of item/scale quality.

\textbf{Guessing} the correct answer in true/false and multiple choice contexts interferes with the goal of \(p\) - .50. In a 1952 issue of \emph{Psychometrika}, Lord provided this guide for optimal \(p\) values based on the number of choices in the objective context:

\begin{longtable}[]{@{}l@{}}
\toprule
Optimal \emph{p} values\tabularnewline
\midrule
\endhead
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}cc@{}}
\toprule
Number of Choices & Optimal Mean Difficulty Level\tabularnewline
\midrule
\endhead
2 (T/F) & 0.85\tabularnewline
3 & 0.77\tabularnewline
4 & 0.74\tabularnewline
5 & 0.69\tabularnewline
Constructed response essay & 0.5\tabularnewline
\bottomrule
\end{longtable}

\textbf{The purpose} of the testing changes the ideal difficulty level.

\begin{itemize}
\tightlist
\item
  If the test is \emph{norm-referenced} (ACT, SAT, GRE), .50 is very useful.
\item
  If the test is mastery oriented, \(p\) values may be be as high as 0.90 since student performance is a function of repeated attempts with feedback.
\end{itemize}

\textbf{Item intercorrelations} impacts interpretation of item difficulty.

\begin{itemize}
\tightlist
\item
  The more homogeneous the test, the higher these intercorrelations will be. If all items were perfectly intercorrelated and all were of the .50 difficulty level:

  \begin{itemize}
  \tightlist
  \item
    the same 50 persons out of 100 would pass each item, that is,
  \item
    half of the test takers would obtain perfect scores; the other half zero scores
  \end{itemize}
\item
  Best to select items with a moderate spread of difficulty, but whose AVERAGE difficulty level of .50
\item
  The percentage of persons passing an item, expresses the item difficulty in terms of which statistical scale of measurement? (i.e., nominal, ordinal, interval, ratio)

  \begin{itemize}
  \tightlist
  \item
    Because of this issue, we can correctly indicate the rank order or relative difficulty of the items
  \item
    However, we cannot infer that the difference in difficulty between Items 1 and 2 is equal to the difference between Items 2 and 3.
  \end{itemize}
\item
  We can make an \emph{equal-interval inference} with the table of normal curve frequencies (i.e., translating the proportion to z-scores). Z-scores would be used as the units if an equal interval inference was required in the analysis. For example,

  \begin{itemize}
  \tightlist
  \item
    \emph{p} = .84 is equal to -1 \emph{SD}
  \item
    \emph{p} = .16 is equal to +1 \emph{SD}
  \end{itemize}
\end{itemize}

\emph{Seem a little upside down? Recall that we are calculating the percent passing and starting the count ``from the top.'' So a relatively easy item where 84\% passed, would have an standard deviation of -1.}

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/p84p16.jpg}
\caption{Image of graphs where p = .84 and p = .16}
\end{figure}

\hypertarget{item-discrimination}{%
\section{Item Discrimination}\label{item-discrimination}}

The degree to which an item differentiates correctly among test takers in the behavior that the test is designed to measure.

\begin{itemize}
\tightlist
\item
  the \emph{criterion} can be internal or external to the test itself

  \begin{itemize}
  \tightlist
  \item
    under some conditions, the two approaches lead to opposite results because (a) items chosen to maximize the validity of the test tend to be the ones rejected on the basis of internal consistency, and (b) rejecting items with low correlations with the total score tends to homogenize the test (we are more likely to keep items with the highest average intercorrelations)
  \end{itemize}
\item
  \emph{internal} criteria serve to maximize internal consistency or homogeneity of the test, example:

  \begin{itemize}
  \tightlist
  \item
    achievement test, where criteria is total score itself
  \end{itemize}
\item
  \emph{external} criteria serve to maximize the validity of an external criterion, example:

  \begin{itemize}
  \tightlist
  \item
    a different assessment of the same ability being assessed
  \end{itemize}
\end{itemize}

\hypertarget{index-of-discrimination}{%
\subsection{Index of Discrimination}\label{index-of-discrimination}}

\begin{itemize}
\tightlist
\item
  Compare the proportion of cases that pass an item in contrasting criterion groups

  \begin{itemize}
  \tightlist
  \item
    upper (U) and lower (L) criterion groups are selected from the extremes of the distribution
  \item
    traditionally these groups are created from the 27\% from each of those sides of the distribution
  \end{itemize}
\item
  This \emph{index of discrimination (D)} can be expressed as a difference of raw frequencies (U - L), or (more conventionally) as the difference of percentages of those who scored it correctly in the upper 27\% and lower 27\% groups

  \begin{itemize}
  \tightlist
  \item
    when all members of the U group and none of the members of the L group pass, D = 100
  \item
    when all members of the L group and none of the members of the U group pass, D = 0
  \item
    optimum point at which these two conditions reach balance is with the upper and lower 27\%
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}l@{}}
\toprule
Optimal Discrimination\tabularnewline
\midrule
\endhead
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}cc@{}}
\toprule
Difficulty & Discrimination\tabularnewline
\midrule
\endhead
0.40 and larger & Excellent\tabularnewline
0.30 - 0.39 & Good\tabularnewline
0.11 - 0.29 & Fair\tabularnewline
0.00 -0.10 & Poor\tabularnewline
Negative values & Mis-keyed or other major flaw\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{application-of-item-difficulty-and-discrimination}{%
\subsection{Application of Item Difficulty and Discrimination}\label{application-of-item-difficulty-and-discrimination}}

Earlier I asked you to ``take the quiz.'' To keep it engaging, I encourage you to look at your own answers and compare them to ``what happened'' from in this actual exam administration. I will demonstrate how to evaluate my exam items with these indices of difficulty and discrimination. I have intentionally selected items with a variety of desirable (and undesirable) characteristics.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=4.16667in,height=3.125in]{images/ItemAnalExam/ULchart.jpg}
\caption{Image of scores and responses of 6 items from 12 students.}\label{id}
}
\end{figure}

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

If we wanted to hand-calculate the index of discrimination for Item \#5, we find that 3 people (100\%) in the upper group selected the correct answer and 3 people (100\%) in the lower group selected the correct answer: 3 - 3 = 0. If you prefer percentages: 100\% - 100\% = 0\%. This means there is no discrimination in performance of the upper and lower performing groupings.

Older scoring systems (e.g., Scantron) used to provide this information.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=10.41667in,height=2.60417in]{images/ItemAnalExam/Item5.jpg}
\caption{Scantron image of item analysis for exam item \#5}\label{id}
}
\end{figure}

Considering what we have learned already, Item \#5 is

\begin{itemize}
\tightlist
\item
  too easy
\item
  does not discriminate between upper and lower performance
\item
  \emph{Yes, there is more data on here, but we will save it for the next level of review\ldots just a few moments.}
\end{itemize}

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

For Item \#11, 2 people (\textasciitilde66\%) from the upper group selected the correct answer, 1 person (\textasciitilde33\%) from the lower group selected the correct answer. Thus, the U-L was +1 (+33\%) and the item is working in the proper direction.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item11.jpg}
\caption{Scantron image of item analysis for exam item \#11}
\end{figure}

Considering what we have learned already, Item \#11 is

\begin{itemize}
\tightlist
\item
  difficult (50\% overall selected the correct item)
\item
  does discriminate between upper and lower performance, with more individuals in the upper groups selecting the correct answer than in the lower group
\end{itemize}

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 3 (100\%), Lower = 3 (100\%). Difference = 0.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item19.jpg}
\caption{Scantron image of item analysis for exam item \#19}
\end{figure}

Considering what we have learned already, Item \#19 is

\begin{itemize}
\tightlist
\item
  somewhat easy (92\% overall selected the correct item)
\item
  using the U - L discrimination index, it does not discriminate between upper and lower performance
\end{itemize}

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 2 (66\%), Lower = 3 (100\%). Difference = -33\%. This item is upside down. This is different than the Scantron snip below because uppers and lowers were likely calculated on exam total that included subjectively scored items (essays; and I no longer have that data).

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item21.jpg}
\caption{Scantron image of item analysis for exam item \#21}
\end{figure}

Considering what we have learned already, Item21 is

\begin{itemize}
\tightlist
\item
  somewhat difficult (58\% overall selected the correct item)
\item
  on the basis of the hand-calculations it does not discriminate between uppers and lowers
\end{itemize}

\textbf{Item 37} Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 2 (66\%), Lower = 0 (0\%). Difference = 66\%.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item37.jpg}
\caption{Scantron image of item analysis for exam item \#37}
\end{figure}

Considering what we have learned already, Item 37 is

\begin{itemize}
\tightlist
\item
  very difficult (33\% overall selected the correct item)
\item
  on the basis of the hand-calculations, this completely discriminates the uppers from the lowers)
\end{itemize}

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson r (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

Hand calculation: Upper = 1 (33\%), Lower = 1 (33\%). Difference = 0\%.

\begin{figure}
\centering
\includegraphics{images/ItemAnalExam/Item38.jpg}
\caption{Scantron image of item analysis for exam item \#38}
\end{figure}

Considering what we have learned already, Item 21 is

\begin{itemize}
\tightlist
\item
  very difficult (25\% overall selected the correct item)
\item
  on the basis of the hand-calculations, this does not discrimniate the uppers from the lowers)
\end{itemize}

\hypertarget{in-the-psych-package}{%
\section{\texorpdfstring{In the \emph{psych} Package}{In the psych Package}}\label{in-the-psych-package}}

Using the \emph{score.multiple.choice()} function in the \emph{psych} package. Documentation is pp.~85-86 in \url{http://personality-project.org/r/overview.pdf}

A multiple choice exam presumes that there is one correct response. We start with a dataset that records the students' responses. It \emph{appears} that the psych package requires these responses to be numerical (rather than A, B, C, D).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#For portability of the lesson, I hand-entered the exam score data. }
\CommentTok{#Variables are items (not students), so the entry is the 41 items for the 12 students}
\NormalTok{Item1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{Item3 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item4 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item5 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item6 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item7 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item8 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item9 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{Item10 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item11 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item12 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item13 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item14 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item15 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item16 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item17 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item18 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item19 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{Item20 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item21 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{Item22 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item23 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item24 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item25 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item26 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item27 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\NormalTok{Item28 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item29 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item30 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item31 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item32 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item33 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item34 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item35 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item36 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\NormalTok{Item37 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item38 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{Item39 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item40 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{Item41 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{)}

\NormalTok{exam <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(Item1, Item2, Item3, Item4, Item5, Item6, Item7, Item8, Item9, Item10, Item11,Item12, Item13, Item14, Item15, Item16, Item17, Item18, Item19, Item20, Item21, Item22, Item23, Item24,Item25, Item26, Item27, Item28, Item29, Item30, Item31, Item32, Item33, Item34, Item35, Item36, Item37, Item38, Item39, Item40, Item41)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(exam, file="exam.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#exam <- read.csv ("exam.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(exam, "exam.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#exam <- readRDS("exam.rds")}
\end{Highlighting}
\end{Shaded}

We create a key of the correct answers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exam.keys <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

We then insert that key into the \emph{psych} package's \emph{score.multiple.choice()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{score.multiple.choice}\NormalTok{(exam.keys, exam, }\DataTypeTok{score =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{short =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{skew =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in cor(items, scores, use = "pairwise"): the standard deviation is zero
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call: NULL

(Unstandardized) Alpha:
[1] 0.73

Average item correlation:
[1] 0.06

item statistics 
       key    1    2    3    4 miss     r  n mean   sd  skew kurtosis   se
Item1    1 0.92 0.00 0.00 0.08 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item2    4 0.08 0.00 0.00 0.92 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item3    1 0.83 0.00 0.08 0.08 0.00  0.34 12 0.83 0.39 -1.57     0.53 0.11
Item4    2 0.00 0.92 0.08 0.00 0.00 -0.11 12 0.92 0.29 -2.65     5.48 0.08
Item5    1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item6    1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item7    3 0.00 0.08 0.67 0.25 0.00  0.72 12 0.67 0.49 -0.62    -1.74 0.14
Item8    2 0.17 0.67 0.00 0.17 0.00 -0.13 12 0.67 0.49 -0.62    -1.74 0.14
Item9    1 0.67 0.00 0.00 0.33 0.00  0.81 12 0.67 0.49 -0.62    -1.74 0.14
Item10   3 0.00 0.33 0.67 0.00 0.00  0.18 12 0.67 0.49 -0.62    -1.74 0.14
Item11   1 0.50 0.00 0.33 0.17 0.00  0.42 12 0.50 0.52  0.00    -2.16 0.15
Item12   2 0.08 0.83 0.00 0.08 0.00  0.17 12 0.83 0.39 -1.57     0.53 0.11
Item13   2 0.08 0.75 0.17 0.00 0.00  0.85 12 0.75 0.45 -1.01    -1.04 0.13
Item14   2 0.00 0.92 0.08 0.00 0.00 -0.04 12 0.92 0.29 -2.65     5.48 0.08
Item15   2 0.17 0.58 0.08 0.17 0.00  0.32 12 0.58 0.51 -0.30    -2.06 0.15
Item16   2 0.08 0.67 0.00 0.25 0.00  0.40 12 0.67 0.49 -0.62    -1.74 0.14
Item17   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item18   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item19   4 0.08 0.00 0.00 0.92 0.00  0.04 12 0.92 0.29 -2.65     5.48 0.08
Item20   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item21   4 0.00 0.42 0.00 0.58 0.00 -0.19 12 0.58 0.51 -0.30    -2.06 0.15
Item22   3 0.08 0.00 0.92 0.00 0.00  0.34 12 0.92 0.29 -2.65     5.48 0.08
Item23   3 0.00 0.25 0.75 0.00 0.00  0.71 12 0.75 0.45 -1.01    -1.04 0.13
Item24   3 0.17 0.08 0.75 0.00 0.00  0.61 12 0.75 0.45 -1.01    -1.04 0.13
Item25   1 0.00 0.92 0.08 0.00 0.00    NA 12 0.00 0.00   NaN      NaN 0.00
Item26   4 0.17 0.00 0.00 0.83 0.00  0.34 12 0.83 0.39 -1.57     0.53 0.11
Item27   4 0.08 0.00 0.00 0.92 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item28   1 1.00 0.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item29   1 0.92 0.00 0.08 0.00 0.00 -0.11 12 0.92 0.29 -2.65     5.48 0.08
Item30   2 0.00 1.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item31   1 0.75 0.17 0.08 0.00 0.00  0.41 12 0.75 0.45 -1.01    -1.04 0.13
Item32   1 0.83 0.00 0.17 0.00 0.00  0.45 12 0.83 0.39 -1.57     0.53 0.11
Item33   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item34   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item35   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item36   2 0.00 1.00 0.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item37   2 0.25 0.33 0.42 0.00 0.00  0.49 12 0.33 0.49  0.62    -1.74 0.14
Item38   1 0.27 0.00 0.00 0.73 0.08 -0.07 11 0.27 0.47  0.88    -1.31 0.14
Item39   3 0.00 0.00 1.00 0.00 0.00    NA 12 1.00 0.00   NaN      NaN 0.00
Item40   3 0.00 0.00 0.92 0.08 0.00  0.65 12 0.92 0.29 -2.65     5.48 0.08
Item41   4 0.08 0.58 0.00 0.33 0.00  0.40 12 0.33 0.49  0.62    -1.74 0.14
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#short=FALSE allows us to produce scores; we will use these later in some IRT analyses}
\CommentTok{#names(results)}
\end{Highlighting}
\end{Shaded}

The first screen of output provides an alpha. In this context, \emph{alpha} should tell us the consistency of getting answers right or wrong. Technically, the alpha is reduced to a KR-20 (Kuder Richardson 20). We interpret it the same. Alpha is directly effected by:

\begin{itemize}
\tightlist
\item
  \emph{interitem correlations} among the items: a large number of positive correlations between items increases alpha
\item
  test length: more items produce higher reliability (all things being equal)
\item
  test content: the more diverse/broad, the lower the reliability coefficient
\end{itemize}

In the context of the classroom, reliabilities above .70 are probably adequate and above .80 are good. Reliabilities below .60 suggest that items should be investigated and additional measures (tests, homework assignments) should be included in assigning grades.

Focus instead on the second screen of output.

\textbf{key} indicates which answer was correct.

\textbf{1, 2, 3, 4} (there would be as many as there are options in the multiple choice exam) provide a \emph{distractor analysis} by indicating the percentage of time that answer was chosen. For item 1, option 1 was correct, and it was chosen 92\% of the time. No individuals chose options 2 or 3. Option 4 was chosen 8\% of the time.

\textbf{miss} indicates how many times the item was skipped.

\emph{r} is a point-biserial correlation with a dichotomous correct/incorrect correlated with the continuously scaled total scale score. Positively scored items let us know that item is working in the proper direction\ldots the students who got the item correct, did better on the overall total score and vice versa.

\begin{itemize}
\tightlist
\item
  one of the best indicators of an items ability to \emph{discriminate} (hence, \textbf{item discrimination}) among the criterion assessed on the test
\item
  it is important to investigate those with values close to zero (no relation between item performance with overall test performance) and those with negative values (meaning that those who had the correct answer on the item were those who scored lower on the exam).
\end{itemize}

\emph{n} tells us how many participants completed the item (this would necessarily be the inverse of ``miss'').

\emph{mean} repeats the proportion of individuals who scored correctly; it would be the same as the percentage in the item keyed as the correct one. This is an indication of \textbf{item dificulty}.

\emph{sd} gives an indication of the variability around that mean

It is mportant to look at the \emph{r} and \emph{mean} columns, together to understand the degree of difficulty and how well each item is discriminating between performance levels.

\emph{skew} can provide an indication of ceiling and floor effects.

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.25in,height=2.08333in]{images/ItemAnalExam/skew.jpg}
\caption{Image of two graphs illustrating positive and negative skew}\label{id}
}
\end{figure}

If a score has a significant negative skew (long tail to the left), then there may be a piling up of items at the upper end of the scale. This would indicate an \emph{insufficient ceiling} and make it more difficult to discriminate among differences among the higher performers.

If a score has a significant positive skew (long tail to the right), then there may be a piling up of items at the low end, indicating an \emph{insufficient floor.} That is, it lacks the ability to discriminate between poorer performers.

How do you tell what is significant?

A general rule of thumb says that anything greater or less than 1.0 is significantly skewed. A formal z-test can be conducted this way: \(z_{skewness}= \frac{S-0}{SE_{skewness}}\)

In our exam dataset, -2.65 is the most extremely negatively skewed item and its \emph{se} = 0.08.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{-2.65}\OperatorTok{/}\FloatTok{0.08}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -33.125
\end{verbatim}

Considering that anything greater than +/- 1.96 is statistically significant\ldots it is safe to say that this item has an insufficient ceiling.

What about the items with -0.30 (\emph{se} = 0.15)

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{-.30}\OperatorTok{/}\NormalTok{.}\DecValTok{15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2
\end{verbatim}

This is not as extreme (and recall my \emph{N} = 12, so I should probably look up a critical \emph{t} value), but there is still some question about whether my exam items can discriminate among high performers.

Please note, because these distributions are \emph{dichotomous} (correct/incorrect) they will never be normally distributed, but, like the difficulty index, they give another glimpse of the ability to discriminate.

Before we look at the specific exam items and their output from the scoring function, let me introduce you to the features of the psych package that draw from \emph{item response theory} (IRT).

\hypertarget{a-mini-introduction-to-irt}{%
\subsection{A Mini-Introduction to IRT}\label{a-mini-introduction-to-irt}}

To recap -- at the instructional level, the combination of percent passing (mean) and point-biserial correlation (discrimination index) is status quo for evaluating/improving the items.

The \emph{psych} package draws from its IRT capacity to conduct distractor analysis. IRT models individual responses to items by estimating individual ability (theta) and item difficulty (diff) parameters.

In these graphs, theta is on the X axis. Theta is the standard unit of the IRT model that represents the level of the domain being measured. Like a z-score, a theta unit of ``1'' is the SD of the calibrated sample.

The pattern of responses to multiple choice ability items can show that some items have poor distractors. This may be done by using the the irt.responses function. A good distractor is one that is negatively related to ability.

As we look at each of the exam items, we will look at the psych input from the scoring function as well as use the \emph{results} objects to create the IRT graphs.

\textbf{Item 5} A grouping variable such as men or women that uses dummy coding of 1 and 0 to categorize the groups is an example of \_\_\_\_\_ scaling.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Nominal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Ordinal
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Interval
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    Ratio
  \end{enumerate}
\end{itemize}

Mean = 1.0 (much too easy), \emph{r} = NA, Distractors: 1.00 0.00 0.00 0.00, skew = -2.65

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#irt.responses(scores$scores, exam[5], breaks = 2)}
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{5}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/graph item5-1.pdf}

With Item \#5, 100\% responded correctly (the flat, solid line at the top); there is not much to see.

\textbf{Item 11} The term ``grade inflation'' has frequently been applied to describe the distribution of grades in graduate school. Which of the following best describes this distribution.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    negatively skewed
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    uniform/rectangular
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    positively skewed and leptokurtic
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    uniform and platykurtic
  \end{enumerate}
\end{itemize}

Mean = .50, \emph{r} = .42, Distractors: 0.50 0.00 0.33 0.17, skew = 0.00

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{11}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/graph item11-1.pdf}

With Item \#11 there is a positive relationship between 1/A (correct answer) and ability (theta), no relationship between 3/C and ability, and a negative relationship between 4/D and ability (indicating that 4/D is a good distractor). These map onto each of the point-biserial correlations associated with the distractors in the Scantron output.

\textbf{Item 19} All distributions of Z-scores will have the identical

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    Mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    Variance
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    Standard deviation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    All of the above
  \end{enumerate}
\end{itemize}

Mean = .92, \emph{r} = .04, Distractors: 0.08 0.00 0.00 0.92 , skew = -2.65

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{19}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/graph item19-1.pdf}

Item \#19 shows rather flat (no relationship) relations with ability for the correct item and the lone distractor.

\textbf{Item 21} The most appropriate score for comparing scores across two or more distributions (e.g., exam scores in math and art classes) is the:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    mean
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    percentile rank
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    raw score
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    z-score
  \end{enumerate}
\end{itemize}

Mean = .58, \emph{r} = -.19, Distractors: 0.00 0.42 0.00 0.58, skew = -0.30

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{21}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/graph item21-1.pdf}

For Item \#21, a positive relationship between the WRONG answer (2/B) and ability (theta) and a negative relationship between 4/D (incorrect answer) and ability. This makes sense as the point biserial for the overall item was 0-.13.

\textbf{Item 37} Of the following, what statement best describes \(r^2\) = .49

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    strong positive correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    strong positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    weak positive or negative correlation
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{3}
  \tightlist
  \item
    weak negative correlation
  \end{enumerate}
\end{itemize}

Mean = .33, \emph{r} = .49, Distractors: 0.25 0.33 0.42 0.00, skew = .62

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{37}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-58-1.pdf}

For Item 37, a negative relation between endorsing 1/A and ability (a good distractor). No relationshp with ability for endorsing 3/C. A positive relation with ability for those endorsing 2/B 9correct answer).

\textbf{Item 38} When there are no ties among ranks, what is the relationship between the Spearman rho (\(\rho\)) and the Pearson r (\(r\))?

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) = \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\roman{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\rho\) \textgreater{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \tightlist
  \item
    \(\rho\) \textless{} \(r\)
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{\alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    no relationship
  \end{enumerate}
\end{itemize}

Mean = .27, \emph{r} = -.07, Distractors: 0.27 0.00 0.00 0.73, skew = .68\\
\emph{Notice anything else that's funky about Item38?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{irt.responses}\NormalTok{(results}\OperatorTok{$}\NormalTok{scores, exam[}\DecValTok{38}\NormalTok{], }\DataTypeTok{breaks =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\begin{verbatim}
Number of categories should be increased  in order to count frequencies. 
\end{verbatim}

\begin{verbatim}
Warning in rbind(items, dummy): number of columns of result is not a multiple of
vector length (arg 1)
\end{verbatim}

\includegraphics{ReC_Psychometrics_files/figure-latex/graph item38-1.pdf}

For item 38, there is a positive relationship with ability for endorsing 1/A (correct answer) and a negative relationship with ability for 4/D (incorrect answer).

\textbf{Regarding overall test characteristics}
\includegraphics{images/ItemAnalExam/examheader.jpg}

\hypertarget{closing-thoughts-on-developing-measures-in-the-educationachievement-context}{%
\section{Closing Thoughts on Developing Measures in the Education/Achievement Context}\label{closing-thoughts-on-developing-measures-in-the-educationachievement-context}}

Item analysis tends to be an assessment of \emph{reliability}. However, in the context of educational assessment and achievement exams, there are also \emph{validity} issues.

\textbf{Content validity} is concerned with whether or not the scale adequately represents the entirety of the \emph{domain} to be assessed.

In educational and achievement contexts, this is often accomplished with a \emph{table of specifications.} I introduced this in the \protect\hyperlink{rxy}{Validity lesson}. As a refresher, I will include another example -- imagining that I am going to write a quiz or short exam based on the learning objectives of this, single, lesson. There are a number of different ways to organize the types of knowledge that is being assessed. Since the American Psychological Association (and others) work in ``KSAs'' (knowledge, skills, attitudes) in their accreditation standards, I will use those.

In creating a table of specifications, we start with the learning objectives. Then we decide what type of items to write and what type of performance level they satisfy. This helps us ensure that all learning objectives are proportionately covered, using a variety of assessment approaches. Otherwise, we might be tempted to include the items that come easily to us or that are from our favorite topics. Personally, I find that when I work on the exam, and am informed by the learning objectives and table of specifications, I find myself tinkering with all three. I am inclined to believe that this results in an ever-increasingly-improved pedagogy.

\textbf{Table of Specifications}

\begin{longtable}[]{@{}lcccc@{}}
\toprule
\begin{minipage}[b]{0.38\columnwidth}\raggedright
Learning Objectives\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Knowledge\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Skills\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
Attitudes\strut
\end{minipage} & \begin{minipage}[b]{0.12\columnwidth}\centering
\% of test\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Provide a rationale for why having a \emph{test bank} might be a good idea.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
30\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Describe the effects of skewness on the interpretation of exam results.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
2 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
10\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Evaluate the the quality of a multiple choice item on the basis of item difficulty, correlation, and discrimination.\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
5 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
25\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
Discuss the challenges of identifying an \emph{ideal} difficulty level for test items. Further elaborate how guessing, speeded tests, interitem correlations, and the purposes of the test influence the \emph{ideal difficulty.}\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
2 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
1 item\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
35\%\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.38\columnwidth}\raggedright
TOTALS\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
4 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
5 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
2 items\strut
\end{minipage} & \begin{minipage}[t]{0.12\columnwidth}\centering
100\%\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

There are a variety of free resources that help with this process. Below are some that I find helpful:

\begin{itemize}
\tightlist
\item
  \href{https://www.fractuslearning.com/blooms-taxonomy-verbs-free-chart/}{Bloom's Taxonomy Verbs}, freely available from Fractus Learning.
\item
  \href{https://wabisabilearning.com/blogs/literacy-numeracy/download-blooms-digital-taxonomy-verbs-poster}{The Bloom's Taxonomy Verbs Poster for Teachers}
\item
  If you have ``writer's block'' for writing objectives, here is a \href{https://elearn.sitehost.iu.edu/courses/tos/gen2/}{learning outcome generator} that may help get you started.
\item
  From APA's Education Directorate, \href{https://www.apa.org/ed/sponsor/resources/objectives.pdf}{Guidance for Writing Behavioral Learning Objectives}. The APA Guidance really emphasizes key components of well-written behavioral leaning objectives. These include:

  \begin{itemize}
  \tightlist
  \item
    \textbf{observable and measurable}, using action verbs that describe measureable behaviors. The APA CE office disallows the use of ``understand'' as an action verb,
  \item
    statements that clearly describe what the learner will know or be able to do \textbf{as a result} of having participated,
  \item
    focused on the learner and learning (as opposed to what the trainer is doing or leading),
  \item
    appropriate in breadth (not too few or too many)
  \end{itemize}
\end{itemize}

\textbf{Takeaway message}: Together, mapping out exam coverage in a table of specifications PLUS item analysis (difficulty/discrimination) can be powerful tools in educational assessment.

\hypertarget{practice-problems-4}{%
\section{Practice Problems}\label{practice-problems-4}}

For this particular lesson, I think some of the most meaningful practice comes from multiple choice and true/false exams that occur in your life. If you are in a class, see if your instructor is willing to share item analysis information that they have received. Learning management systems like Canvas, automatically calculate these.

If you are an instructor, calculate and review item analysis data on your own items. Think about how you might improve items between exams and cconsider how the dificulty and discrimination capacity of the item changes.

\hypertarget{ItemAnalSurvey}{%
\chapter{Item Analysis for Likert Type Scale Construction}\label{ItemAnalSurvey}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=09cc7469-de42-4288-ad39-ad9e01650656}{Screencasted Lecture Link}

The focus of this lecture is on item analysis for surveys. We use information about alpha coefficients and item-total correlations (within and across subscales) to help assess what we might consider to be \emph{within-scale convergent and discriminant validity} (although we tend to think of it as an assessment of reliability).

\hypertarget{navigating-this-lesson-5}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-5}}

There is about 45 minutes of lecture. If you work through the materials with me it would be plan for an additional hour.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-5}{%
\subsection{Learning Objectives}\label{learning-objectives-5}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Define the corrected item-total correlation and compare it to an item-total correlation.
\item
  List the preliminary steps essential for scale construction, beginning with item development.
\item
  Name the type(s; e.g., reliability, validity) of psychometric evaluation that item analytic procedures assess..
\item
  Identify threats to the interpretation of item-total correlations and alpha coefficients.
\item
  Make decisions about item retention, deletion, and revision that balances statistical output with construct definitions.
\end{itemize}

\hypertarget{planning-for-practice-5}{%
\subsection{Planning for Practice}\label{planning-for-practice-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. For this lesson, please locate item-level data for a scale that has the potential for at least two subscales and a total scale score. Ideally, the data you utilized in one or more of the prior lessons (e.g., changing the random seed in the lesson data, downloading the data from the \emph{ReCentering Psych Stats} survey, or data you found elsewhere) will allow you to continue with these analyses. Then, please examine the following:

\begin{itemize}
\tightlist
\item
  produce alpha coefficients, average inter-item correlations, and corrected item-total correlations for the total and subscales, separately
\item
  produce correlations between the individual items of one subscale and the subscale scores of all other scales
\item
  draft an APA style results section with an accompanying table.
\end{itemize}

In my example there were only two subscales. If you have more, you will need to compare each subscale with all the others. For example, if you had three subscales: A, B, C, you would need to compare A/B, B/C, and A/C.

\hypertarget{readings-resources-5}{%
\subsection{Readings \& Resources}\label{readings-resources-5}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Green \& Salkind (2018). Lesson 38: Item analysis using the reliability Procedure. In S.B. Green and N.J. Salkind's, "Using SPSS for Windows and Macintosh: Analyzing and understanding data (8th ed). New York: Pearson.

  \begin{itemize}
  \tightlist
  \item
    Even though the operation of the chapter uses SPSS, the narration of the ``what'' and ``why'' of item analysis is clear and concise. Further, I have not found another chapter (not even in psychometrics texts) that addresses this as completely.
  \end{itemize}
\item
  Lewis, J. A., \& Neville, H. A. (2015). Construction and initial validation of the Gendered Racial Microaggressions Scale for Black women. \emph{Journal of Counseling Psychology, 62}(2), 289--302. \url{https://doi.org/10.1037/cou0000062}

  \begin{itemize}
  \tightlist
  \item
    Our research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-5}{%
\subsection{Packages}\label{packages-5}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{#if(!require(MASS))\{install.packages("MASS")\}}
\CommentTok{#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{#if(!require(apaTables))\{install.packages("apaTables")\}}
\CommentTok{#if(!require(sjstats))\{install.packages("sjstats")\}}
\CommentTok{#if(!require(qualtRics))\{install.packages("qualtRics")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{intro-to-item-analysis-for-survey-development}{%
\section{Intro to Item Analysis for Survey Development}\label{intro-to-item-analysis-for-survey-development}}

Item analysis can be used to decide which items to include and exclude from a scale or subscale. The goal is to select a set of items that yields a summary score (total or mean) that is strongly related to the construct identified and defined in the scale.

\begin{itemize}
\tightlist
\item
  Item analysis is somewhat limiting because we usually cannot relate our items to a direct (external) measure of a construct to select our items.
\item
  Instead, we \emph{trust} (term used lightly) that the items we have chosen, together, represent the construct and we make decisions about the relative strength of each item's correlation to the total score.
\item
  This makes it imperative that we look to both statistics and our construct definition (e.g., how well does each item map onto the construct definition)
\end{itemize}

\hypertarget{research-vignette-4}{%
\section{Research Vignette}\label{research-vignette-4}}

This lesson will also use the research vignette that presented the development and psychometric evaluation of the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020}.

The scale is six items with responses rated on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicate more negative perceptions of the LGBTQ campus climate. Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} have suggested that the psychometric evaluation supports using the scale in its entirety or as subscales composed of the following items:

\begin{itemize}
\tightlist
\item
  College response to LGBTQ students:

  \begin{itemize}
  \tightlist
  \item
    My university/college is cold and uncaring toward LGBTQ students. (cold)
  \item
    My university/college is unresponsive to the needs of LGBTQ students. (unresponsive)
  \item
    My university/colleg provides a supportive environment for LGBTQ students. {[}un{]}supportive; must be reverse-scored
  \end{itemize}
\item
  LGBTQ Stigma:

  \begin{itemize}
  \tightlist
  \item
    Negative attitudes toward LGBTQ persons are openly expressed on my university/college campus. (negative)
  \item
    Heterosexism, homophobia, biphobia, transphobia, and cissexism are visible on my university/college campus. (heterosexism)
  \item
    LGBTQ students are harassed on my university/college campus. (harassed)
  \end{itemize}
\end{itemize}

A \href{https://www.researchgate.net/publication/332062781_Perceptions_of_the_LGBTQ_College_Campus_Climate_Scale_Development_and_Psychometric_Evaluation/link/5ca0bef945851506d7377da7/download}{preprint} of the article is available at ResearchGate.Below is the script for simulating item-level data from the factor loadings, means, and sample size presented in the published article.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210827}\NormalTok{)}
\NormalTok{SzyT1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{88}\NormalTok{, }\FloatTok{.73}\NormalTok{, }\FloatTok{.73}\NormalTok{, }\FloatTok{-.07}\NormalTok{,}\OperatorTok{-}\NormalTok{.}\DecValTok{02}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{.10}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.86}\NormalTok{, }\FloatTok{.76}\NormalTok{, }\FloatTok{.71}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{) }\CommentTok{#primary factor loadings for the two factors}
\KeywordTok{rownames}\NormalTok{(SzyT1) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"supportiveNR"}\NormalTok{, }\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{) }\CommentTok{#variable names for the six items}
\CommentTok{#rownames(Szyf2) <- paste("V", seq(1:6), sep=" ") #prior code I replaced with above}
\KeywordTok{colnames}\NormalTok{(SzyT1) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"F1"}\NormalTok{, }\StringTok{"F2"}\NormalTok{)}
\NormalTok{SzyCorMat <-}\StringTok{ }\NormalTok{SzyT1 }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(SzyT1) }\CommentTok{#create the correlation matrix}
\KeywordTok{diag}\NormalTok{(SzyCorMat) <-}\StringTok{ }\DecValTok{1}
\CommentTok{#SzyCorMat #prints the correlation matrix}
\NormalTok{SzyM <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{2.31}\NormalTok{, }\FloatTok{3.11}\NormalTok{, }\FloatTok{2.40}\NormalTok{, }\FloatTok{3.18}\NormalTok{, }\FloatTok{4.44}\NormalTok{, }\FloatTok{3.02}\NormalTok{) }\CommentTok{#item means}
\NormalTok{SzySD <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.35}\NormalTok{, }\FloatTok{1.46}\NormalTok{, }\FloatTok{1.26}\NormalTok{, }\FloatTok{1.60}\NormalTok{, }\FloatTok{1.75}\NormalTok{, }\FloatTok{1.50}\NormalTok{) }\CommentTok{#item standard deviations; turns out we won't need these since we have a covariance matrix}
\NormalTok{SzyCovMat <-}\StringTok{ }\NormalTok{SzySD }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(SzySD) }\OperatorTok{*}\StringTok{ }\NormalTok{SzyCorMat }\CommentTok{#creates a covariance matrix from the correlation matrix}
\CommentTok{#SzyCovMat #displays the covariance matrix}

\NormalTok{dfSzyT1 <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{646}\NormalTok{, }\DataTypeTok{mu =}\NormalTok{ SzyM, }\DataTypeTok{Sigma =}\NormalTok{ SzyCovMat, }\DataTypeTok{empirical =} \OtherTok{TRUE}\NormalTok{),}\DecValTok{0}\NormalTok{)) }\CommentTok{#creates the item level data from the sample size, mean, and covariance matrix}
\NormalTok{dfSzyT1[dfSzyT1}\OperatorTok{>}\DecValTok{7}\NormalTok{]<-}\DecValTok{7} \CommentTok{#restricts the upperbound of all variables to be 7 or less}
\NormalTok{dfSzyT1[dfSzyT1}\OperatorTok{<}\DecValTok{1}\NormalTok{]<-}\DecValTok{1} \CommentTok{#resticts the lowerbound of all variable to be 1 or greater}
\CommentTok{#colMeans(dfSzy) #displays column means}

\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{row_number}\NormalTok{()) }\CommentTok{#add ID to each row}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1}\OperatorTok{%>%}\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(ID, }\KeywordTok{everything}\NormalTok{())}\CommentTok{#moving the ID number to the first column; requires}
\NormalTok{dfSzyT1<-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{supportive =} \DecValTok{8} \OperatorTok{-}\StringTok{ }\NormalTok{supportiveNR) }\CommentTok{#because the original scale had 1 reversed item, I reversed it so that we can re-reverse it for practice; reversing means subracting from 1 greater than the scaling of the scale (in our case 1 to 7, so we subtract from 8)}
\NormalTok{dfSzyT1 <-}\StringTok{ }\NormalTok{dfSzyT1}\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{supportiveNR)}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(dfSzyT1, file="dfSzyT1.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#dfSzyT1 <- read.csv ("dfSzyT1.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(dfSzyT1, "dfSzyT1.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#dfSzyT1 <- readRDS("dfSzyT1.rds")}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfSzyT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             vars   n   mean     sd median trimmed    mad min max range  skew
ID              1 646 323.50 186.63  323.5  323.50 239.44   1 646   645  0.00
cold            2 646   2.42   1.17    2.0    2.34   1.48   1   6     5  0.49
unresponsive    3 646   3.17   1.43    3.0    3.13   1.48   1   7     6  0.20
negative        4 646   3.22   1.52    3.0    3.16   1.48   1   7     6  0.33
heterosexism    5 646   4.41   1.61    4.0    4.45   1.48   1   7     6 -0.20
harassed        6 646   3.07   1.42    3.0    3.02   1.48   1   7     6  0.33
supportive      7 646   5.52   1.14    6.0    5.57   1.48   2   7     5 -0.30
             kurtosis   se
ID              -1.21 7.34
cold            -0.52 0.05
unresponsive    -0.65 0.06
negative        -0.47 0.06
heterosexism    -0.60 0.06
harassed        -0.36 0.06
supportive      -0.76 0.04
\end{verbatim}

Although Szymanski and Bissonette report inter-item correlations, it does not appear that they used item analysis to guide their selection of items. In fact, it is not necessary to do so. I like to teach item analysis because I think it provides a conceptual grounding for future lessons on exploratory and confirmatory factor analysis.

\hypertarget{steps-to-item-analysis}{%
\section{Steps to Item Analysis}\label{steps-to-item-analysis}}

If this is initial scale development, the researcher(s) are wise to write more items than needed so that there is flexibility in selecting items with optimal functioning. Szymaski and Bissonette \citeyearpar{szymanski_perceptions_2020} do this. Their article narrates how they began with 36 items, narrowed it to 24, and -- on the basis of subject matter expertise and peer review -- further narrowed it to 10. The reduction of additional items happened on the basis of exploratory factor analysis.

\hypertarget{step-i-corrected-item-total-correlations}{%
\subsection{Step I: Corrected item-total correlations**}\label{step-i-corrected-item-total-correlations}}

\emph{A within-scale version of convergent validity.}

\begin{itemize}
\tightlist
\item
  If needed, transform any items (i.e., reverse-coding) and calculate a total score.
\item
  Calculate \emph{corrected item-total correlations} by correlating each item to the total score LESS the item being evaluated.

  \begin{itemize}
  \tightlist
  \item
    to the degree that the item total represents the construct of interest, the items should be strongly correlated with the corrected total score
  \end{itemize}
\item
  Make decisions about items and scales. For items that have low or negative correlations

  \begin{itemize}
  \tightlist
  \item
    consider deletion
  \item
    consider revision (requires readministering the scale)
  \end{itemize}
\item
  Each time an item is deleted, the process needs to be run again because it changes the total scale score.

  \begin{itemize}
  \tightlist
  \item
    In fact, it's a very iterative process. It could be that you ``add back'' a previously deleted item (once others are deleted) because with each deletion/addition the statistical construct definition is evolving.
  \end{itemize}
\item
  In multidimensional scales, if the total scale score is ever used, researchers should conduct these separately for both the total scale score and the subscales scores.
\end{itemize}

There are reasons to not ``blindly follow the results of an item analysis'' \citep{green_using_2010}.

\begin{itemize}
\tightlist
\item
  \textbf{Method factors} (aka \emph{method effects}) are common methods (e.g., negatively word items, common phrasing such as ``My supervisor tells me'' versus ``I receive feedback'') that are irrelevant to the characteristics or traits being measured -- yet when analyze they share variance \citep{chyung_evidencebased_2018}.
\item
  Adequacy of construct representation. That is, how broad is the construct and to what degree do the items represent the entire construct? If the construct is broad, but there may be a tendency to

  \begin{itemize}
  \tightlist
  \item
    write items on a particular, narrow, aspect of the construct, ignoring others
  \item
    one or more items to strongly correlate, tempting us to delete items that are not as strongly correlated (although they represent the construct)
  \end{itemize}
\end{itemize}

This means we should think carefully and simultaneously about

\begin{itemize}
\tightlist
\item
  statistical properties of the item and overall scale
\item
  construct definition
\item
  scale structure: unidimensional? multidimensional? hierarchical?
\end{itemize}

\textbf{Step II: Correlation of each subscale's items with other subscale totals}
\emph{A within-scale version of discriminant validity.}

\begin{itemize}
\tightlist
\item
  Calculate scale scores for each of the subscales of a measure.
\item
  Focusing on one subscale at a time, correlate each of the subscale's items with the total scores of all the other subscales.
\item
  Comparing to the results of Step I's corrected item-total process, each item should have stronger correlations with its own items (i.e., the corrected item-total correlation) than with the other subscale total scores.
\end{itemize}

\hypertarget{data-prep}{%
\subsection{Data Prep}\label{data-prep}}

Let's do the operational work to get all the pieces we need:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reverse-code the \emph{supportive} variable
\item
  Raw data for each of the scales

  \begin{itemize}
  \tightlist
  \item
    total scale score
  \item
    campus response subscale
  \item
    stigma subscale
  \end{itemize}
\item
  Mean scale scores for total and subscale scores
\item
  A merged dataset with the raw, item-level, data PLUS the three mean scores (total, college response, stigma)
\end{enumerate}

If we look at the information about this particular scale, we recognize that the \emph{supportive} item is scaled in the opposite direction of the rest of the items. That is, a higher score on \emph{supportive} would indicate a positive perception of the campus climate for LGBTQ individuals, whereas higher scores on the remaining items indicate a more negative perception. Before moving forward, we must reverse score this item.

In doing this, I will briefly note that in this case I have given my variables one-word names that represent each item. Many researchers (including myself) will often give variable names that are alpha numerical: LGBTQ1, LGBTQ2, LGBTQ\emph{n}. Either is acceptable. In the psychometrics case, the one-word names may be useful shortcuts as one begins to understand the inter-item relations.

In reverse-scoring the \emph{supportive} item, I will rename it ``unsupportive'' as an indication of its reversed direction.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfSzyT1<-}\StringTok{ }\NormalTok{dfSzyT1 }\OperatorTok{%>%}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{unsupportive =} \DecValTok{8} \OperatorTok{-}\StringTok{ }\NormalTok{supportive) }\CommentTok{#scaling 1 to 7; so we subtract from 8}

\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfSzyT1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             vars   n   mean     sd median trimmed    mad min max range  skew
ID              1 646 323.50 186.63  323.5  323.50 239.44   1 646   645  0.00
cold            2 646   2.42   1.17    2.0    2.34   1.48   1   6     5  0.49
unresponsive    3 646   3.17   1.43    3.0    3.13   1.48   1   7     6  0.20
negative        4 646   3.22   1.52    3.0    3.16   1.48   1   7     6  0.33
heterosexism    5 646   4.41   1.61    4.0    4.45   1.48   1   7     6 -0.20
harassed        6 646   3.07   1.42    3.0    3.02   1.48   1   7     6  0.33
supportive      7 646   5.52   1.14    6.0    5.57   1.48   2   7     5 -0.30
unsupportive    8 646   2.48   1.14    2.0    2.43   1.48   1   6     5  0.30
             kurtosis   se
ID              -1.21 7.34
cold            -0.52 0.05
unresponsive    -0.65 0.06
negative        -0.47 0.06
heterosexism    -0.60 0.06
harassed        -0.36 0.06
supportive      -0.76 0.04
unsupportive    -0.76 0.04
\end{verbatim}

Next, we score the items.In our simulation, we have no missing data. Using an available information approach (AIA; \citep{parent_handling_2013}) where is common to allow 20-25\% missingness, we might allow the total scale score to calculate if there is 1 variable missing; but none for the subscale scores. The \emph{mean\_n()} function in the \emph{sjstats} packages is especially helpul for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQvars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'cold'}\NormalTok{, }\StringTok{'unresponsive'}\NormalTok{, }\StringTok{'negative'}\NormalTok{, }\StringTok{'heterosexism'}\NormalTok{, }\StringTok{'harassed'}\NormalTok{, }\StringTok{'unsupportive'}\NormalTok{)}
\NormalTok{ResponseVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'cold'}\NormalTok{, }\StringTok{'unresponsive'}\NormalTok{, }\StringTok{'unsupportive'}\NormalTok{)}
\NormalTok{Stigmavars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'negative'}\NormalTok{, }\StringTok{'heterosexism'}\NormalTok{, }\StringTok{'harassed'}\NormalTok{)}

\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{TotalT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,LGBTQvars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (this means there must be at least 5 of 6)}
\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{ResponseT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,ResponseVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\NormalTok{dfSzyT1}\OperatorTok{$}\NormalTok{StigmaT1 <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfSzyT1[,Stigmavars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\end{Highlighting}
\end{Shaded}

While we are at it, let's just create tiny dfs with just our variables of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, cold, unresponsive, unsupportive, negative, heterosexism, harassed)}
\NormalTok{ResponseT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, cold, unresponsive, unsupportive)}
\NormalTok{StigmaT1 <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, negative, heterosexism, harassed)}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-item-total-correlation-coefficients}{%
\section{Calculating Item-Total Correlation Coefficients}\label{calculating-item-total-correlation-coefficients}}

Let's first ask, ``Is there support for this instrument as a unidimensional measure?'' To do that, we get an alpha for the whole scale score.

The easiest way to do this is apply the \emph{alpha()} function to a tiny df with the variables in that particular scale or subscale. Any variables should be pre-reversed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LGBTQalpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(LGBTQT1)}\CommentTok{#Although unnecessary, I have saved the output as objects because I will use the objects to create a table }
\NormalTok{LGBTQalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = LGBTQT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.64      0.64    0.73      0.23 1.8 0.023  3.1 0.83    0.089

 lower alpha upper     95% confidence boundaries
0.6 0.64 0.69 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.62      0.60    0.66      0.23 1.5    0.024 0.075 0.111
unresponsive      0.60      0.58    0.67      0.22 1.4    0.024 0.094 0.076
unsupportive      0.63      0.62    0.70      0.25 1.6    0.023 0.083 0.124
negative          0.59      0.62    0.67      0.24 1.6    0.026 0.070 0.124
heterosexism      0.60      0.61    0.69      0.24 1.6    0.026 0.081 0.124
harassed          0.55      0.57    0.68      0.21 1.3    0.030 0.100 0.033

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
cold         646  0.52  0.60  0.53   0.32  2.4 1.2
unresponsive 646  0.60  0.64  0.56   0.37  3.2 1.4
unsupportive 646  0.48  0.56  0.45   0.28  2.5 1.1
negative     646  0.63  0.57  0.50   0.39  3.2 1.5
heterosexism 646  0.64  0.58  0.48   0.39  4.4 1.6
harassed     646  0.70  0.66  0.57   0.50  3.1 1.4

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.26 0.29 0.25 0.14 0.04 0.00 0.00    0
unresponsive 0.15 0.20 0.24 0.23 0.13 0.04 0.01    0
unsupportive 0.24 0.28 0.28 0.16 0.04 0.00 0.00    0
negative     0.15 0.19 0.24 0.22 0.12 0.05 0.02    0
heterosexism 0.05 0.08 0.14 0.25 0.22 0.14 0.12    0
harassed     0.16 0.21 0.24 0.24 0.11 0.02 0.02    0
\end{verbatim}

Examining our list, the overall alpha is 0.64. Further, the average inter-item correlation (\emph{average\_r}) is .23.\\
\emph{And just hold up a minute, I thought you told us alpha was bad!}

\begin{itemize}
\tightlist
\item
  While it is less than ideal, we still use it all the time,

  \begin{itemize}
  \tightlist
  \item
    keeping in mind its relative value (does it increase/decrease, holding other things {[}like sample size{]} constant), and
  \item
    also looking at alpha alternatives (such as we obtained from the omega output)
  \end{itemize}
\item
  Why alpha in this context? Its information about \emph{consistency} is essential. In evaluating a scale's reliability we do want to know if items (unidimensionally or across subscales) are responding consistently high/middle/low.
\end{itemize}

We take note of two columns:

\begin{itemize}
\tightlist
\item
  \emph{r.cor} is the correlation between the item and the total scale score WITH THIS ITEM INCLUDED. When our focus is on the contribution of a specific item, this information is not helpful since this column gets ``extra credit'' for the redundancy of the duplicated item.
\item
  \emph{r.drop} is the corrected item-total correlation. This is the better choice because it deletes the individual item being evaluated (eliminating the redundancy) prior to conducting the correlation.

  \begin{itemize}
  \tightlist
  \item
    Looking at the two columns, notice that the \emph{r.drop} correlations are lower. This is the more honest correlation of the item with the \emph{other} items.
  \item
    In item analysis, we look for items that have relatively high (assessing redundancy or duplication) of items and relatively low (indicating they are unlike the other items) values.
  \end{itemize}
\end{itemize}

If we thought an item was problematic, we could eliminate it and rerun the analysis. Because we are looking at a list of items that ``made the cut,'' we don't have any items that are concerningly high or low. For demonstration purposes, though, the corrected item-total correlation (\emph{r.drop}) of the \emph{unresponsive} variable was the lowest (.28). Let's re-run the analysis excluding this item.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{minus_unresponsive <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfSzyT1, cold,  unsupportive, negative, heterosexism, harassed)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(minus_unresponsive) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in psych::alpha(minus_unresponsive): Some items were negatively correlated with the total scale and probably 
should be reversed.  
To do this, run the function again with the 'check.keys=TRUE' option
\end{verbatim}

\begin{verbatim}
Some items ( cold unsupportive ) were negatively correlated with the total scale and 
probably should be reversed.  
To do this, run the function again with the 'check.keys=TRUE' option
\end{verbatim}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = minus_unresponsive)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
       0.6      0.58    0.67      0.22 1.4 0.024  3.1 0.86    0.076

 lower alpha upper     95% confidence boundaries
0.55 0.6 0.65 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r  med.r
cold              0.64      0.60    0.62      0.27 1.48    0.021 0.104  0.277
unsupportive      0.64      0.60    0.63      0.27 1.50    0.022 0.103  0.290
negative          0.47      0.48    0.55      0.19 0.93    0.035 0.077  0.076
heterosexism      0.48      0.49    0.58      0.19 0.95    0.034 0.093  0.076
harassed          0.44      0.43    0.57      0.16 0.76    0.037 0.117 -0.049

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
cold         646  0.41  0.51  0.36   0.15  2.4 1.2
unsupportive 646  0.40  0.50  0.35   0.15  2.5 1.1
negative     646  0.73  0.67  0.60   0.49  3.2 1.5
heterosexism 646  0.73  0.66  0.57   0.46  4.4 1.6
harassed     646  0.76  0.72  0.63   0.55  3.1 1.4

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.26 0.29 0.25 0.14 0.04 0.00 0.00    0
unsupportive 0.24 0.28 0.28 0.16 0.04 0.00 0.00    0
negative     0.15 0.19 0.24 0.22 0.12 0.05 0.02    0
heterosexism 0.05 0.08 0.14 0.25 0.22 0.14 0.12    0
harassed     0.16 0.21 0.24 0.24 0.11 0.02 0.02    0
\end{verbatim}

Both alpha (.58) and the overall inter-item correlations (\emph{average\_r}; .22) decrease. This decrease in alpha is an example of how sample size can effect the result.

Examining item-level statistics, we do see greater variability (.15 to .55) in the corrected item-total correlations (\emph{r.drop}). What might this mean?

\begin{itemize}
\tightlist
\item
  The item we dropped (\emph{unresponsive}) may be clustering with \emph{cold} and \emph{unsupportive} in a subordinate factor (think subscale).
\item
  Although item analysis is more of a tool in assessing reliability, the statistical information that \emph{unresponsive} provided may broaden the construct definition (definitions are a concern of \emph{validity}) of perceptions of campus climate such that it is necessary to ground/anchor \emph{cold} and \emph{unsupportive}.
\end{itemize}

Tentative conclusion: there is evidence that this is not a unidimensional measure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RESPalpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(ResponseT1)}
\NormalTok{RESPalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = ResponseT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean sd median_r
      0.78      0.79    0.72      0.56 3.8 0.015  2.7  1     0.58

 lower alpha upper     95% confidence boundaries
0.75 0.78 0.81 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
cold              0.64      0.65    0.48      0.48 1.8    0.028    NA  0.48
unresponsive      0.74      0.74    0.58      0.58 2.8    0.021    NA  0.58
unsupportive      0.75      0.76    0.61      0.61 3.1    0.019    NA  0.61

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
cold         646  0.86  0.87  0.78   0.69  2.4 1.2
unresponsive 646  0.86  0.83  0.69   0.61  3.2 1.4
unsupportive 646  0.80  0.82  0.67   0.59  2.5 1.1

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
cold         0.26 0.29 0.25 0.14 0.04 0.00 0.00    0
unresponsive 0.15 0.20 0.24 0.23 0.13 0.04 0.01    0
unsupportive 0.24 0.28 0.28 0.16 0.04 0.00 0.00    0
\end{verbatim}

The alpha for the College Response subscale is .79 much higher than the .64 of the total scale; similarly the average inter-item correlation (\emph{average\_r}) is higher (.56 versus .23).

Examining the corrected item-total correlations indicates a strong correlation with the item (excluded) with the remaining variables (.59 to .69).

Let's look at the Stigma subscale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{STIGalpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(StigmaT1)}
\NormalTok{STIGalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = StigmaT1)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r
      0.79      0.79    0.72      0.56 3.8 0.014  3.6 1.3     0.57

 lower alpha upper     95% confidence boundaries
0.76 0.79 0.82 

 Reliability if an item is dropped:
             raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
negative          0.66      0.66    0.49      0.49 1.9    0.027    NA  0.49
heterosexism      0.72      0.72    0.57      0.57 2.6    0.022    NA  0.57
harassed          0.76      0.76    0.62      0.62 3.2    0.019    NA  0.62

 Item statistics 
               n raw.r std.r r.cor r.drop mean  sd
negative     646  0.87  0.87  0.77   0.69  3.2 1.5
heterosexism 646  0.85  0.84  0.71   0.63  4.4 1.6
harassed     646  0.80  0.82  0.66   0.59  3.1 1.4

Non missing response frequency for each item
                1    2    3    4    5    6    7 miss
negative     0.15 0.19 0.24 0.22 0.12 0.05 0.02    0
heterosexism 0.05 0.08 0.14 0.25 0.22 0.14 0.12    0
harassed     0.16 0.21 0.24 0.24 0.11 0.02 0.02    0
\end{verbatim}

The alpha for the Stigma subscale is also much higher (.79) than the alpha for the total scale (.64); similarly the average inter-item correlation (\emph{average\_r}) is higher (.56 versus .23).

Examining the corrected item-total correlations indicates a strong correlation with the item (excluded) with the remaining variables (.59 to .69).

In addition to needing strong inter-item correlations (which we just assessed) we want the individual items to correlate more strongly with themselves than with the other scale. Let's do that, next.

\hypertarget{correlating-items-with-other-scale-totals}{%
\section{Correlating Items with Other Scale Totals}\label{correlating-items-with-other-scale-totals}}

In this first analysis we will correlate the individual items of the College Response scale to the Stigma subscale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\OperatorTok{::}\KeywordTok{apa.cor.table}\NormalTok{(dfSzyT1[}\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{, }\StringTok{"StigmaT1"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable        M    SD   1           2          3          
  1. cold         2.42 1.17                                   
                                                              
  2. unresponsive 3.17 1.43 .61**                             
                            [.56, .65]                        
                                                              
  3. unsupportive 2.48 1.14 .58**       .48**                 
                            [.53, .63]  [.42, .54]            
                                                              
  4. StigmaT1     3.57 1.28 -.02        .09*       -.02       
                            [-.10, .06] [.01, .16] [-.10, .05]
                                                              

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

We want the items to have higher correlations with each other (.48 to .61) than with the StigmaT1 scale (-.02 to .09) with all three items). These are uncharacteristically low and are influenced by the simulation of raw data from factor loadings and the authors' excellent work in developing, evaluating, and retaining items that reflect two distinct factors.

Let's examine the individual items from the Stigma scale with the College Response subscale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\OperatorTok{::}\KeywordTok{apa.cor.table}\NormalTok{(dfSzyT1[}\KeywordTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{, }\StringTok{"ResponseT1"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Means, standard deviations, and correlations with confidence intervals
 

  Variable        M    SD   1           2           3         
  1. negative     3.22 1.52                                   
                                                              
  2. heterosexism 4.41 1.61 .62**                             
                            [.57, .66]                        
                                                              
  3. harassed     3.07 1.42 .57**       .49**                 
                            [.51, .62]  [.43, .55]            
                                                              
  4. ResponseT1   2.69 1.04 -.05        -.01        .13**     
                            [-.13, .03] [-.09, .07] [.05, .20]
                                                              

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

Again, the inter-item correlations are strong (.49 to .62) while their correlation with the College Response scale is 0.00

\hypertarget{interpreting-and-writing-up-the-results}{%
\section{Interpreting and Writing up the Results}\label{interpreting-and-writing-up-the-results}}

Tabling these results can be really useful to present them effectively. As is customary in APA style tables, when the item is in bold, the value represents its relationship with it's own factor. These values come from the corrected item-total (\emph{r.drop}) values where the item is singled out and correlated with the remaining items in its subscale.

\begin{longtable}[]{@{}l@{}}
\toprule
Item-Total Correlations of Items with their Own and Other Subscales\tabularnewline
\midrule
\endhead
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lcc@{}}
\toprule
Variables & College Response & Stigma\tabularnewline
\midrule
\endhead
cold & \textbf{.69} & -.02\tabularnewline
unresponsive & \textbf{.61} & .09\tabularnewline
unsupportive & \textbf{.59} & -.02\tabularnewline
negative & -.05 & \textbf{.69}\tabularnewline
heterosexism & -.01 & \textbf{.63}\tabularnewline
harassed & .13 & \textbf{.59}\tabularnewline
\bottomrule
\end{longtable}

Although I pitched this type of item-analysis as \emph{reliability}, to some degree it assesses within-scale \textbf{convergent and discriminant validity} because we can see the item relates more strongly to members of its own scale (higher correlation coefficients indicate \emph{convergence}) than to the subscale scores of the other scales. When this pattern occurs, we can argue that the items \emph{discriminate} well.

\textbf{Results}

Item analyses were conducted on the six items hypothesized to assess perceptions of campus climate for members of the LGBTQ community.To assess the within-scale convergent and discrimninant validity of the College Response and Stigma subscales, each item was correlated with its own scale (with the item removed) and with the other coping scale (see Table 1). In all cases, items were more highly correlated with their own scale than with the other scale. Coefficient alphas were .79, .79, and .64 for the College Response, Stigma, and total scale scores, respectively. We concluded that the within-scale convergent and discriminant validity of this measure is strong.

For your consideration:
You are at your dissertation defense. For one of your measures, the Cronbach's alpha is .45. A committee member asks, ``So why was the alpha coefficient so low?'' On the basis of what you have learned in this lesson, how do you respond?

\hypertarget{practice-problems-5}{%
\section{Practice Problems}\label{practice-problems-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. For this lesson, please locate item-level data for a scale that has the potential for at least two subscales and a total scale score. Ideally, you selected such data for practice from the prior lesson. Then, please examine the following:

\begin{itemize}
\tightlist
\item
  produce alpha coefficients, average inter-item correlations, and corrected item-total correlations for the total and subscales, separately
\item
  produce correlations between the individual items of one subscale and the subscale scores of all other scales
\item
  draft an APA style results section with an accompanying table.
\end{itemize}

In my example there were only two subscales. If you have more, you will need to compare each subscale with all the others. For example, if you had three subscales: A, B, C, you would need to compare A/B, B/C, and A/C.

\hypertarget{problem-1-play-around-with-this-simulation.-2}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-2}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

If item analysis is new to you, copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. Perhaps you just change the number in ``set.seed(210827)'' from 210827 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Report alpha coefficients and average inter-item correlations for the total and subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Produce and interpret corrected item-total correlations for total and subscales, separately\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Produce and interpret correlations between the individual items of a given subscale and the subscale scores of all other subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. APA style results section with table\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-2-use-raw-data-from-the-recentering-psych-stats-survey-on-qualtrics.}{%
\subsection{Problem \#2: Use raw data from the ReCentering Psych Stats survey on Qualtrics.}\label{problem-2-use-raw-data-from-the-recentering-psych-stats-survey-on-qualtrics.}}

The script below pulls live data directly from the ReCentering Psych Stats survey on Qualtrics. As described in the \href{https://lhbikos.github.io/ReC_MultivariateModeling/}{Scrubbing and Scoring chapters} of the ReCentering Psych Stats Multivariate Modeling volume, the Perceptions of the LGBTQ College Campus Climate Scale \citep{szymanski_perceptions_2020} was included (LGBTQ) and further adapted to assess perceptions of campus climate for Black students (BLst), non-Black students of color (nBSoC), international students (INTst), and students with disabilities (wDIS). Consider conducting the analyses on one of these scales or merging them together and imagining subscales according to identity/group (LGBTQ, Black, non-Black, disability, international) or College Response and Stigma across the different groups.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Report alpha coefficients and average inter-item correlations for the total and subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Produce and interpret corrected item-total correlations for total and subscales, separately\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Produce and interpret correlations between the individual items of a given subscale and the subscale scores of all other subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. APA style results section with table\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{#only have to run this ONCE to draw from the same Qualtrics account...but will need to get different token if you are changing between accounts }
\KeywordTok{library}\NormalTok{(qualtRics)}
\CommentTok{#qualtrics_api_credentials(api_key = "mUgPMySYkiWpMFkwHale1QE5HNmh5LRUaA8d9PDg",}
              \CommentTok{#base_url = "spupsych.az1.qualtrics.com", overwrite = TRUE, install = TRUE)}
\NormalTok{QTRX_df <-qualtRics}\OperatorTok{::}\KeywordTok{fetch_survey}\NormalTok{(}\DataTypeTok{surveyID =} \StringTok{"SV_b2cClqAlLGQ6nLU"}\NormalTok{, }\DataTypeTok{time_zone =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{label=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{convert=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{force_request =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{import_id =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{climate_df <-}\StringTok{ }\NormalTok{QTRX_df}\OperatorTok{%>%}
\StringTok{  }\KeywordTok{select}\NormalTok{(}\StringTok{'Blst_1'}\NormalTok{, }\StringTok{'Blst_2'}\NormalTok{,}\StringTok{'Blst_3'}\NormalTok{,}\StringTok{'Blst_4'}\NormalTok{,}\StringTok{'Blst_5'}\NormalTok{,}\StringTok{'Blst_6'}\NormalTok{,}
         \StringTok{'nBSoC_1'}\NormalTok{, }\StringTok{'nBSoC_2'}\NormalTok{,}\StringTok{'nBSoC_3'}\NormalTok{,}\StringTok{'nBSoC_4'}\NormalTok{,}\StringTok{'nBSoC_5'}\NormalTok{,}\StringTok{'nBSoC_6'}\NormalTok{,}
         \StringTok{'INTst_1'}\NormalTok{, }\StringTok{'INTst_2'}\NormalTok{,}\StringTok{'INTst_3'}\NormalTok{,}\StringTok{'INTst_4'}\NormalTok{,}\StringTok{'INTst_5'}\NormalTok{,}\StringTok{'INTst_6'}\NormalTok{,}
         \StringTok{'wDIS_1'}\NormalTok{, }\StringTok{'wDIS_2'}\NormalTok{,}\StringTok{'wDIS_3'}\NormalTok{,}\StringTok{'wDIS_4'}\NormalTok{,}\StringTok{'wDIS_5'}\NormalTok{,}\StringTok{'wDIS_6'}\NormalTok{,}
         \StringTok{'LGBTQ_1'}\NormalTok{, }\StringTok{'LGBTQ_2'}\NormalTok{,}\StringTok{'LGBTQ_3'}\NormalTok{,}\StringTok{'LGBTQ_4'}\NormalTok{,}\StringTok{'LGBTQ_5'}\NormalTok{,}\StringTok{'LGBTQ_6'}\NormalTok{)}
\CommentTok{#Item numbers are supported with the following items:}
\CommentTok{#_1 "My campus unit provides a supportive environment for ___ students"}
\CommentTok{#_2 "________ is visible in my campus unit"}
\CommentTok{#_3 "Negative attitudes toward persons who are ____ are openly expressed in my campus unit."}
\CommentTok{#_4 "My campus unit is unresponsive to the needs of ____ students."}
\CommentTok{#_5 "Students who are_____ are harassed in my campus unit."}
\CommentTok{#_6 "My campus unit is cold and uncaring toward ____ students."}

\CommentTok{#Item 1 on each subscale should be reverse coded.}
\CommentTok{#The College Response scale is composed of items 1, 4, 6, }
\CommentTok{#The Stigma scale is composed of items 2,3, 5}
\end{Highlighting}
\end{Shaded}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(climate_df, file="climate_df.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#climate_df <- read.csv ("climate_df.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(climate_df, "climate_df.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#climate_df <- readRDS("climate_df.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-3-try-something-entirely-new.-2}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-2}}

Complete the same steps using data for which you have permission and access. This might be data of your own, from your lab, simulated from an article, or located on an open repository.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Report alpha coefficients and average inter-item correlations for the total and subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Produce and interpret corrected item-total correlations for total and subscales, separately\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Produce and interpret correlations between the individual items of a given subscale and the subscale scores of all other subscales\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. APA style results section with table\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
30\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{bonus-reel}{%
\section{Bonus Reel:}\label{bonus-reel}}

\begin{figure}
\centering
\includegraphics{images/film-strip-1.jpg}
\caption{Image of a filmstrip}
\end{figure}

For our interpretation and results, I created the table by manually typing in the results. Since there were only two subscales, this was easy. However, it can be a very useful skill (and prevent typing errors) by leveraging R's capabilities to build a table.

The script below

\begin{itemize}
\tightlist
\item
  Creates a correlation matrix of the items of each scale and correlates them with the ``other'' subscale, separately for both subscales.
\item
  Extracts the r.drop from each subscale
\item
  Joins (adds more variables) the analyses across the corrected item-total and item-other subscale analyses
\item
  Binds (adds more cases) the two sets of items together
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Resp_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfSzyT1[}\KeywordTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{, }\StringTok{"ResponseT1"}\NormalTok{)])}\CommentTok{#Run the correlation of the subscale and the items that are *not* on the subscale}
\NormalTok{Resp_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Resp_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#extracts the "r" matrix and makes it a df}
\NormalTok{Resp_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{, }\StringTok{"ResponseT1"}\NormalTok{)}\CommentTok{#Assigning names to the items}
\NormalTok{Resp_othR <-}\StringTok{ }\NormalTok{Resp_othR[}\OperatorTok{!}\NormalTok{Resp_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "ResponseT1"}\NormalTok{,]}\CommentTok{#Removing the subscale score as a a row in the df}
\NormalTok{Resp_othR[, }\StringTok{'StigmaT1'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA} \CommentTok{#We need a column for this to bind the items later}
\NormalTok{Resp_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Resp_othR, Items, ResponseT1, StigmaT1) }\CommentTok{#All we need is the item name and the correlations with the subscales}
\NormalTok{RESPalpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(RESPalpha}\OperatorTok{$}\NormalTok{item.stats)}\CommentTok{#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{RESPalpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{)}

\NormalTok{Stig_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfSzyT1[}\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{, }\StringTok{"StigmaT1"}\NormalTok{)])}\CommentTok{#Run the correlation of the subscale and the items that are *not* on the subscale}
\NormalTok{Stig_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Stig_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#extracts the "r" matrix and makes it a df}
\NormalTok{Stig_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"cold"}\NormalTok{, }\StringTok{"unresponsive"}\NormalTok{, }\StringTok{"unsupportive"}\NormalTok{, }\StringTok{"StigmaT1"}\NormalTok{)}\CommentTok{#Assigning names to the items}
\NormalTok{Stig_othR <-}\StringTok{ }\NormalTok{Stig_othR[}\OperatorTok{!}\NormalTok{Stig_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "StigmaT1"}\NormalTok{,]}\CommentTok{#Removing the subscale score as a a row in the df}
\NormalTok{Stig_othR[, }\StringTok{'ResponseT1'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA} \CommentTok{#We need a column for this to bind the items later}
\NormalTok{Stig_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Stig_othR, Items, ResponseT1, StigmaT1) }\CommentTok{#All we need is the item name and the correlations with the subscales}
\NormalTok{STIGalpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(STIGalpha}\OperatorTok{$}\NormalTok{item.stats)}\CommentTok{#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{STIGalpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"negative"}\NormalTok{, }\StringTok{"heterosexism"}\NormalTok{, }\StringTok{"harassed"}\NormalTok{)}

\CommentTok{#Combining these four dfs}
\NormalTok{ResponseStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(RESPalpha, Stig_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{ResponseStats}\OperatorTok{$}\NormalTok{ResponseT1 <-}\StringTok{ }\NormalTok{ResponseStats}\OperatorTok{$}\NormalTok{r.drop}
\NormalTok{ResponseStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(ResponseStats, Items, ResponseT1, StigmaT1)}
\NormalTok{StigmaStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(STIGalpha, Resp_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{StigmaStats}\OperatorTok{$}\NormalTok{StigmaT1 <-}\StringTok{ }\NormalTok{StigmaStats}\OperatorTok{$}\NormalTok{r.drop}
\NormalTok{StigmaStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(StigmaStats, Items, ResponseT1, StigmaT1)}
\NormalTok{ItemAnalyses <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(ResponseStats, StigmaStats)}
\NormalTok{ItemAnalyses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Items   ResponseT1    StigmaT1
1         cold  0.691836225 -0.01933630
2 unresponsive  0.611240042  0.08696749
3 unsupportive  0.586410276 -0.02454147
4     negative -0.051291879  0.68802067
5 heterosexism -0.009656154  0.62875558
6     harassed  0.128551812  0.58709766
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Writing them to a .csv file allows post-r formatting}
\KeywordTok{write.csv}\NormalTok{(ItemAnalyses, }\DataTypeTok{file =} \StringTok{"LGBTQ_Climate_ItemAnalyses.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in write.csv(ItemAnalyses, file = "LGBTQ_Climate_ItemAnalyses.csv", :
attempt to set 'col.names' ignored
\end{verbatim}

\begin{verbatim}
Warning in write.csv(ItemAnalyses, file = "LGBTQ_Climate_ItemAnalyses.csv", :
attempt to set 'sep' ignored
\end{verbatim}

\hypertarget{exploratory-factor-analysis-1}{%
\chapter*{\texorpdfstring{EXPLORATORY \emph{FACTOR} ANALYSIS}{EXPLORATORY FACTOR ANALYSIS}}\label{exploratory-factor-analysis-1}}
\addcontentsline{toc}{chapter}{EXPLORATORY \emph{FACTOR} ANALYSIS}

The next two lessons are devoted to exploratory \emph{factor} analysis. The two approaches are principal components analysis (PCA) and principal axis factoring (PAF). In truth, only PAF is considered factor analysis. I will explain why in the lesson.

These approaches are loosely termed \emph{exploratory} because the statistical process (not the researcher) produces the factor (think scale or subscale) and identifies which items belong to it. This is contrasted with \emph{confirmatory} approaches (which use structural equation modeling) where the researcher assigns items to factors and analyzes the goodness of fit.

\hypertarget{PCA}{%
\chapter{EFA: Principal Components Analysis}\label{PCA}}

\href{link}{Screencasted Lecture Link}

In this lesson on principal components analysis (PCA) I provide an introduction to the exploratory factor analysis (EFA) arena. We will review the theoretical and technical aspects of PCA, we will work through a research vignette, and then consider the relationship of PCA to item analysis and reliability coefficients.

Please note, although PCA is frequently grouped into EFA techniques, it is \emph{exploratory} but it is not \emph{factor analysis}. We'll discuss the difference in the lecture.

\hypertarget{navigating-this-lesson-6}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-6}}

There is AMOUNT OF TIME of lecture. If you work through the materials with me it would be plan for an additional MORE TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-6}{%
\subsection{Learning Objectives}\label{learning-objectives-6}}

Focusing on this week's materials, make sure you can:

\emph{Distinguish between PCA and PAF on several levels:\\
- which path diagram represents each best
- keywords associated with each: factor loadings, linear components, describe versus explain.
} Recognize/define an identity matrix -- what test would you use to diagnose it?
* Recognize/define multicollinearity and singularity -- what test would you use to diagnose it?
* Describe the pattern of ``loadings'' (i.e., the relative weights of an item on its own scale compared to other scales)that supports the structure of the instrument.
* Compare the results from item analysis and PCA.

\hypertarget{planning-for-practice-6}{%
\subsection{Planning for Practice}\label{planning-for-practice-6}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's \citeyearpar{szymanski_perceptions_2020}Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PCA.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. These could include other vignettes from this OER, other simualated data, or your own data (presuming you have permissoin to use it). In either case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of components to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of components).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data.
\end{itemize}

\hypertarget{readings-resources-6}{%
\subsection{Readings \& Resources}\label{readings-resources-6}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In \emph{An introduction to psychometric theory with applications in R}. Retrieved from \url{https://personality-project.org/r/book/\#chapter6}

  \begin{itemize}
  \tightlist
  \item
    pp.~145 to 150 (we'll continue with the rest in the next lecture). Stop at ``6.2 Exploratory Factor Analysis.''
  \item
    A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts.
  \end{itemize}
\item
  Revelle, W. (2019). \emph{How To: Use the psych package for Factor Analysis and data reduction}.

  \begin{itemize}
  \tightlist
  \item
    pp.~13 throuh 24 provide technical information about what we are doing
  \end{itemize}
\item
  Dekay, Nicole (2020). How to Make a Survey: A Quick Reference Guide on Psychometric Steps and Statistics. \url{https://www.linkedin.com/posts/humanalysts_psychometricians-psychometrics-statistics-activity-6739634099712999424-VbUm}
\item
  Lewis, J. A., \& Neville, H. A. (2015). Construction and initial validation of the Gendered Racial Microaggressions Scale for Black women. \emph{Journal of Counseling Psychology, 62}(2), 289--302. \url{https://doi.org/10.1037/cou0000062}

  \begin{itemize}
  \tightlist
  \item
    Our research vignette for this lesson.
  \end{itemize}
\end{itemize}

\hypertarget{packages-6}{%
\subsection{Packages}\label{packages-6}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{#if(!require(MASS))\{install.packages("MASS")\}}
\CommentTok{#if(!require(sjstats))\{install.packages("sjstats")\}}
\CommentTok{#if(!require(apaTables))\{install.packages("apaTables")\}}
\CommentTok{#if(!require(qualtRics))\{install.packages("qualtRics")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploratory-principal-components-analysis}{%
\section{Exploratory Principal Components Analysis}\label{exploratory-principal-components-analysis}}

The psychometric version of \emph{parsimony} is seen in our attempt to \emph{describe} (components) or to \emph{explain} (factors) in the relationships between many observed variables in terms of a more limited set of components or latent factors.

That is, we are trying to

\begin{itemize}
\tightlist
\item
  understand the structure of a set of variables,
\item
  construct a questionnaire to measure an underlying latent variable, and
\item
  reduce a data set to a more manageable size (think fewer columns\ldots scale scores) while retainng as much of the information as possible
\end{itemize}

\hypertarget{some-framing-ideas-in-very-lay-terms}{%
\subsection{Some framing ideas (in very lay terms)}\label{some-framing-ideas-in-very-lay-terms}}

\emph{Exploratory} versus \emph{confirmatory} factor analysis.

\begin{itemize}
\item
  Both exploratory and confirmatory approaches to components/factor analysis are used in scale construction. Think of ``scales'' as being interchangeable with ``factors'' and ``components.''

  \begin{itemize}
  \tightlist
  \item
    That said, ``factors'' and ``components'' are not interchangeable terms.
  \end{itemize}
\item
  \textbf{Exploratory}: Even though we may have an a priori model in mind, we \emph{explore} the structure of the items by using diagnostics (KMO, Barlett's, determinant), factor extraction, and rotation to determine the number of scales (i.e., components or factors) that exist within the raw data or correlation matrix. The algorithms (including matrix algebra) determine the relationship of each item to its respective scales (i.e., components or factors).
\item
  \textbf{Confirmatory}: Starting with an a priori theory, we specify the structure (i.e., number and levels of factors) and which items belong to factors. We use structural equation modeling as the framework. And we only work with ``factors.'' We evaluate the quality of the model with a number of fit indices.
\end{itemize}

Within the \emph{exploratory} category we will focus on two further distinctions (there are even more): component analysis (principal components, or PCA) and principal axis factoring (PAF; one of the approaches that is commonly termed exploratory factor analysis, or EFA). In this first lecturette we focus on the differences between PCA and EFA.

\begin{itemize}
\item
  \textbf{Option \#1/Component model}: PCA approximates the correlation matrix in terms of the product of components where each is a weighted linear sum of the variables. In the figure below, note how the arrows in the Components Analysis (a \emph{path} model) point from variables to the component. Perhaps an oversimplification, think of each of these as a predictor variable contributing to an outcome.
\item
  \textbf{Option \#2/Factor model}: EFA (and in the next lesson, PAF/principal axis factoring) approximates the correlation matrix by the product of the two factors; this approach presumes that the factors are the causes (rather than as consequences). In the figure below, note how the arrows in the Factor Analysis model (a \emph{structural} model) point from latent variable (or factor) to the observed variables (items). Factor analysis has been termed \emph{causal modeling} because the latent variables are theorized to cause the responses to the individual items. There are other popular approaches, including parallel analysis (which is what the authors used in this lesson's research vignette).
\end{itemize}

Well-crafted figures provide important clues to the analyses. In structural models, rectangles and squares indicate the presence of \emph{observed} (also called \emph{manifest}) variables. These are variables that have a column in the dataset. In our particular case, they are the responses to the 25 items in the GRMS.

Circles or ovals represent latent variables or factors. These were never raw data, but are composed of the relations of variables that were collected. They are more complex than mean or sum scores. Rather, they represent what the variables that represent them share in common.

\begin{figure}
\centering
\includegraphics{images/PCA/PCAvPAF.png}
\caption{Comparison of path models for PCA and EFA for our research vignette}
\end{figure}

Our focus today is on the principal component analysis (PCA) approach to scale construction.

\hypertarget{pca-workflow}{%
\section{PCA Workflow}\label{pca-workflow}}

Below is a screenshot of the workflow. The original document is located in the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the ReCentering Psych Stats: Psychometrics OER.

\begin{figure}
\centering
\includegraphics{images/PCA/PCAworkflow.png}
\caption{Image of the workflow for PCA}
\end{figure}

Steps in the process include:

\begin{itemize}
\tightlist
\item
  Creating an items only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored).
\item
  Conducting tests that assess the statistical assumptions of PCA to ensure that the data is appropriate for PCA.
\item
  Determining the number of components (think ``subscales'') to extract.
\item
  Conducting the component extraction -- this process will likely occur iteratively,

  \begin{itemize}
  \tightlist
  \item
    exploring orthogonal (uncorrelated/independent) and oblique (correlated)components, and
  \item
    changing the number of components to extract
  \end{itemize}
\end{itemize}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PCA can be quite complex. Some important notions to consider that may not be obvious from lesson, are these:

\begin{itemize}
\tightlist
\item
  The values of component loadings are directly related to the correlation matrix.

  \begin{itemize}
  \tightlist
  \item
    Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix.
  \end{itemize}
\item
  PCA is about \emph{dimension reduction}; our goal is fewer components (think subscales) than there are items.

  \begin{itemize}
  \tightlist
  \item
    In this lesson's vignette there are 25 items on the scale and we will have 4 subscales.
  \end{itemize}
\item
  Principal component analysis is \emph{exploratory}, but it is not ``factor analysis.''
\item
  Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution.
\end{itemize}

\hypertarget{research-vignette-5}{%
\section{Research Vignette}\label{research-vignette-5}}

This lesson's research vignette emerges from Lewis and Neville's Gendered Racial Microaggressions Scale for Black Women \citeyearpar{lewis_construction_2015}. The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. We simulate data from the final construction of the stress appraisal version as the basis of the lecture.

Lewis and Neville \citeyearpar{lewis_construction_2015} reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them. The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification

  \begin{itemize}
  \tightlist
  \item
    10 items
  \item
    ``Objectified me based on physical features.''
  \item
    Abbreviated in the simulated data as ``Obj\#''
  \end{itemize}
\item
  Silenced and Marginalized

  \begin{itemize}
  \tightlist
  \item
    7 items
  \item
    ``Someone has tried to `put me in my place.'\,''
  \item
    Abbreviated in the simulated data as ``Marg\#''
  \end{itemize}
\item
  Strong Black Woman Stereotype

  \begin{itemize}
  \tightlist
  \item
    5 items
  \item
    ``I have been told that I am too assertive.''
  \item
    Abbreviated in the simulated data as ``Strong\#''
  \end{itemize}
\item
  Angry Black Woman Stereotype

  \begin{itemize}
  \tightlist
  \item
    3 items
  \item
    ``Someone accused me of being angry when speaking calm.''
  \item
    Abbreviated in the simulated data as ``Angry\#''
  \end{itemize}
\end{itemize}

Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210921}\NormalTok{)}
\NormalTok{GRMSmat <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{69}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.60}\NormalTok{, }\FloatTok{.59}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.41}\NormalTok{, }\FloatTok{.41}\NormalTok{, }\FloatTok{.04}\NormalTok{, }\FloatTok{-.15}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.12}\NormalTok{, }\FloatTok{.20}\NormalTok{, }\FloatTok{-.01}\NormalTok{, }\FloatTok{-.22}\NormalTok{, }\FloatTok{-.02}\NormalTok{, }\FloatTok{.02}\NormalTok{, }\FloatTok{.12}\NormalTok{, }\FloatTok{-.09}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.19}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{-.13}\NormalTok{,}
                  \FloatTok{.07}\NormalTok{, }\FloatTok{-.07}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.07}\NormalTok{, }\FloatTok{-.18}\NormalTok{, }\FloatTok{.22}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{-.01}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.02}\NormalTok{, }\FloatTok{.93}\NormalTok{, }\FloatTok{.81}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.67}\NormalTok{, }\FloatTok{.61}\NormalTok{, }\FloatTok{.58}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{-.07}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.19}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.04}\NormalTok{, }\FloatTok{.08}\NormalTok{,}
                  \FloatTok{-.08}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\DecValTok{00}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.08}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.22}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.01}\NormalTok{, }\FloatTok{-.05}\NormalTok{, }\FloatTok{-.11}\NormalTok{, }\FloatTok{-.16}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.59}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.51}\NormalTok{, }\FloatTok{-.12}\NormalTok{, }\FloatTok{.08}\NormalTok{, }\FloatTok{.03}\NormalTok{,}
                  \FloatTok{-.06}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.01}\NormalTok{, }\FloatTok{.05}\NormalTok{, }\FloatTok{.09}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.07}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\FloatTok{.18}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{-.21}\NormalTok{, }\FloatTok{.21}\NormalTok{, }\FloatTok{.21}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.26}\NormalTok{, }\FloatTok{-.14}\NormalTok{, }\FloatTok{.70}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.68}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{4}\NormalTok{) }\CommentTok{#primary factor loadings for the four factors taken from the stress appraisal (left hand) factor loadings in Table 1 of the manuscript}
\KeywordTok{rownames}\NormalTok{(GRMSmat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#variable names for the 25 items}
\KeywordTok{colnames}\NormalTok{(GRMSmat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{) }\CommentTok{#component (subscale) names}
\NormalTok{GRMSCorMat <-}\StringTok{ }\NormalTok{GRMSmat }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(GRMSmat) }\CommentTok{#create the correlation matrix via some matrix algebra}
\KeywordTok{diag}\NormalTok{(GRMSCorMat) <-}\StringTok{ }\DecValTok{1}
\CommentTok{#SzyCorMat #prints the correlation matrix}
\NormalTok{GRMS_M <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.78}\NormalTok{,   }\FloatTok{1.85}\NormalTok{,   }\FloatTok{1.97}\NormalTok{,   }\FloatTok{1.93}\NormalTok{,   }\FloatTok{2.01}\NormalTok{,   }\FloatTok{1.76}\NormalTok{,   }\FloatTok{1.91}\NormalTok{,   }\FloatTok{2.22}\NormalTok{,   }\FloatTok{1.83}\NormalTok{,   }\FloatTok{1.88}\NormalTok{, }\DecValTok{2}\NormalTok{,    }\FloatTok{3.5}\NormalTok{,    }\FloatTok{2.43}\NormalTok{,   }\FloatTok{3.44}\NormalTok{,   }\FloatTok{2.39}\NormalTok{,   }\FloatTok{2.89}\NormalTok{,   }\FloatTok{2.7}\NormalTok{, }\FloatTok{1.28}\NormalTok{,  }\FloatTok{2.25}\NormalTok{,   }\FloatTok{1.45}\NormalTok{,   }\FloatTok{1.57}\NormalTok{,   }\FloatTok{1.4}\NormalTok{, }\FloatTok{2.02}\NormalTok{,  }\FloatTok{2.53}\NormalTok{,   }\FloatTok{2.39}\NormalTok{) }\CommentTok{#item means; I made these up based on the M and SDs for the factors}
\NormalTok{GRMS_SD <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.11}\NormalTok{,  }\FloatTok{1.23}\NormalTok{,   }\FloatTok{0.97}\NormalTok{,   }\FloatTok{0.85}\NormalTok{,   }\FloatTok{1.19}\NormalTok{,   }\FloatTok{1.32}\NormalTok{,   }\FloatTok{1.04}\NormalTok{,   }\FloatTok{0.98}\NormalTok{,   }\FloatTok{1.01}\NormalTok{,   }\FloatTok{1.03}\NormalTok{, }\FloatTok{1.01}\NormalTok{, }\FloatTok{0.97}\NormalTok{,   }\FloatTok{1.32}\NormalTok{,   }\FloatTok{1.24}\NormalTok{,   }\FloatTok{1.31}\NormalTok{,   }\FloatTok{1.42}\NormalTok{,   }\FloatTok{1.2}\NormalTok{, }\FloatTok{0.85}\NormalTok{,  }\FloatTok{0.94}\NormalTok{,   }\FloatTok{0.78}\NormalTok{,   }\FloatTok{1.11}\NormalTok{,   }\FloatTok{0.84}\NormalTok{, }\FloatTok{1.14}\NormalTok{, }\FloatTok{1.2}\NormalTok{,    }\FloatTok{1.21}\NormalTok{) }\CommentTok{#item standard deviations; I made these up based on the M and SDs for the factors}
\NormalTok{GRMSCovMat <-}\StringTok{ }\NormalTok{GRMS_SD }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(GRMS_SD) }\OperatorTok{*}\StringTok{ }\NormalTok{GRMSCorMat }\CommentTok{#creates a covariance matrix (with more matrix algebra) from the correlation matrix}
\NormalTok{dfGRMS <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{259}\NormalTok{, }\DataTypeTok{mu =}\NormalTok{ GRMS_M, }\DataTypeTok{Sigma =}\NormalTok{ GRMSCovMat, }\DataTypeTok{empirical =} \OtherTok{TRUE}\NormalTok{),}\DecValTok{0}\NormalTok{)) }\CommentTok{#creates the item level data from the sample size, mean, and covariance matrix; wrapped in commands to round to 0 decimal places and format as a df}
\NormalTok{dfGRMS[dfGRMS}\OperatorTok{>}\DecValTok{5}\NormalTok{]<-}\DecValTok{5} \CommentTok{#restricts the upperbound of all variables to be 5 or less}
\NormalTok{dfGRMS[dfGRMS}\OperatorTok{<}\DecValTok{0}\NormalTok{]<-}\DecValTok{0} \CommentTok{#resticts the lowerbound of all variable to be 0 or greater}
\CommentTok{#colMeans(GRMS) #displays column means}

\CommentTok{#Below is code if you would like an ID number for each case. Expecially at first, the ID number would just need to be removed, so I will not include it in the original simulation. We will add it later.}
\CommentTok{#library(tidyverse)}
\CommentTok{#dfGRMS <- dfGRMS %>% dplyr::mutate(ID = row_number()) #add ID to each row}
\CommentTok{#dfGRMS <- dfGRMS%>%dplyr::select(ID, everything())#moving the ID number to the first column; requires}
\end{Highlighting}
\end{Shaded}

Let's take a quick peek at the data to see if everthing looks right.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        vars   n mean   sd median trimmed  mad min max range  skew kurtosis
Obj1       1 259 1.78 1.09      2    1.78 1.48   0   5     5  0.16    -0.63
Obj2       2 259 1.90 1.14      2    1.90 1.48   0   5     5  0.10    -0.61
Obj3       3 259 1.97 1.01      2    1.96 1.48   0   4     4  0.10    -0.56
Obj4       4 259 1.92 0.89      2    1.90 1.48   0   4     4  0.18    -0.17
Obj5       5 259 2.05 1.19      2    2.05 1.48   0   5     5  0.00    -0.63
Obj6       6 259 1.81 1.29      2    1.75 1.48   0   5     5  0.32    -0.70
Obj7       7 259 1.95 1.09      2    1.95 1.48   0   5     5  0.16    -0.22
Obj8       8 259 2.21 1.00      2    2.21 1.48   0   5     5 -0.13    -0.39
Obj9       9 259 1.81 0.99      2    1.84 1.48   0   4     4  0.02    -0.40
Obj10     10 259 1.88 1.05      2    1.87 1.48   0   4     4  0.18    -0.43
Marg1     11 259 2.02 1.02      2    2.00 1.48   0   5     5  0.13    -0.28
Marg2     12 259 3.47 0.99      4    3.50 1.48   1   5     4 -0.31    -0.33
Marg3     13 259 2.44 1.30      2    2.45 1.48   0   5     5 -0.01    -0.71
Marg4     14 259 3.35 1.17      3    3.38 1.48   1   5     4 -0.14    -0.92
Marg5     15 259 2.40 1.31      2    2.39 1.48   0   5     5  0.11    -0.58
Marg6     16 259 2.85 1.37      3    2.89 1.48   0   5     5 -0.22    -0.67
Marg7     17 259 2.68 1.21      3    2.66 1.48   0   5     5  0.02    -0.32
Strong1   18 259 1.27 0.88      1    1.23 1.48   0   4     4  0.26    -0.49
Strong2   19 259 2.29 0.95      2    2.30 1.48   0   5     5 -0.17    -0.31
Strong3   20 259 1.45 0.83      1    1.44 1.48   0   4     4  0.09    -0.37
Strong4   21 259 1.60 1.10      2    1.57 1.48   0   5     5  0.27    -0.49
Strong5   22 259 1.41 0.83      1    1.41 1.48   0   4     4 -0.01    -0.40
Angry1    23 259 2.03 1.15      2    2.01 1.48   0   5     5  0.13    -0.48
Angry2    24 259 2.53 1.20      3    2.54 1.48   0   5     5 -0.04    -0.49
Angry3    25 259 2.39 1.16      2    2.41 1.48   0   5     5 -0.07    -0.45
          se
Obj1    0.07
Obj2    0.07
Obj3    0.06
Obj4    0.06
Obj5    0.07
Obj6    0.08
Obj7    0.07
Obj8    0.06
Obj9    0.06
Obj10   0.07
Marg1   0.06
Marg2   0.06
Marg3   0.08
Marg4   0.07
Marg5   0.08
Marg6   0.09
Marg7   0.08
Strong1 0.05
Strong2 0.06
Strong3 0.05
Strong4 0.07
Strong5 0.05
Angry1  0.07
Angry2  0.07
Angry3  0.07
\end{verbatim}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(dfGRMS, file="dfGRMS.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#dfGRMS <- read.csv ("dfGRMS.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(dfGRMS, "dfGRMS.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#dfGRMS <- readRDS("dfGRMS.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-vignette}{%
\section{Working the Vignette}\label{working-the-vignette}}

Below we will create a correlation matrix of our items. Whether we are conducting PCA or PAF, the \emph{dimension-reduction} we are seeking is looking for clusters of correlated items in the \(R\)-matrix. Essentially, these are (CITE FIELD):

\begin{itemize}
\tightlist
\item
  statistical entities that can be plotted as classification axes where coordinates of variables along each axis represent the strength of the relationship between that variable to each factor.
\item
  mathematical equations, resembling regression equations, where each variable is represented according to its relative weight
\end{itemize}

PCA in particular establishes which linear components exist within the data and how a particular variable might contribute to that component.

Here is the correlation matrix of our items. It would be quite a daunting exercise to visually inspect this and manually cluster the correlations of items.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSmatrix<-}\KeywordTok{cor}\NormalTok{(dfGRMS) }\CommentTok{#correlation matrix created and saved as object}
\KeywordTok{round}\NormalTok{(GRMSmatrix, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Obj1  Obj2  Obj3  Obj4  Obj5  Obj6  Obj7  Obj8  Obj9 Obj10 Marg1 Marg2
Obj1     1.00  0.41  0.38  0.33  0.34  0.34  0.37  0.35  0.25  0.24  0.11  0.00
Obj2     0.41  1.00  0.43  0.40  0.40  0.34  0.35  0.29  0.28  0.24 -0.02 -0.13
Obj3     0.38  0.43  1.00  0.36  0.34  0.31  0.30  0.28  0.22  0.24  0.01 -0.03
Obj4     0.33  0.40  0.36  1.00  0.32  0.32  0.32  0.25  0.27  0.28  0.06 -0.03
Obj5     0.34  0.40  0.34  0.32  1.00  0.23  0.26  0.26  0.27  0.25 -0.14 -0.18
Obj6     0.34  0.34  0.31  0.32  0.23  1.00  0.31  0.24  0.19  0.16  0.20  0.16
Obj7     0.37  0.35  0.30  0.32  0.26  0.31  1.00  0.26  0.19  0.26  0.23  0.11
Obj8     0.35  0.29  0.28  0.25  0.26  0.24  0.26  1.00  0.21  0.22  0.00 -0.09
Obj9     0.25  0.28  0.22  0.27  0.27  0.19  0.19  0.21  1.00  0.22  0.07  0.01
Obj10    0.24  0.24  0.24  0.28  0.25  0.16  0.26  0.22  0.22  1.00  0.06 -0.03
Marg1    0.11 -0.02  0.01  0.06 -0.14  0.20  0.23  0.00  0.07  0.06  1.00  0.66
Marg2    0.00 -0.13 -0.03 -0.03 -0.18  0.16  0.11 -0.09  0.01 -0.03  0.66  1.00
Marg3    0.05 -0.01  0.02  0.04 -0.11  0.16  0.12 -0.03  0.07  0.02  0.62  0.50
Marg4    0.17  0.05  0.10  0.07 -0.03  0.21  0.21  0.05  0.05  0.07  0.58  0.47
Marg5    0.20  0.07  0.11  0.13  0.00  0.21  0.22  0.04  0.10  0.12  0.55  0.41
Marg6    0.00 -0.09 -0.06  0.03 -0.11  0.06  0.16  0.03  0.05  0.13  0.51  0.43
Marg7   -0.14 -0.22 -0.09 -0.13 -0.17  0.00 -0.01 -0.10 -0.04 -0.04  0.42  0.42
Strong1 -0.05 -0.07  0.03 -0.02  0.11 -0.05  0.01  0.03  0.14  0.10 -0.04  0.05
Strong2  0.01 -0.06  0.01  0.04  0.13 -0.01  0.04  0.13  0.12  0.13 -0.10 -0.06
Strong3  0.04  0.05  0.07  0.14  0.15  0.02  0.10  0.10  0.13  0.14 -0.06 -0.09
Strong4 -0.11 -0.11  0.01 -0.04  0.06 -0.04 -0.05  0.05  0.11  0.09 -0.04  0.05
Strong5  0.02  0.00 -0.01  0.01  0.04  0.00  0.10  0.11  0.05  0.11  0.14  0.10
Angry1   0.11  0.15  0.20  0.15  0.11  0.15  0.01  0.08  0.08  0.03 -0.04  0.06
Angry2  -0.12 -0.04  0.08 -0.02  0.01  0.01 -0.07 -0.01  0.07 -0.05 -0.03  0.11
Angry3  -0.12 -0.09  0.03 -0.06 -0.05  0.02 -0.12 -0.11  0.05 -0.07  0.02  0.21
        Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Strong3 Strong4 Strong5
Obj1     0.05  0.17  0.20  0.00 -0.14   -0.05    0.01    0.04   -0.11    0.02
Obj2    -0.01  0.05  0.07 -0.09 -0.22   -0.07   -0.06    0.05   -0.11    0.00
Obj3     0.02  0.10  0.11 -0.06 -0.09    0.03    0.01    0.07    0.01   -0.01
Obj4     0.04  0.07  0.13  0.03 -0.13   -0.02    0.04    0.14   -0.04    0.01
Obj5    -0.11 -0.03  0.00 -0.11 -0.17    0.11    0.13    0.15    0.06    0.04
Obj6     0.16  0.21  0.21  0.06  0.00   -0.05   -0.01    0.02   -0.04    0.00
Obj7     0.12  0.21  0.22  0.16 -0.01    0.01    0.04    0.10   -0.05    0.10
Obj8    -0.03  0.05  0.04  0.03 -0.10    0.03    0.13    0.10    0.05    0.11
Obj9     0.07  0.05  0.10  0.05 -0.04    0.14    0.12    0.13    0.11    0.05
Obj10    0.02  0.07  0.12  0.13 -0.04    0.10    0.13    0.14    0.09    0.11
Marg1    0.62  0.58  0.55  0.51  0.42   -0.04   -0.10   -0.06   -0.04    0.14
Marg2    0.50  0.47  0.41  0.43  0.42    0.05   -0.06   -0.09    0.05    0.10
Marg3    1.00  0.42  0.40  0.35  0.34   -0.06   -0.09   -0.09   -0.02    0.07
Marg4    0.42  1.00  0.44  0.28  0.27   -0.03   -0.08   -0.08   -0.04    0.06
Marg5    0.40  0.44  1.00  0.25  0.26   -0.10   -0.08   -0.10   -0.06    0.01
Marg6    0.35  0.28  0.25  1.00  0.29    0.10    0.09    0.10    0.11    0.23
Marg7    0.34  0.27  0.26  0.29  1.00    0.09    0.08   -0.01    0.19    0.13
Strong1 -0.06 -0.03 -0.10  0.10  0.09    1.00    0.26    0.24    0.34    0.19
Strong2 -0.09 -0.08 -0.08  0.09  0.08    0.26    1.00    0.27    0.31    0.21
Strong3 -0.09 -0.08 -0.10  0.10 -0.01    0.24    0.27    1.00    0.25    0.27
Strong4 -0.02 -0.04 -0.06  0.11  0.19    0.34    0.31    0.25    1.00    0.22
Strong5  0.07  0.06  0.01  0.23  0.13    0.19    0.21    0.27    0.22    1.00
Angry1   0.05  0.08  0.07 -0.16  0.08    0.09   -0.05   -0.10    0.06   -0.11
Angry2   0.02  0.06 -0.01 -0.08  0.19    0.22    0.05   -0.02    0.24   -0.02
Angry3   0.05  0.07  0.04 -0.09  0.20    0.18    0.03   -0.07    0.20   -0.05
        Angry1 Angry2 Angry3
Obj1      0.11  -0.12  -0.12
Obj2      0.15  -0.04  -0.09
Obj3      0.20   0.08   0.03
Obj4      0.15  -0.02  -0.06
Obj5      0.11   0.01  -0.05
Obj6      0.15   0.01   0.02
Obj7      0.01  -0.07  -0.12
Obj8      0.08  -0.01  -0.11
Obj9      0.08   0.07   0.05
Obj10     0.03  -0.05  -0.07
Marg1    -0.04  -0.03   0.02
Marg2     0.06   0.11   0.21
Marg3     0.05   0.02   0.05
Marg4     0.08   0.06   0.07
Marg5     0.07  -0.01   0.04
Marg6    -0.16  -0.08  -0.09
Marg7     0.08   0.19   0.20
Strong1   0.09   0.22   0.18
Strong2  -0.05   0.05   0.03
Strong3  -0.10  -0.02  -0.07
Strong4   0.06   0.24   0.20
Strong5  -0.11  -0.02  -0.05
Angry1    1.00   0.44   0.43
Angry2    0.44   1.00   0.47
Angry3    0.43   0.47   1.00
\end{verbatim}

This correlation matrix is so big that you might wish to write code so that you can examine it in sections

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#round(GRMSmatrix[,1:8], 2)}
\CommentTok{#round(GRMSmatrix[,9:16], 2)}
\CommentTok{#round(GRMSmatrix[,17:25], 2)}
\end{Highlighting}
\end{Shaded}

With component and factor analytic procedures we can analyze the data with either raw data or correlation matrix. Producing the matrix helps us see how this is a ``structural'' analysis. That is, we are trying to see if our more parsimonious extraction ``reproduces'' this original correlation matrix.

\hypertarget{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis}{%
\subsection{Three diagnostic tests to evaluate the appropriateness of the data for component-or-factor analysis}\label{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis}}

\hypertarget{is-my-sample-adequate-for-pca}{%
\subsubsection{Is my sample adequate for PCA?}\label{is-my-sample-adequate-for-pca}}

There have been a number of generic guidelines (some supported by analyses; some not) about ``how big'' the sample size should be:

\begin{itemize}
\tightlist
\item
  10-15 participants per variable
\item
  10X as many participants as variables (Nunnally, 1978)
\item
  5 and 10 participants per variable up to 300 (Kass \& Tinsley, 1979)
\item
  300 (Tabachnick \& Fidell, 2007)
\item
  1000 = excellent, 300 = good, 100 = poor (Comrey \& Lee, 1992)
\end{itemize}

Of course it is more complicated. Monte Carlo studies have shown that

\begin{itemize}
\tightlist
\item
  if factor loadings are large (\textasciitilde.6), the solution is reliable regardless of size
\item
  if communalities are large (\textasciitilde.6), relatively small samples (\textasciitilde100) are sufficient, but when they are lower (well below .5), then larger samples (\textgreater500 are indicated).
\end{itemize}

The \textbf{Kaiser-Meyer-Olkin} index (KMO) is an index of \emph{sampling adequacy} that can be used with the actual sample to let us know if the sample size is sufficient relative to the statistical characteristics of the data. If it is below the thresshold we should probably collect more data to see if it can achieve a satisfactory value.

Kaiser's 1974 recommendations were:

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 as mediocre
\item
  values between .7 and .8 as good
\item
  values above .9 are superb
\end{itemize}

Revelle has included a KMO test in the psych package. The function can use either raw or matrix data. Either way, the only variables in the matrix should be the items of interest. This means that everything else (e.g., total or subscale scores, ID numbers) should be removed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{KMO}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = dfGRMS)
Overall MSA =  0.86
MSA for each item = 
   Obj1    Obj2    Obj3    Obj4    Obj5    Obj6    Obj7    Obj8    Obj9   Obj10 
   0.89    0.88    0.90    0.90    0.91    0.92    0.92    0.90    0.88    0.90 
  Marg1   Marg2   Marg3   Marg4   Marg5   Marg6   Marg7 Strong1 Strong2 Strong3 
   0.83    0.88    0.91    0.91    0.91    0.87    0.90    0.79    0.79    0.80 
Strong4 Strong5  Angry1  Angry2  Angry3 
   0.79    0.81    0.72    0.75    0.75 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#psych::KMO(GRMSmatrix)}
\end{Highlighting}
\end{Shaded}

We examine the KMO values for both the overall matrix and the individual items.

At the matrix level, our \(KMO = .86\), which falls into Kaiser's definition of \emph{good}.

At the item level, the KMO should be \textgreater{} .50. Variables with values below .5 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference). Because removing and adding variables impacts the KMO, be sure to re-evaluate the sampling adequacy if changes are made to the items (and/or sample size).

At the item level, our KMO values range between .72 (Angry1) and .92 (Obj6, Obj7).

Considering both item- and matrix- levels, we conclude that the sample size and the data are adequate for component-or-factor analysis.

\hypertarget{are-there-correlations-among-the-variables-that-are-large-enough-to-be-analyzed}{%
\subsubsection{Are there correlations among the variables that are large enough to be analyzed?}\label{are-there-correlations-among-the-variables-that-are-large-enough-to-be-analyzed}}

\textbf{Bartlett's test} let's us know if a matrix is an \emph{identity matrix.} In an identity matrix, then all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0.

A signifcant Barlett's (i.e., \(p < .05\)) tells that the \(R\)-matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

The \emph{cortest.bartlett()} function is in the \emph{psych} package and can be run either from the raw data or R matrix formats.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{cortest.bartlett}\NormalTok{(dfGRMS) }\CommentTok{#from the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1683.76

$p.value
[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005520916

$df
[1] 300
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#raw data produces the warning "R was not square, finding R from data." This means nothing other than we fed it raw data and the function is creating a matrix from which to do the analysis.}

\CommentTok{#psych::cortest.bartlett(GRMSmatrix, n = 259) #if using the matrix, must specify sample}
\end{Highlighting}
\end{Shaded}

Our Bartlett's test is significant: \(\chi ^{1}(300)=1683.76, p < .001\). This supports a component-or-factor analytic approach for investigating the data.

\hypertarget{is-there-multicollinearity-or-singularity-in-my-data}{%
\subsubsection{Is there multicollinearity or singularity in my data?}\label{is-there-multicollinearity-or-singularity-in-my-data}}

The \textbf{determinant of the correlation matrix} should be greater than 0.00001 (that would be 4 zeros, then the 1). If it is smaller than 0.00001 then we may have an issue with \emph{multicollinearity} (i.e., variables that are too highly correlated) or \emph{singularity} (variables that are perfectly correlated).

The determinant function we use comes from base R. It is easiest to compute when the correlation matrix is the object. However, it is also possible to specify the command to work with the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{det}\NormalTok{(GRMSmatrix) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.001151581
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#det(cor(dfGRMS))#if using the raw data}
\end{Highlighting}
\end{Shaded}

With a value of 0.00115, our determinant is greater than the 0.00001 requirement. If it were not, then we could identify problematic variables (i.e., those correlating too highly with others; those not correlating sufficiently with others) and re-run the diagnostic statitics.

\textbf{Summary:} Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi ^{1}(300)=1683.76, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

\hypertarget{principal-component-analysis}{%
\subsection{Principal Component Analysis}\label{principal-component-analysis}}

We can use the \emph{principal()} function from the \emph{psych} package with raw or matrix data.

We start by creating a principal components model that has the same number of components as there are variables in the data. This allows us to inspect the component's eigenvalues and make decisions about which to extract.

\begin{itemize}
\tightlist
\item
  Note, this is different than actual \emph{factor} analysis -- where you \emph{must} extract fewer factors than variables (e.g., extracting 18 {[}an arbitray number{]} instead of 23).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#The numerous codes  all result in the same. They simply swap out using the df or r-matrix, and whether I specify the number of factors or write code to instruct R to calculate it.}

\CommentTok{#pca1 <- psych::principal(GRMSmatrix, nfactors=25, rotate = "none") #using the matrix form of the data and specifying the # factors}

\CommentTok{#pca1 <- psych::principal(GRMSmatrix, nfactors=length(GRMSmatrix[,1]), rotate = "none") #using the matrix form of the data and letting the length function automatically calculate the # factors as a function of how many columns in the matrix}

\CommentTok{#pca1 <- psych::principal(dfGRMS, nfactors=25, rotate="none") #using raw data and specifying # factors}

\NormalTok{pca1 <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{principal}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors=}\KeywordTok{length}\NormalTok{(dfGRMS), }\DataTypeTok{rotate=}\StringTok{"none"}\NormalTok{)}\CommentTok{# using raw data and letting the length function automatically calculate the # factors as a function of how many columns in the raw data}
\NormalTok{pca1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = length(dfGRMS), rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
         PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11  PC12
Obj1    0.55  0.40 -0.17  0.02  0.12 -0.17 -0.13 -0.12 -0.16  0.02  0.14 -0.01
Obj2    0.46  0.54 -0.15  0.14  0.02  0.18 -0.03 -0.09  0.03  0.10 -0.15  0.03
Obj3    0.48  0.43  0.04  0.21  0.13  0.08  0.11  0.01 -0.16  0.05 -0.20 -0.29
Obj4    0.50  0.42 -0.03  0.04 -0.15  0.25  0.12  0.27  0.24 -0.06  0.16  0.00
Obj5    0.33  0.57  0.13  0.02 -0.08 -0.01 -0.05 -0.01 -0.32  0.23 -0.19  0.04
Obj6    0.56  0.19 -0.09  0.15  0.17  0.12 -0.19  0.33  0.06 -0.27 -0.31  0.21
Obj7    0.59  0.22 -0.09 -0.13  0.11  0.09  0.15 -0.03 -0.21 -0.13  0.13  0.27
Obj8    0.38  0.40  0.06 -0.09  0.37 -0.44 -0.14 -0.15  0.32 -0.20  0.09 -0.25
Obj9    0.40  0.28  0.21 -0.01 -0.55 -0.05 -0.48 -0.17  0.23  0.12  0.03  0.06
Obj10   0.40  0.28  0.12 -0.19 -0.33 -0.24  0.62 -0.06  0.17  0.02 -0.09  0.04
Marg1   0.63 -0.62 -0.13 -0.10 -0.02  0.04 -0.03 -0.03  0.00 -0.01  0.02 -0.03
Marg2   0.47 -0.66  0.05  0.05  0.00  0.07 -0.06 -0.02 -0.03 -0.14 -0.03  0.05
Marg3   0.50 -0.53 -0.11  0.02 -0.08  0.06 -0.10  0.01  0.12  0.08 -0.08 -0.11
Marg4   0.57 -0.42 -0.10  0.08  0.10 -0.05  0.00 -0.09 -0.23  0.02  0.17 -0.11
Marg5   0.58 -0.35 -0.17  0.08 -0.11 -0.17  0.05  0.08 -0.14  0.27  0.20 -0.06
Marg6   0.40 -0.44  0.07 -0.42 -0.08  0.03  0.09 -0.06  0.15 -0.30 -0.10  0.03
Marg7   0.23 -0.59  0.28  0.01  0.08 -0.10  0.04  0.19 -0.01  0.21 -0.16 -0.06
Strong1 0.04  0.02  0.65 -0.13 -0.13  0.07 -0.02 -0.35 -0.33 -0.34  0.01  0.03
Strong2 0.04  0.13  0.53 -0.35  0.04 -0.35 -0.11  0.43 -0.11  0.02  0.19  0.26
Strong3 0.09  0.21  0.41 -0.45  0.04  0.44 -0.01  0.18  0.01  0.02  0.34 -0.32
Strong4 0.02 -0.04  0.70 -0.15  0.00 -0.09 -0.03  0.12 -0.04  0.05 -0.33 -0.26
Strong5 0.18 -0.06  0.34 -0.47  0.39  0.19  0.02 -0.30  0.23  0.38 -0.05  0.30
Angry1  0.19  0.07  0.30  0.70  0.11  0.01  0.12 -0.02  0.14  0.01  0.16  0.06
Angry2  0.03 -0.11  0.55  0.55  0.08  0.02  0.06 -0.08  0.09 -0.02  0.08 -0.03
Angry3  0.02 -0.21  0.49  0.59 -0.04  0.05  0.02  0.02  0.03  0.01  0.07  0.16
         PC13  PC14  PC15  PC16  PC17  PC18  PC19  PC20  PC21  PC22  PC23  PC24
Obj1     0.19  0.01 -0.13  0.11 -0.35  0.27  0.01 -0.21 -0.01 -0.13  0.24  0.08
Obj2    -0.12  0.00 -0.02 -0.04  0.14  0.08  0.13 -0.03  0.25  0.46  0.21 -0.03
Obj3    -0.06  0.07 -0.52  0.02  0.11 -0.09 -0.10  0.11  0.00 -0.14 -0.13 -0.02
Obj4    -0.10  0.29  0.04 -0.26 -0.20 -0.26 -0.07 -0.10 -0.16 -0.02  0.12  0.04
Obj5    -0.07  0.28  0.40  0.06  0.09  0.14 -0.18  0.11 -0.11 -0.09 -0.06 -0.03
Obj6     0.33 -0.17  0.12  0.11  0.06 -0.13  0.00  0.00  0.09 -0.10 -0.03  0.06
Obj7    -0.43 -0.36  0.03 -0.03 -0.07  0.04  0.13  0.08 -0.13 -0.06 -0.12  0.00
Obj8    -0.08  0.01  0.15  0.03  0.05 -0.09 -0.05  0.19 -0.11  0.09  0.00 -0.02
Obj9    -0.07 -0.18 -0.10  0.06  0.04 -0.06 -0.07 -0.09 -0.02 -0.05 -0.13 -0.03
Obj10    0.23 -0.11  0.02  0.12  0.12  0.03  0.05 -0.04 -0.06 -0.02  0.05 -0.05
Marg1    0.00  0.03  0.00 -0.01  0.02  0.01  0.00  0.03 -0.02  0.02  0.03 -0.06
Marg2    0.05  0.04 -0.05 -0.05 -0.01  0.09 -0.11  0.07 -0.11  0.02  0.13 -0.46
Marg3   -0.03  0.20  0.01  0.06  0.10  0.13  0.49  0.13 -0.14 -0.13  0.02  0.15
Marg4    0.14 -0.05  0.08 -0.20  0.30 -0.07 -0.08 -0.37 -0.12  0.11 -0.13  0.12
Marg5    0.12 -0.10  0.08 -0.12 -0.15 -0.19 -0.05  0.32  0.33 -0.01  0.00  0.05
Marg6   -0.15  0.18 -0.03 -0.02 -0.04  0.27 -0.27 -0.01  0.25  0.00 -0.11  0.20
Marg7   -0.25 -0.02  0.02  0.44 -0.16 -0.20 -0.09 -0.17 -0.06  0.14  0.06  0.05
Strong1  0.10  0.16  0.00  0.09 -0.09 -0.30  0.15  0.05  0.04  0.07  0.04  0.04
Strong2 -0.04  0.18 -0.20 -0.05  0.19  0.06  0.10 -0.03  0.08  0.04  0.02 -0.04
Strong3  0.13 -0.14  0.09  0.22  0.07  0.14 -0.02  0.06  0.02  0.02 -0.01 -0.03
Strong4  0.00 -0.21  0.08 -0.38 -0.23  0.11  0.12 -0.08 -0.01  0.04 -0.04 -0.02
Strong5  0.13  0.04 -0.06 -0.09 -0.01 -0.10 -0.02  0.01 -0.01 -0.08 -0.03  0.01
Angry1   0.05  0.16  0.06  0.09 -0.17  0.11  0.14 -0.11  0.15  0.06 -0.38 -0.14
Angry2  -0.21 -0.08  0.11 -0.04  0.22  0.00 -0.03 -0.09  0.21 -0.33  0.28  0.02
Angry3   0.13 -0.09 -0.12 -0.03  0.00  0.17 -0.18  0.26 -0.27  0.20  0.07  0.23
         PC25 h2                   u2 com
Obj1     0.00  1 -0.00000000000000289 6.7
Obj2     0.01  1  0.00000000000000155 5.4
Obj3    -0.01  1  0.00000000000000089 5.8
Obj4     0.01  1  0.00000000000000000 8.1
Obj5    -0.02  1  0.00000000000000078 6.1
Obj6     0.00  1  0.00000000000000011 6.8
Obj7     0.02  1  0.00000000000000100 5.4
Obj8     0.01  1  0.00000000000000144 8.2
Obj9     0.01  1  0.00000000000000100 5.5
Obj10    0.00  1 -0.00000000000000133 4.9
Marg1   -0.43  1 -0.00000000000000133 3.0
Marg2    0.16  1 -0.00000000000000067 3.5
Marg3    0.10  1 -0.00000000000000067 4.9
Marg4    0.08  1 -0.00000000000000133 5.9
Marg5    0.08  1  0.00000000000000011 6.3
Marg6    0.07  1 -0.00000000000000089 8.3
Marg7    0.04  1  0.00000000000000133 5.5
Strong1  0.00  1  0.00000000000000200 4.3
Strong2 -0.02  1  0.00000000000000155 6.6
Strong3  0.01  1 -0.00000000000000022 7.2
Strong4 -0.02  1  0.00000000000000011 3.6
Strong5  0.02  1  0.00000000000000022 7.6
Angry1  -0.03  1  0.00000000000000044 3.6
Angry2  -0.02  1  0.00000000000000078 4.9
Angry3  -0.01  1  0.00000000000000011 5.2

                       PC1  PC2  PC3  PC4  PC5  PC6  PC7  PC8  PC9 PC10 PC11
SS loadings           4.04 3.61 2.41 2.09 0.88 0.84 0.80 0.78 0.75 0.71 0.68
Proportion Var        0.16 0.14 0.10 0.08 0.04 0.03 0.03 0.03 0.03 0.03 0.03
Cumulative Var        0.16 0.31 0.40 0.49 0.52 0.55 0.59 0.62 0.65 0.68 0.70
Proportion Explained  0.16 0.14 0.10 0.08 0.04 0.03 0.03 0.03 0.03 0.03 0.03
Cumulative Proportion 0.16 0.31 0.40 0.49 0.52 0.55 0.59 0.62 0.65 0.68 0.70
                      PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22
SS loadings           0.67 0.65 0.63 0.61 0.59 0.56 0.56 0.54 0.52 0.51 0.50
Proportion Var        0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02
Cumulative Var        0.73 0.76 0.78 0.81 0.83 0.85 0.87 0.90 0.92 0.94 0.96
Proportion Explained  0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02
Cumulative Proportion 0.73 0.76 0.78 0.81 0.83 0.85 0.87 0.90 0.92 0.94 0.96
                      PC23 PC24 PC25
SS loadings           0.45 0.39 0.25
Proportion Var        0.02 0.02 0.01
Cumulative Var        0.97 0.99 1.00
Proportion Explained  0.02 0.02 0.01
Cumulative Proportion 0.97 0.99 1.00

Mean item complexity =  5.7
Test of the hypothesis that 25 components are sufficient.

The root mean square of the residuals (RMSR) is  0 
 with the empirical chi square  0  with prob <  NA 

Fit based upon off diagonal values = 1
\end{verbatim}

\emph{The total variance for a particular variable will have two components: some of it will be share with other variables (common variance, h2) and some of it will be specific to that measure (unique variance, u2). Random variance is also specific to one item, but not reliably so.}

We can examine this most easily by examining the matrix (second screen).

The columns PC1 thru PC25 are the (uninteresting at this point) unrotated loadings. PC stands for ``principal component.'' Although these don't align with the specific items, at this point in the procedure there are as many components as variables.

\textbf{Communalities} are represented as \(h^2\). These are the proportions of common variance present in the variables. A variable that has no specific (or random) variance would have a communality of 1.0. If a variable shares none of its variance with any other variable its communality would be 0.0.

Because we extracted the same number components as variables they all equal 1.0. That is we have explained all the variance in each variable. When we specify fewer components the value of the communalities will decrease.

**Uniquenessess* are represented as \(u2\). These are the amount of unique variance for each variable. They are calculated as \(1 - h^2\) (or 1 minus the communality). Technically (at this point in the analysis where we have an equal number of components as items), they should all be zero, but the \emph{psych} package is very quantsy and decimals are reported to the 15th and 16th decimal places! (hence the u2 for Q1 is -0.0000000000000028865799).

The final column, \emph{com} represents \emph{item complexity.} This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one component If it is 2.0, it loads evenly on two components, and so forth. For now, we can ignore this. \emph{I mostly wanted to reassure you that ``com'' is not ``communality''; h2 is communality}.

Let's switch to the first screen of output.

\textbf{Eigenvalues} are displayed in the row called, \emph{SS loadings} (i.e., the sum of squared loadings). They represent the variance explained by the particular linear component. PC1 explains 4.04 units of variance (out of a possible 25; the \# of components). As a proportion, this is 4.04/23 = 0.16 (reported in the \emph{Proportion Var} row).

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{4.04}\OperatorTok{/}\DecValTok{25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1616
\end{verbatim}

Note:

\begin{itemize}
\tightlist
\item
  \emph{Cumulative Var} is helpful in determining how many components we would like to retain to balance parsimony (few as possible) with the amount of variance we want to explain
\item
  The eigenvalues are in descending order. If we were to use the \emph{eigenvalue \textgreater{} 1.0} (aka, ``Kaiser's'') criteria to determine how many components to extract, we would select 4. Joliffe's critera was 0.7 (thus, we would select 10 components). Eigenvalues are only one criteria, let's look at he scree plot.
\end{itemize}

\emph{Scree plot}: we
Eigenvalues are stored in the pca1 object's variable, ``values''. We can see all the values captured by this object with the \emph{names()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(pca1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "chi"          "EPVAL"        "R2"           "objective"   
[16] "residual"     "rms"          "factors"      "dof"          "null.dof"    
[21] "null.model"   "criteria"     "STATISTIC"    "PVAL"         "weights"     
[26] "r.scores"     "Vaccounted"   "Structure"    "scores"      
\end{verbatim}

Plotting the eigen\emph{values} produces a scree plot. We can use this to further guage the number of factors we should extract.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(pca1}\OperatorTok{$}\NormalTok{values, }\DataTypeTok{type=}\StringTok{"b"}\NormalTok{) }\CommentTok{#type = "b" gives us "both" lines and points;  type = "l" gives lines and is relatively worthless}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-94-1.pdf}

We look for the point of \emph{inflexion}. That is, where the baseline levels out into a plateau. There are four components above the plateau.

\hypertarget{specifying-the-number-of-components}{%
\subsection{Specifying the number of components}\label{specifying-the-number-of-components}}

Having determined the number of components, we rerun the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville CITATION indicated that they (incuding Lewis and Neville CITATION) examined solutions with 2, 3, 4, and 5 factors (they did a parallel \emph{factor} analysis; we are still examining components).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#pca2 <- psych::principal(GRMSmatrix, nfactors=4, rotate="none")}
\NormalTok{pca2 <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{principal}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors=}\DecValTok{4}\NormalTok{, }\DataTypeTok{rotate=}\StringTok{"none"}\NormalTok{) }\CommentTok{#can copy prior script, but change nfactors and object name}
\NormalTok{pca2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
         PC1   PC2   PC3   PC4   h2   u2 com
Obj1    0.55  0.40 -0.17  0.02 0.50 0.50 2.1
Obj2    0.46  0.54 -0.15  0.14 0.55 0.45 2.2
Obj3    0.48  0.43  0.04  0.21 0.46 0.54 2.4
Obj4    0.50  0.42 -0.03  0.04 0.42 0.58 2.0
Obj5    0.33  0.57  0.13  0.02 0.44 0.56 1.7
Obj6    0.56  0.19 -0.09  0.15 0.38 0.62 1.4
Obj7    0.59  0.22 -0.09 -0.13 0.43 0.57 1.5
Obj8    0.38  0.40  0.06 -0.09 0.32 0.68 2.2
Obj9    0.40  0.28  0.21 -0.01 0.28 0.72 2.4
Obj10   0.40  0.28  0.12 -0.19 0.30 0.70 2.5
Marg1   0.63 -0.62 -0.13 -0.10 0.80 0.20 2.1
Marg2   0.47 -0.66  0.05  0.05 0.66 0.34 1.8
Marg3   0.50 -0.53 -0.11  0.02 0.54 0.46 2.1
Marg4   0.57 -0.42 -0.10  0.08 0.52 0.48 2.0
Marg5   0.58 -0.35 -0.17  0.08 0.49 0.51 1.9
Marg6   0.40 -0.44  0.07 -0.42 0.54 0.46 3.0
Marg7   0.23 -0.59  0.28  0.01 0.48 0.52 1.7
Strong1 0.04  0.02  0.65 -0.13 0.45 0.55 1.1
Strong2 0.04  0.13  0.53 -0.35 0.42 0.58 1.9
Strong3 0.09  0.21  0.41 -0.45 0.42 0.58 2.5
Strong4 0.02 -0.04  0.70 -0.15 0.51 0.49 1.1
Strong5 0.18 -0.06  0.34 -0.47 0.38 0.62 2.2
Angry1  0.19  0.07  0.30  0.70 0.62 0.38 1.5
Angry2  0.03 -0.11  0.55  0.55 0.61 0.39 2.1
Angry3  0.02 -0.21  0.49  0.59 0.63 0.37 2.2

                       PC1  PC2  PC3  PC4
SS loadings           4.04 3.61 2.41 2.09
Proportion Var        0.16 0.14 0.10 0.08
Cumulative Var        0.16 0.31 0.40 0.49
Proportion Explained  0.33 0.30 0.20 0.17
Cumulative Proportion 0.33 0.63 0.83 1.00

Mean item complexity =  2
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.05 
 with the empirical chi square  360.58  with prob <  0.00000000015 

Fit based upon off diagonal values = 0.94
\end{verbatim}

Our eigenvalues/SS loadings remain the same. With 4 components, we explain 49\% of the variance (we can see this in the ``Cumulative Var'' row.

\emph{Communality} is the proportion of common variance within a variable. Principal components analysis assumes that all variance is common; therefore, before extraction, all variance was set at 1.0. Therefore, changing from 25 to 4 components will change this value (\(h2\)) as well as its associated \emph{uniqueness} (\(u2\)), which is calculated as ``1.0 minus the communality.''

The \emph{communalities} (\(h2\)) and \emph{uniquenesses} (\(u2\)) are changed.

Now we see that 50\% of the variance associate with Obj1 is common/shared (the \(h2\) value).

Recall that we could represent this scale with all 25 items as components. But we want a more \emph{parsimonious} explanation. By respecifying a smaller number of components, we lose some information. That is, the retained components (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 50\%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction.

We can examine the communalities through the lens of Kaiser's criterion (the eigenvalue \textgreater{} 1 criteria) to see if we think that ``four'' was a good number of components to extract.

Kaiser's criterion is believed to be accurate if:

\begin{itemize}
\tightlist
\item
  when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70

  \begin{itemize}
  \tightlist
  \item
    looking at our data, only 1 communality (Marg1) is \textgreater{} .70, so, this does not support extracting four components
  \end{itemize}
\item
  when the sample size is greater than 250 (ours was 259) and the average communality is \textgreater{} .60

  \begin{itemize}
  \tightlist
  \item
    we can extract the communalities from our object and calculate the mean the average communality
  \end{itemize}
\end{itemize}

Using the \emph{names()} function again, we see that ``communality'' is available. Thus, we can easily calculate their mean. To get this value let's first examine the possible contents of the object we created from this PCA analysis by asking for its names.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(pca2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "chi"          "EPVAL"        "R2"           "objective"   
[16] "residual"     "rms"          "factors"      "dof"          "null.dof"    
[21] "null.model"   "criteria"     "STATISTIC"    "PVAL"         "weights"     
[26] "r.scores"     "Vaccounted"   "Structure"    "scores"      
\end{verbatim}

We see that it includes communalities. Thus, we can easily calculate their mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(pca2}\OperatorTok{$}\NormalTok{communality)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4857967
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#sum(pca2$communality) #checking my work by calculating the sum and dividing by 25}
\CommentTok{#12.14492/25}
\end{Highlighting}
\end{Shaded}

We see that the average communality is 0.48. These two criteria would suggest that we may not have the best solution. That said (in our defense):

\begin{itemize}
\tightlist
\item
  We used the scree plot as a guide and it was very clear.
\item
  We have an adequate sample size and that was supported with the KMO.
\item
  Are the number of components consistent with theory? We have not yet inspected the component loadings. This will provide us with more information.
\end{itemize}

We could do several things:

\begin{itemize}
\tightlist
\item
  rerun with a different number of components (recall Lewis and Neville \citeyearpar{lewis_construction_2015} ran models with 2, 3, 4, and 5 factors)
\item
  conduct more diagnostics

  \begin{itemize}
  \tightlist
  \item
    reproduced correlation matrix
  \item
    the difference between the reproduced correlation matrix and the correlation matrix in the data
  \end{itemize}
\end{itemize}

The \emph{factor.model()} function in \emph{psych} produces the \emph{reproduced correlation matrix} by using the \emph{loadings} in our extracted object. Conceptually, this matrix is the correlations that should be produced if we did not have the raw data but we only had the component loadings. We could do fancy matrix algebra and produce these.

The questions, though, is: How close did we get? How different is the \emph{reproduced correlation matrix} from \emph{GRMSmatrix} -- the \(R\)-matrix produced from our raw data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(psych}\OperatorTok{::}\KeywordTok{factor.model}\NormalTok{(pca2}\OperatorTok{$}\NormalTok{loadings),}\DecValTok{3}\NormalTok{)}\CommentTok{#produces the reproduced correlation matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1     0.495  0.501  0.435  0.447  0.386  0.403  0.428  0.359  0.294  0.311
Obj2     0.501  0.547  0.477  0.466  0.442  0.395  0.390  0.372  0.303  0.297
Obj3     0.435  0.477  0.460  0.426  0.408  0.380  0.349  0.339  0.318  0.281
Obj4     0.447  0.466  0.426  0.424  0.396  0.367  0.384  0.351  0.307  0.307
Obj5     0.386  0.442  0.408  0.396  0.444  0.282  0.306  0.358  0.315  0.307
Obj6     0.403  0.395  0.380  0.367  0.282  0.383  0.362  0.271  0.257  0.240
Obj7     0.428  0.390  0.349  0.384  0.306  0.362  0.426  0.322  0.278  0.317
Obj8     0.359  0.372  0.339  0.351  0.358  0.271  0.322  0.319  0.278  0.295
Obj9     0.294  0.303  0.318  0.307  0.315  0.257  0.278  0.278  0.281  0.267
Obj10    0.311  0.297  0.281  0.307  0.307  0.240  0.317  0.295  0.267  0.298
Marg1    0.116 -0.042  0.010  0.052 -0.162  0.232  0.258 -0.005  0.051  0.082
Marg2   -0.016 -0.143 -0.045 -0.043 -0.213  0.144  0.118 -0.086  0.012 -0.003
Marg3    0.081 -0.038  0.015  0.031 -0.148  0.195  0.184 -0.028  0.030  0.034
Marg4    0.165  0.062  0.109  0.115 -0.060  0.264  0.243  0.038  0.089  0.084
Marg5    0.208  0.112  0.137  0.149 -0.029  0.286  0.270  0.064  0.096  0.098
Marg6    0.020 -0.123 -0.082 -0.007 -0.114  0.070  0.187  0.022  0.054  0.128
Marg7   -0.164 -0.258 -0.133 -0.145 -0.226 -0.007 -0.028 -0.135 -0.017 -0.046
Strong1 -0.086 -0.081  0.028  0.003  0.110 -0.049 -0.016  0.078  0.161  0.129
Strong2 -0.028 -0.037  0.020  0.040  0.148 -0.055  0.047  0.132  0.163  0.185
Strong3  0.056  0.039  0.059  0.103  0.198 -0.011  0.124  0.189  0.186  0.237
Strong4 -0.131 -0.136 -0.013 -0.037  0.070 -0.080 -0.043  0.048  0.144  0.112
Strong5  0.003 -0.065 -0.026  0.031  0.060 -0.013  0.123  0.109  0.128  0.189
Angry1   0.094  0.175  0.278  0.143  0.152  0.200  0.005  0.054  0.154 -0.003
Angry2  -0.108 -0.048  0.106 -0.022  0.030  0.037 -0.129 -0.047  0.096 -0.056
Angry3  -0.147 -0.098  0.059 -0.069 -0.040  0.018 -0.161 -0.101  0.048 -0.106
         Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7 Strong1 Strong2
Obj1     0.116 -0.016  0.081  0.165  0.208  0.020 -0.164  -0.086  -0.028
Obj2    -0.042 -0.143 -0.038  0.062  0.112 -0.123 -0.258  -0.081  -0.037
Obj3     0.010 -0.045  0.015  0.109  0.137 -0.082 -0.133   0.028   0.020
Obj4     0.052 -0.043  0.031  0.115  0.149 -0.007 -0.145   0.003   0.040
Obj5    -0.162 -0.213 -0.148 -0.060 -0.029 -0.114 -0.226   0.110   0.148
Obj6     0.232  0.144  0.195  0.264  0.286  0.070 -0.007  -0.049  -0.055
Obj7     0.258  0.118  0.184  0.243  0.270  0.187 -0.028  -0.016   0.047
Obj8    -0.005 -0.086 -0.028  0.038  0.064  0.022 -0.135   0.078   0.132
Obj9     0.051  0.012  0.030  0.089  0.096  0.054 -0.017   0.161   0.163
Obj10    0.082 -0.003  0.034  0.084  0.098  0.128 -0.046   0.129   0.185
Marg1    0.800  0.690  0.648  0.619  0.592  0.554  0.469  -0.062  -0.089
Marg2    0.690  0.664  0.579  0.544  0.498  0.458  0.512   0.027  -0.063
Marg3    0.648  0.579  0.538  0.517  0.492  0.412  0.396  -0.065  -0.114
Marg4    0.619  0.544  0.517  0.516  0.499  0.368  0.349  -0.063  -0.114
Marg5    0.592  0.498  0.492  0.499  0.491  0.338  0.290  -0.108  -0.142
Marg6    0.554  0.458  0.412  0.368  0.338  0.537  0.365   0.108   0.146
Marg7    0.469  0.512  0.396  0.349  0.290  0.365  0.481   0.175   0.073
Strong1 -0.062  0.027 -0.065 -0.063 -0.108  0.108  0.175   0.447   0.394
Strong2 -0.089 -0.063 -0.114 -0.114 -0.142  0.146  0.073   0.394   0.419
Strong3 -0.080 -0.102 -0.119 -0.113 -0.125  0.164  0.002   0.334   0.404
Strong4 -0.035  0.064 -0.043 -0.052 -0.105  0.142  0.223   0.476   0.415
Strong5  0.157  0.117  0.077  0.057  0.032  0.326  0.167   0.289   0.344
Angry1  -0.040  0.092  0.040  0.103  0.083 -0.232  0.092   0.118  -0.071
Angry2  -0.042  0.142  0.028  0.053  0.004 -0.133  0.229   0.286   0.081
Angry3   0.014  0.200  0.080  0.095  0.042 -0.115  0.270   0.240   0.024
        Strong3 Strong4 Strong5 Angry1 Angry2 Angry3
Obj1      0.056  -0.131   0.003  0.094 -0.108 -0.147
Obj2      0.039  -0.136  -0.065  0.175 -0.048 -0.098
Obj3      0.059  -0.013  -0.026  0.278  0.106  0.059
Obj4      0.103  -0.037   0.031  0.143 -0.022 -0.069
Obj5      0.198   0.070   0.060  0.152  0.030 -0.040
Obj6     -0.011  -0.080  -0.013  0.200  0.037  0.018
Obj7      0.124  -0.043   0.123  0.005 -0.129 -0.161
Obj8      0.189   0.048   0.109  0.054 -0.047 -0.101
Obj9      0.186   0.144   0.128  0.154  0.096  0.048
Obj10     0.237   0.112   0.189 -0.003 -0.056 -0.106
Marg1    -0.080  -0.035   0.157 -0.040 -0.042  0.014
Marg2    -0.102   0.064   0.117  0.092  0.142  0.200
Marg3    -0.119  -0.043   0.077  0.040  0.028  0.080
Marg4    -0.113  -0.052   0.057  0.103  0.053  0.095
Marg5    -0.125  -0.105   0.032  0.083  0.004  0.042
Marg6     0.164   0.142   0.326 -0.232 -0.133 -0.115
Marg7     0.002   0.223   0.167  0.092  0.229  0.270
Strong1   0.334   0.476   0.289  0.118  0.286  0.240
Strong2   0.404   0.415   0.344 -0.071  0.081  0.024
Strong3   0.423   0.345   0.355 -0.158 -0.043 -0.106
Strong4   0.345   0.511   0.314  0.108  0.305  0.263
Strong5   0.355   0.314   0.377 -0.201 -0.063 -0.096
Angry1   -0.158   0.108  -0.201  0.624  0.551  0.548
Angry2   -0.043   0.305  -0.063  0.551  0.615  0.613
Angry3   -0.106   0.263  -0.096  0.548  0.613  0.626
\end{verbatim}

We're not really interested in this matrix. We just need it to compare it to the \emph{GRMSmatrix} to produce the residuals. We do that next.

\textbf{Residuals} are the difference between the reproduced (i.e., those created from our component loadings) and \(R\)-matrix produced by the raw data.

If we look at the \(r_{_{Obj1Obj2}}\) in our original correlation matrix (theoreticaly from the raw data {[}although we simulated data{]}), the value is 0.41 The reproduced correlation that we just calculated for this pair is 0.50. The diffference is -0.09.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{.41} \OperatorTok{-}\StringTok{ }\FloatTok{.50}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.09
\end{verbatim}

By using the \emph{factor.residuals()} function we can calculate the residuals. Here we will see this difference calculated for us, for all the elements in the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(psych}\OperatorTok{::}\KeywordTok{factor.residuals}\NormalTok{(GRMSmatrix, pca2}\OperatorTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1     0.505 -0.093 -0.052 -0.118 -0.047 -0.067 -0.062 -0.010 -0.039 -0.070
Obj2    -0.093  0.453 -0.047 -0.066 -0.045 -0.055 -0.038 -0.079 -0.026 -0.060
Obj3    -0.052 -0.047  0.540 -0.068 -0.067 -0.068 -0.049 -0.059 -0.102 -0.045
Obj4    -0.118 -0.066 -0.068  0.576 -0.074 -0.046 -0.059 -0.097 -0.038 -0.030
Obj5    -0.047 -0.045 -0.067 -0.074  0.556 -0.054 -0.050 -0.097 -0.049 -0.060
Obj6    -0.067 -0.055 -0.068 -0.046 -0.054  0.617 -0.057 -0.033 -0.064 -0.077
Obj7    -0.062 -0.038 -0.049 -0.059 -0.050 -0.057  0.574 -0.067 -0.086 -0.056
Obj8    -0.010 -0.079 -0.059 -0.097 -0.097 -0.033 -0.067  0.681 -0.070 -0.070
Obj9    -0.039 -0.026 -0.102 -0.038 -0.049 -0.064 -0.086 -0.070  0.719 -0.047
Obj10   -0.070 -0.060 -0.045 -0.030 -0.060 -0.077 -0.056 -0.070 -0.047  0.702
Marg1   -0.011  0.017  0.002  0.005  0.018 -0.030 -0.027  0.004  0.015 -0.018
Marg2    0.016  0.016  0.011  0.014  0.030  0.011 -0.012  0.001 -0.002 -0.024
Marg3   -0.026  0.025  0.003  0.011  0.039 -0.034 -0.060  0.001  0.036 -0.010
Marg4    0.002 -0.013 -0.005 -0.044  0.028 -0.054 -0.028  0.014 -0.044 -0.018
Marg5   -0.012 -0.039 -0.029 -0.023  0.032 -0.078 -0.045 -0.019  0.009  0.027
Marg6   -0.018  0.035  0.020  0.032  0.007 -0.007 -0.028  0.004 -0.003  0.005
Marg7    0.020  0.036  0.043  0.014  0.054  0.008  0.016  0.031 -0.023  0.006
Strong1  0.040  0.015  0.002 -0.026 -0.001 -0.005  0.021 -0.043 -0.019 -0.030
Strong2  0.034 -0.023 -0.008  0.001 -0.020  0.042 -0.008 -0.001 -0.044 -0.056
Strong3 -0.015  0.015  0.011  0.038 -0.053  0.032 -0.019 -0.085 -0.056 -0.095
Strong4  0.026  0.031  0.021 -0.003 -0.008  0.038 -0.005 -0.003 -0.038 -0.025
Strong5  0.019  0.062  0.021 -0.020 -0.022  0.014 -0.020  0.002 -0.074 -0.080
Angry1   0.015 -0.026 -0.077  0.006 -0.045 -0.046  0.006  0.022 -0.072  0.028
Angry2  -0.007  0.012 -0.030 -0.002 -0.019 -0.031  0.061  0.040 -0.030  0.011
Angry3   0.023  0.006 -0.027  0.008 -0.010 -0.001  0.040 -0.008  0.001  0.040
         Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7 Strong1 Strong2
Obj1    -0.011  0.016 -0.026  0.002 -0.012 -0.018  0.020   0.040   0.034
Obj2     0.017  0.016  0.025 -0.013 -0.039  0.035  0.036   0.015  -0.023
Obj3     0.002  0.011  0.003 -0.005 -0.029  0.020  0.043   0.002  -0.008
Obj4     0.005  0.014  0.011 -0.044 -0.023  0.032  0.014  -0.026   0.001
Obj5     0.018  0.030  0.039  0.028  0.032  0.007  0.054  -0.001  -0.020
Obj6    -0.030  0.011 -0.034 -0.054 -0.078 -0.007  0.008  -0.005   0.042
Obj7    -0.027 -0.012 -0.060 -0.028 -0.045 -0.028  0.016   0.021  -0.008
Obj8     0.004  0.001  0.001  0.014 -0.019  0.004  0.031  -0.043  -0.001
Obj9     0.015 -0.002  0.036 -0.044  0.009 -0.003 -0.023  -0.019  -0.044
Obj10   -0.018 -0.024 -0.010 -0.018  0.027  0.005  0.006  -0.030  -0.056
Marg1    0.200 -0.026 -0.033 -0.041 -0.044 -0.041 -0.049   0.020  -0.006
Marg2   -0.026  0.336 -0.077 -0.070 -0.090 -0.026 -0.092   0.020   0.002
Marg3   -0.033 -0.077  0.462 -0.101 -0.091 -0.066 -0.058   0.004   0.019
Marg4   -0.041 -0.070 -0.101  0.484 -0.055 -0.087 -0.074   0.038   0.032
Marg5   -0.044 -0.090 -0.091 -0.055  0.509 -0.093 -0.031   0.010   0.066
Marg6   -0.041 -0.026 -0.066 -0.087 -0.093  0.463 -0.074  -0.007  -0.056
Marg7   -0.049 -0.092 -0.058 -0.074 -0.031 -0.074  0.519  -0.085   0.006
Strong1  0.020  0.020  0.004  0.038  0.010 -0.007 -0.085   0.553  -0.133
Strong2 -0.006  0.002  0.019  0.032  0.066 -0.056  0.006  -0.133   0.581
Strong3  0.022  0.013  0.033  0.034  0.024 -0.062 -0.016  -0.090  -0.135
Strong4 -0.005 -0.012  0.019  0.015  0.040 -0.037 -0.032  -0.133  -0.100
Strong5 -0.013 -0.020 -0.011  0.001 -0.020 -0.096 -0.032  -0.096  -0.130
Angry1   0.003 -0.029  0.009 -0.023 -0.013  0.076 -0.014  -0.030   0.021
Angry2   0.009 -0.029 -0.005  0.006 -0.010  0.053 -0.044  -0.071  -0.031
Angry3   0.007  0.006 -0.027 -0.026 -0.004  0.029 -0.070  -0.061   0.009
        Strong3 Strong4 Strong5 Angry1 Angry2 Angry3
Obj1     -0.015   0.026   0.019  0.015 -0.007  0.023
Obj2      0.015   0.031   0.062 -0.026  0.012  0.006
Obj3      0.011   0.021   0.021 -0.077 -0.030 -0.027
Obj4      0.038  -0.003  -0.020  0.006 -0.002  0.008
Obj5     -0.053  -0.008  -0.022 -0.045 -0.019 -0.010
Obj6      0.032   0.038   0.014 -0.046 -0.031 -0.001
Obj7     -0.019  -0.005  -0.020  0.006  0.061  0.040
Obj8     -0.085  -0.003   0.002  0.022  0.040 -0.008
Obj9     -0.056  -0.038  -0.074 -0.072 -0.030  0.001
Obj10    -0.095  -0.025  -0.080  0.028  0.011  0.040
Marg1     0.022  -0.005  -0.013  0.003  0.009  0.007
Marg2     0.013  -0.012  -0.020 -0.029 -0.029  0.006
Marg3     0.033   0.019  -0.011  0.009 -0.005 -0.027
Marg4     0.034   0.015   0.001 -0.023  0.006 -0.026
Marg5     0.024   0.040  -0.020 -0.013 -0.010 -0.004
Marg6    -0.062  -0.037  -0.096  0.076  0.053  0.029
Marg7    -0.016  -0.032  -0.032 -0.014 -0.044 -0.070
Strong1  -0.090  -0.133  -0.096 -0.030 -0.071 -0.061
Strong2  -0.135  -0.100  -0.130  0.021 -0.031  0.009
Strong3   0.577  -0.096  -0.087  0.056  0.024  0.040
Strong4  -0.096   0.489  -0.097 -0.047 -0.069 -0.064
Strong5  -0.087  -0.097   0.623  0.092  0.041  0.044
Angry1    0.056  -0.047   0.092  0.376 -0.108 -0.115
Angry2    0.024  -0.069   0.041 -0.108  0.385 -0.147
Angry3    0.040  -0.064   0.044 -0.115 -0.147  0.374
\end{verbatim}

There are several strategies to evaluate this matrix:

\begin{itemize}
\tightlist
\item
  see how large the residuals are compared to the original correlations

  \begin{itemize}
  \tightlist
  \item
    the worst possible model would occur if we extracted no components and would be the size of the original correlations
  \item
    if the correlations were small to start with, we expect small residuals
  \item
    if the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic)
  \end{itemize}
\item
  comparing residuals requires squaring them first (because residuals can be both positive and negative)

  \begin{itemize}
  \tightlist
  \item
    the sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit. Subtracting this from 1.0 means that it ranges from 0 to 1. Values \textgreater{} .95 are an indication of good fit.
  \end{itemize}
\end{itemize}

Analyzing the residuals means we need to extract only the upper right of the triangle them into an object. We can do this in steps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca2_resids <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{factor.residuals}\NormalTok{(GRMSmatrix, pca2}\OperatorTok{$}\NormalTok{loadings)}\CommentTok{#first extract the resids}
\NormalTok{pca2_resids <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(pca2_resids[}\KeywordTok{upper.tri}\NormalTok{(pca2_resids)])}\CommentTok{#the object has the residuals in a single column}
\KeywordTok{head}\NormalTok{(pca2_resids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            [,1]
[1,] -0.09259539
[2,] -0.05244358
[3,] -0.04745227
[4,] -0.11845843
[5,] -0.06581435
[6,] -0.06842963
\end{verbatim}

One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is \textgreater{} \textbar0.05\textbar{} and false if it is smaller. The sum function will tell us how many TRUE repsonses are in the matrix. Further, we can write script to obtain the proportion of total number of residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{large.resid <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(pca2_resids) }\OperatorTok{>}\StringTok{ }\FloatTok{0.05}
\CommentTok{#large.resid}
\KeywordTok{sum}\NormalTok{(large.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 85
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(large.resid)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(pca2_resids),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.283
\end{verbatim}

We learn that there are 85 residuals greater than the absolute value of 0.05. This represents 28\% of the total number of residuals.

There are no hard rules about what proportion of residuals can be greater than 0.05. A common practice is to stay below 50\% \citep{field_discovering_2012}.

Another approach to analyzing residuals is to look at their mean. Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(pca2_resids}\OperatorTok{^}\DecValTok{2}\NormalTok{)),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.048
\end{verbatim}

While there are no clear guidelines to interpret these, one recommendation is to consider extracting more components if the value is higher than 0.08 \citep{field_discovering_2012}.

Finally, we expect our residuals to be normally distributed. A histogram can help us inspect the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(pca2_resids)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-103-1.pdf}

Not bad! Looks reasonably normal. No outliers.

\hypertarget{quick-recap-of-how-to-evaluate-the-of-components-we-extracted}{%
\subsubsection{Quick recap of how to evaluate the \# of components we extracted}\label{quick-recap-of-how-to-evaluate-the-of-components-we-extracted}}

\begin{itemize}
\tightlist
\item
  If fewer than 30 variables, the eigenvalue \textgreater{} 1 (Kaiser's) critera is fine, so long as communalities are all \textgreater{} .70.
\item
  If sample size \textgreater{} 250 and the average communalitie are .6 or greater, this is fine.
\item
  When \emph{N} \textgreater{} 200, the scree plot can be used.
\item
  Regarding residuals

  \begin{itemize}
  \tightlist
  \item
    fewer than 50\% should have absolute values \textgreater{} 0.05
  \item
    model fit should be \textgreater{} 0.90
  \end{itemize}
\end{itemize}

\hypertarget{component-rotation}{%
\subsection{Component rotation}\label{component-rotation}}

Rotation improves the interpretation of the components by maximizing the loading on each variable on one of the extracted components while minimizing the loading on all other components. Rotation works by changing the absolute values of the variables while keeping their differential vales constant.

There are two big choices(to be made on theoretical grounds):

\begin{itemize}
\tightlist
\item
  Orthogonal rotation if you think that the components are independent/unrelated.

  \begin{itemize}
  \tightlist
  \item
    most common orthogonal rotation is varimax
  \end{itemize}
\item
  Oblique rotation if you think that the components are related correlated.

  \begin{itemize}
  \tightlist
  \item
    oblimin and promax are common oblique rotations
  \end{itemize}
\end{itemize}

Which to do?

\begin{itemize}
\tightlist
\item
  Orthogonal is ``easy'' because it minimizes cross-loadings, but
\item
  Can you think of a measure where the subscales would \emph{not} be correlated?
\end{itemize}

\hypertarget{orthogonal-rotation}{%
\subsubsection{Orthogonal Rotation}\label{orthogonal-rotation}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#pcaORTH <- psych::principal(GRMSmatrix, nfactors = 4, rotate = "varimax")}
\NormalTok{pcaORTH <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{principal}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{pcaORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
          RC1   RC2   RC3   RC4   h2   u2 com
Obj1     0.69  0.08 -0.10 -0.09 0.50 0.50 1.1
Obj2     0.72 -0.10 -0.14  0.00 0.55 0.45 1.1
Obj3     0.65 -0.01 -0.04  0.17 0.46 0.54 1.1
Obj4     0.65  0.02  0.00  0.00 0.42 0.58 1.0
Obj5     0.61 -0.21  0.14  0.05 0.44 0.56 1.4
Obj6     0.56  0.23 -0.11  0.08 0.38 0.62 1.5
Obj7     0.58  0.24  0.05 -0.16 0.43 0.57 1.5
Obj8     0.54 -0.04  0.15 -0.06 0.32 0.68 1.2
Obj9     0.47  0.04  0.22  0.11 0.28 0.72 1.5
Obj10    0.47  0.06  0.25 -0.10 0.30 0.70 1.7
Marg1    0.06  0.89 -0.02 -0.08 0.80 0.20 1.0
Marg2   -0.09  0.80  0.02  0.14 0.66 0.34 1.1
Marg3    0.03  0.73 -0.08  0.02 0.54 0.46 1.0
Marg4    0.16  0.69 -0.10  0.07 0.52 0.48 1.2
Marg5    0.21  0.65 -0.15  0.01 0.49 0.51 1.3
Marg6   -0.03  0.61  0.32 -0.25 0.54 0.46 1.9
Marg7   -0.24  0.57  0.22  0.23 0.48 0.52 2.0
Strong1  0.00 -0.03  0.61  0.27 0.45 0.55 1.4
Strong2  0.06 -0.09  0.64  0.00 0.42 0.58 1.1
Strong3  0.16 -0.10  0.60 -0.15 0.42 0.58 1.3
Strong4 -0.06  0.01  0.66  0.28 0.51 0.49 1.4
Strong5  0.04  0.17  0.56 -0.18 0.38 0.62 1.4
Angry1   0.22  0.01 -0.13  0.75 0.62 0.38 1.2
Angry2  -0.03  0.03  0.13  0.77 0.61 0.39 1.1
Angry3  -0.11  0.10  0.06  0.77 0.63 0.37 1.1

                       RC1  RC2  RC3  RC4
SS loadings           3.84 3.79 2.32 2.20
Proportion Var        0.15 0.15 0.09 0.09
Cumulative Var        0.15 0.31 0.40 0.49
Proportion Explained  0.32 0.31 0.19 0.18
Cumulative Proportion 0.32 0.63 0.82 1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.05 
 with the empirical chi square  360.58  with prob <  0.00000000015 

Fit based upon off diagonal values = 0.94
\end{verbatim}

Essentially we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the component loadings while keeping their differential vales constant). Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance will remain the same.

The \emph{print.psych()} function facilitates interpretation and prioritizes the information about which we care most:

\begin{itemize}
\tightlist
\item
  ``cut'' will display loadings above .3

  \begin{itemize}
  \tightlist
  \item
    if some items load on no factors
  \item
    if some items have cross-loadings (and their relative weights)
  \end{itemize}
\item
  ``sort'' will reorder the loadings to make it clearer (to the best of its ability\ldots in the case of ties) to which component/scale it belongs
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca_table <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{print.psych}\NormalTok{(pcaORTH, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{sort=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
        item   RC1   RC2   RC3   RC4   h2   u2 com
Obj2       2  0.72                   0.55 0.45 1.1
Obj1       1  0.69                   0.50 0.50 1.1
Obj3       3  0.65                   0.46 0.54 1.1
Obj4       4  0.65                   0.42 0.58 1.0
Obj5       5  0.61                   0.44 0.56 1.4
Obj7       7  0.58                   0.43 0.57 1.5
Obj6       6  0.56                   0.38 0.62 1.5
Obj8       8  0.54                   0.32 0.68 1.2
Obj9       9  0.47                   0.28 0.72 1.5
Obj10     10  0.47                   0.30 0.70 1.7
Marg1     11        0.89             0.80 0.20 1.0
Marg2     12        0.80             0.66 0.34 1.1
Marg3     13        0.73             0.54 0.46 1.0
Marg4     14        0.69             0.52 0.48 1.2
Marg5     15        0.65             0.49 0.51 1.3
Marg6     16        0.61  0.32       0.54 0.46 1.9
Marg7     17        0.57             0.48 0.52 2.0
Strong4   21              0.66       0.51 0.49 1.4
Strong2   19              0.64       0.42 0.58 1.1
Strong1   18              0.61       0.45 0.55 1.4
Strong3   20              0.60       0.42 0.58 1.3
Strong5   22              0.56       0.38 0.62 1.4
Angry3    25                    0.77 0.63 0.37 1.1
Angry2    24                    0.77 0.61 0.39 1.1
Angry1    23                    0.75 0.62 0.38 1.2

                       RC1  RC2  RC3  RC4
SS loadings           3.84 3.79 2.32 2.20
Proportion Var        0.15 0.15 0.09 0.09
Cumulative Var        0.15 0.31 0.40 0.49
Proportion Explained  0.32 0.31 0.19 0.18
Cumulative Proportion 0.32 0.63 0.82 1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.05 
 with the empirical chi square  360.58  with prob <  0.00000000015 

Fit based upon off diagonal values = 0.94
\end{verbatim}

In the unrotated solution, most variables loaded on the first component. After rotation, there are four clear components/scales. Further, there is clear (or at least reasonable) component/scale membership for each item. The item Marg7 was the only one that included cross-loadings. However, the difference was so great (.61 on its primary factor; .32 on the secondary factor) that the item's membership is clearly on the second component.

If this were a new scale and we had not yet established ideas for subscales, the next step is to look back at the items, themselves, and try to name the scales/components. If our scale construction included a priori/planned subscales, here's where we hope the items fall where they were hypothesized to do so. Our simulated data worked perfectly and replicated the four scales that Lewis and Neville CITATION reported in the article.

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification
\item
  Silenced and Marginalized
\item
  Strong Woman Stereotype
\item
  Angry Woman Stereotype
\end{itemize}

We can also create a figure of the result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.diagram}\NormalTok{(pcaORTH)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-106-1.pdf}

We can extract the component loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(pcaORTH)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "values"       "rotation"     "n.obs"        "communality"  "loadings"    
 [6] "fit"          "fit.off"      "fn"           "Call"         "uniquenesses"
[11] "complexity"   "chi"          "EPVAL"        "R2"           "objective"   
[16] "residual"     "rms"          "factors"      "dof"          "null.dof"    
[21] "null.model"   "criteria"     "STATISTIC"    "PVAL"         "weights"     
[26] "r.scores"     "rot.mat"      "Vaccounted"   "Structure"    "scores"      
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaORTH_table <-}\StringTok{ }\KeywordTok{round}\NormalTok{(pcaORTH}\OperatorTok{$}\NormalTok{loadings,}\DecValTok{3}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(pcaORTH_table, }\DataTypeTok{file=}\StringTok{"pcaORTH_table.csv"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{pcaORTH_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
        RC1    RC2    RC3    RC4   
Obj1     0.686        -0.100       
Obj2     0.720        -0.142       
Obj3     0.654                0.174
Obj4     0.651                     
Obj5     0.614 -0.212  0.142       
Obj6     0.558  0.231 -0.109       
Obj7     0.583  0.241        -0.159
Obj8     0.541         0.147       
Obj9     0.470         0.216  0.107
Obj10    0.470         0.253       
Marg1           0.888              
Marg2           0.797         0.145
Marg3           0.728              
Marg4    0.158  0.691              
Marg5    0.211  0.651 -0.153       
Marg6           0.609  0.317 -0.253
Marg7   -0.240  0.570  0.217  0.226
Strong1                0.613  0.266
Strong2                0.638       
Strong3  0.160         0.604 -0.150
Strong4                0.656  0.278
Strong5         0.171  0.559 -0.184
Angry1   0.219        -0.133  0.747
Angry2                 0.134  0.771
Angry3  -0.110                0.775

                 RC1   RC2   RC3   RC4
SS loadings    3.839 3.788 2.316 2.204
Proportion Var 0.154 0.152 0.093 0.088
Cumulative Var 0.154 0.305 0.398 0.486
\end{verbatim}

\hypertarget{oblique-rotation}{%
\subsubsection{Oblique Rotation}\label{oblique-rotation}}

Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the coponents, an oblique rotation will allow them to be correlated. Researchers often explore both solutions, but then report only one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#pcaOBL <- psych::principal(GRMSmatrix, nfactors = 4, rotate = "oblimin")}
\NormalTok{pcaOBL <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{principal}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{pcaOBL}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
          TC1   TC2   TC3   TC4   h2   u2 com
Obj1     0.68  0.08 -0.07 -0.08 0.50 0.50 1.1
Obj2     0.72 -0.09 -0.11  0.02 0.55 0.45 1.1
Obj3     0.66 -0.02 -0.01  0.19 0.46 0.54 1.2
Obj4     0.65  0.02  0.03  0.01 0.42 0.58 1.0
Obj5     0.62 -0.21  0.17  0.06 0.44 0.56 1.4
Obj6     0.55  0.23 -0.09  0.10 0.38 0.62 1.5
Obj7     0.58  0.25  0.07 -0.15 0.43 0.57 1.5
Obj8     0.54 -0.04  0.17 -0.06 0.32 0.68 1.2
Obj9     0.47  0.04  0.23  0.11 0.28 0.72 1.6
Obj10    0.46  0.06  0.27 -0.10 0.30 0.70 1.8
Marg1    0.03  0.89 -0.03 -0.08 0.80 0.20 1.0
Marg2   -0.11  0.79  0.01  0.14 0.66 0.34 1.1
Marg3    0.01  0.73 -0.08  0.03 0.54 0.46 1.0
Marg4    0.14  0.69 -0.10  0.07 0.52 0.48 1.1
Marg5    0.20  0.65 -0.15  0.02 0.49 0.51 1.3
Marg6   -0.05  0.61  0.31 -0.26 0.54 0.46 1.9
Marg7   -0.26  0.56  0.20  0.21 0.48 0.52 2.0
Strong1  0.00 -0.04  0.61  0.24 0.45 0.55 1.3
Strong2  0.06 -0.10  0.64 -0.02 0.42 0.58 1.1
Strong3  0.15 -0.10  0.61 -0.17 0.42 0.58 1.3
Strong4 -0.06 -0.01  0.65  0.25 0.51 0.49 1.3
Strong5  0.03  0.17  0.56 -0.20 0.38 0.62 1.5
Angry1   0.23 -0.01 -0.12  0.76 0.62 0.38 1.2
Angry2  -0.03  0.01  0.13  0.77 0.61 0.39 1.1
Angry3  -0.11  0.08  0.06  0.77 0.63 0.37 1.1

                       TC1  TC2  TC3  TC4
SS loadings           3.82 3.79 2.33 2.20
Proportion Var        0.15 0.15 0.09 0.09
Cumulative Var        0.15 0.30 0.40 0.49
Proportion Explained  0.31 0.31 0.19 0.18
Cumulative Proportion 0.31 0.63 0.82 1.00

 With component correlations of 
      TC1  TC2   TC3   TC4
TC1  1.00 0.02 -0.03 -0.02
TC2  0.02 1.00  0.02  0.03
TC3 -0.03 0.02  1.00  0.03
TC4 -0.02 0.03  0.03  1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.05 
 with the empirical chi square  360.58  with prob <  0.00000000015 

Fit based upon off diagonal values = 0.94
\end{verbatim}

We can make it a little easier to interpret by removing all factor loadings below .30.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{print.psych}\NormalTok{(pcaOBL, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{sort=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Principal Components Analysis
Call: psych::principal(r = dfGRMS, nfactors = 4, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
        item   TC1   TC2   TC3   TC4   h2   u2 com
Obj2       2  0.72                   0.55 0.45 1.1
Obj1       1  0.68                   0.50 0.50 1.1
Obj3       3  0.66                   0.46 0.54 1.2
Obj4       4  0.65                   0.42 0.58 1.0
Obj5       5  0.62                   0.44 0.56 1.4
Obj7       7  0.58                   0.43 0.57 1.5
Obj6       6  0.55                   0.38 0.62 1.5
Obj8       8  0.54                   0.32 0.68 1.2
Obj9       9  0.47                   0.28 0.72 1.6
Obj10     10  0.46                   0.30 0.70 1.8
Marg1     11        0.89             0.80 0.20 1.0
Marg2     12        0.79             0.66 0.34 1.1
Marg3     13        0.73             0.54 0.46 1.0
Marg4     14        0.69             0.52 0.48 1.1
Marg5     15        0.65             0.49 0.51 1.3
Marg6     16        0.61  0.31       0.54 0.46 1.9
Marg7     17        0.56             0.48 0.52 2.0
Strong4   21              0.65       0.51 0.49 1.3
Strong2   19              0.64       0.42 0.58 1.1
Strong1   18              0.61       0.45 0.55 1.3
Strong3   20              0.61       0.42 0.58 1.3
Strong5   22              0.56       0.38 0.62 1.5
Angry3    25                    0.77 0.63 0.37 1.1
Angry2    24                    0.77 0.61 0.39 1.1
Angry1    23                    0.76 0.62 0.38 1.2

                       TC1  TC2  TC3  TC4
SS loadings           3.82 3.79 2.33 2.20
Proportion Var        0.15 0.15 0.09 0.09
Cumulative Var        0.15 0.30 0.40 0.49
Proportion Explained  0.31 0.31 0.19 0.18
Cumulative Proportion 0.31 0.63 0.82 1.00

 With component correlations of 
      TC1  TC2   TC3   TC4
TC1  1.00 0.02 -0.03 -0.02
TC2  0.02 1.00  0.02  0.03
TC3 -0.03 0.02  1.00  0.03
TC4 -0.02 0.03  0.03  1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 components are sufficient.

The root mean square of the residuals (RMSR) is  0.05 
 with the empirical chi square  360.58  with prob <  0.00000000015 

Fit based upon off diagonal values = 0.94
\end{verbatim}

All of the items stayed in their respective components. Note, though, that because our specification included ``sort=TRUE'' that the relative weights wiggled around and so the items are listed in a different order than in the orthogonal rotation.

Let's create a table and write it to our file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL_table <-}\StringTok{ }\KeywordTok{round}\NormalTok{(pcaOBL}\OperatorTok{$}\NormalTok{loadings,}\DecValTok{3}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(pcaOBL_table, }\DataTypeTok{file=}\StringTok{"pcaOBL_table.csv"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{pcaOBL_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
        TC1    TC2    TC3    TC4   
Obj1     0.685                     
Obj2     0.724        -0.112       
Obj3     0.657                0.185
Obj4     0.650                     
Obj5     0.618 -0.213  0.169       
Obj6     0.554  0.232              
Obj7     0.576  0.246        -0.151
Obj8     0.540         0.169       
Obj9     0.467         0.235  0.107
Obj10    0.465         0.272       
Marg1           0.891              
Marg2   -0.108  0.793         0.143
Marg3           0.729              
Marg4    0.142  0.691              
Marg5    0.197  0.653 -0.149       
Marg6           0.612  0.311 -0.265
Marg7   -0.255  0.561  0.204  0.215
Strong1                0.614  0.245
Strong2                0.641       
Strong3  0.154 -0.102  0.611 -0.169
Strong4                0.654  0.254
Strong5         0.169  0.559 -0.203
Angry1   0.226        -0.123  0.756
Angry2                 0.134  0.766
Angry3  -0.108                0.771

                 TC1   TC2   TC3   TC4
SS loadings    3.829 3.786 2.334 2.198
Proportion Var 0.153 0.151 0.093 0.088
Cumulative Var 0.153 0.305 0.398 0.486
\end{verbatim}

The same four components/scales seemed have emerged, but they are in different order.

The oblique rotation allows us to see the correlation between the components/scales. This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/components are uncorrelated; hence in the analysis they were fixed to 0.0.

We can see that all the scales have almost no relation with each other. That is the the correlations range between -0.03 to 0.03. This is unusual and likely a biproduct of simulating data. It does, though, support the orthogonal rotation as the preferred one.

Of course there is always a little complexity. In oblique rotations, there is a distinction between the \emph{pattern} matrix (which reports component loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the \emph{structure} matrix (takes into account the relationship between the components/scales -- it is a product of the pattern matrix and the matrix containing the correlation coefficients between the components/scales). Most interpret the pattern matrix because it is simpler; however it could be that values in the pattern matrix are suppressed because of relations between the components. Therefore, the structure matrix can be a useful check and some editors will request it.

Obtaining the structure matrix requires two steps. First, multiply the factor loadings with the phi matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL}\OperatorTok{$}\NormalTok{loadings }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcaOBL}\OperatorTok{$}\NormalTok{Phi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                TC1          TC2         TC3          TC4
Obj1     0.69050128  0.092733819 -0.09342837 -0.096688897
Obj2     0.72526181 -0.078285639 -0.13389319 -0.007970087
Obj3     0.65222185  0.002163388 -0.02080291  0.169195306
Obj4     0.64980993  0.034592152  0.01021951 -0.003172452
Obj5     0.60724873 -0.195994047  0.14916017  0.040795794
Obj6     0.55920407  0.244637871 -0.09553755  0.084850366
Obj7     0.58233119  0.255604967  0.05567917 -0.156139162
Obj8     0.53550142 -0.026606024  0.15167765 -0.064536944
Obj9     0.45868906  0.055743636  0.22592591  0.104498059
Obj10    0.46065458  0.072992922  0.25626058 -0.099641255
Marg1    0.05429308  0.889387150 -0.01272292 -0.060136145
Marg2   -0.09540568  0.794155634  0.03439142  0.165860367
Marg3    0.02785178  0.728248568 -0.06944307  0.041476497
Marg4    0.15779653  0.693656404 -0.08609067  0.083060621
Marg5    0.21422529  0.654667296 -0.14130873  0.030721117
Marg6   -0.03873213  0.610566300  0.31476620 -0.237541460
Marg7   -0.25458376  0.564865999  0.22877602  0.242123251
Strong1 -0.02493831 -0.026459940  0.62139121  0.264583641
Strong2  0.03727021 -0.083266239  0.63714719 -0.002354881
Strong3  0.13898024 -0.091205429  0.59914813 -0.154079964
Strong4 -0.08957417  0.008131805  0.66462011  0.277765084
Strong5  0.02206318  0.175205559  0.55465506 -0.180203186
Angry1   0.21157554  0.012544720 -0.10378683  0.745823358
Angry2  -0.05377743  0.030179345  0.16057125  0.771738772
Angry3  -0.12591995  0.094122499  0.08785893  0.777830061
\end{verbatim}

Then use Field's \citeyearpar{field_discovering_2012} function to produce the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Field's function to produce the structure matrix}
\NormalTok{factor.structure <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fa, }\DataTypeTok{cut =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{decimals =} \DecValTok{2}\NormalTok{)\{}
\NormalTok{    structure.matrix <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.sort}\NormalTok{(fa}\OperatorTok{$}\NormalTok{loadings }\OperatorTok{%*%}\StringTok{ }\NormalTok{fa}\OperatorTok{$}\NormalTok{Phi)}
\NormalTok{    structure.matrix <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{abs}\NormalTok{(structure.matrix) }\OperatorTok{<}\StringTok{ }\NormalTok{cut, }\StringTok{""}\NormalTok{, }\KeywordTok{round}\NormalTok{(structure.matrix, decimals)))}
    \KeywordTok{return}\NormalTok{(structure.matrix)}
\NormalTok{    \}}
    
\KeywordTok{factor.structure}\NormalTok{(pcaOBL, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         TC1  TC2  TC3  TC4
Obj2    0.73               
Obj1    0.69               
Obj3    0.65               
Obj4    0.65               
Obj5    0.61               
Obj7    0.58               
Obj6    0.56               
Obj8    0.54               
Obj10   0.46               
Obj9    0.46               
Marg1        0.89          
Marg2        0.79          
Marg3        0.73          
Marg4        0.69          
Marg5        0.65          
Marg6        0.61 0.31     
Marg7        0.56          
Strong4           0.66     
Strong2           0.64     
Strong1           0.62     
Strong3            0.6     
Strong5           0.55     
Angry3                 0.78
Angry2                 0.77
Angry1                 0.75
\end{verbatim}

Although some of the relative values changed, our items were stable regarding their component membership.

\hypertarget{component-scores}{%
\subsection{Component Scores}\label{component-scores}}

Component \emph{scores} (PC scores) can be created for each case (row) on each component (column). These can be used to assess the relative standing of one person on the construct/variable to another. We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicolliearity.

Computation involves multiplying an individual's item-level responses by the component loadings we obtained through the PCA process. The results will be one score per component for each row/case.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaOBL <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{principal}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors=}\DecValTok{4}\NormalTok{, }\DataTypeTok{rotate=}\StringTok{"oblimin"}\NormalTok{, }\DataTypeTok{scores=}\OtherTok{TRUE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(pcaOBL}\OperatorTok{$}\NormalTok{scores, }\DecValTok{10}\NormalTok{) }\CommentTok{#shows us only the first 10 (of N = 2571)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               TC1        TC2         TC3         TC4
 [1,] -0.434736430  1.2702286  0.83668623 -0.01950134
 [2,] -0.358346641  0.6998443 -1.28684407 -0.10546432
 [3,] -1.245491638 -0.1908764 -1.42929276 -2.51420936
 [4,] -0.004981576 -0.1127983 -1.38956225  1.86355065
 [5,]  0.296789107  0.5137755  0.08833053 -2.09774692
 [6,]  0.684949888  1.6400602  0.44203340 -0.27258969
 [7,] -0.771083947 -1.1922357  1.09617794  0.36910174
 [8,] -1.284265060 -0.4144262  0.08708796 -1.07524322
 [9,]  0.666522142  0.2999858  1.81030433  1.46348684
[10,]  0.292763466 -0.3285169  0.30101493  0.23847094
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfGRMS <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(dfGRMS, pcaOBL}\OperatorTok{$}\NormalTok{scores) }\CommentTok{#adds them to our raw dataset}
\end{Highlighting}
\end{Shaded}

To bring this full circle, we can see the correlation of the component scores; the pattern maps onto what we saw previously.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS [}\KeywordTok{c}\NormalTok{(}\StringTok{"TC1"}\NormalTok{, }\StringTok{"TC4"}\NormalTok{, }\StringTok{"TC3"}\NormalTok{, }\StringTok{"TC2"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("TC1", "TC4", "TC3", "TC2")])
Correlation matrix 
      TC1   TC4   TC3  TC2
TC1  1.00 -0.02 -0.03 0.02
TC4 -0.02  1.00  0.03 0.03
TC3 -0.03  0.03  1.00 0.02
TC2  0.02  0.03  0.02 1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
     TC1  TC4  TC3 TC2
TC1 0.00 1.00 1.00   1
TC4 0.70 0.00 1.00   1
TC3 0.65 0.59 0.00   1
TC2 0.74 0.69 0.76   0

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.diagram}\NormalTok{ (pcaOBL, }\DataTypeTok{error=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{side=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/Oblique diagram-1.pdf}

\hypertarget{apa-style-results}{%
\section{APA Style Results}\label{apa-style-results}}

\textbf{Results}

The dimensionality of the 25 items from the Gendered Racial Microagressions Scale for Black Women was analyzed using principal components analysis. First, data were screened to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi ^{1}(300)=1683.76, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

Four criteria were used to determine the number of components to rotate: a priori theory, the scree test, the Eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested four components, and, in combination explained 49\% of the variance. The scree plot was showed an inflexion that would justified retaining four components. Based on the convergence of these decisions, four components were extracted. We investigated each with orthogonal (Varimax) and oblique (oblimin) procedures. Given the non-significant correlations (ranging from -0.03 to 0.03) and the clear component loadings in the orthogonal rotation, we determined that an orthogonal solution was most appropriate.

The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable components, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (15\%), silenced and marginalized (15\%), strong woman stereotype (9\%), and angry woman stereotype (15\%).

Regarding the Table 1, I would include a table with ALL the values, bolding those with component membership. This is easy, though, because it is how the table was exported when we wrote it to a .csv file.

\hypertarget{back-to-the-future-the-relationship-between-pca-and-item-analysis}{%
\section{Back to the futuRe: The relationship between PCA and Item Analysis}\label{back-to-the-future-the-relationship-between-pca-and-item-analysis}}

I included the lesson on item analysis because I find it to be a useful stepping stone into principal components and principal factor analysis. How do the results we obtained from PCA compare to those found in item analysis?

First, we score the total and subscales using the dataset we simulated above (dfGRMS).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{GRMSVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}
\NormalTok{ObjectifiedVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{)}
\NormalTok{MarginalizedVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{)}
\NormalTok{StrongVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{)}
\NormalTok{AngryVars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}

\NormalTok{dfGRMS}\OperatorTok{$}\NormalTok{GRMStot <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfGRMS[,GRMSVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present }
\NormalTok{dfGRMS}\OperatorTok{$}\NormalTok{Objectified <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfGRMS[,ObjectifiedVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present }
\NormalTok{dfGRMS}\OperatorTok{$}\NormalTok{Marginalized <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfGRMS[,MarginalizedVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present }
\NormalTok{dfGRMS}\OperatorTok{$}\NormalTok{Strong <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfGRMS[,StrongVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\NormalTok{dfGRMS}\OperatorTok{$}\NormalTok{Angry <-}\StringTok{ }\NormalTok{sjstats}\OperatorTok{::}\KeywordTok{mean_n}\NormalTok{(dfGRMS[,AngryVars], }\FloatTok{.80}\NormalTok{)}\CommentTok{#will create the mean for each individual if 80% of variables are present (in this case all variables must be present)}
\end{Highlighting}
\end{Shaded}

While we are at it, let's just create tiny dfs with just our variables of interest.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMStotal <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfGRMS, Obj1}\OperatorTok{:}\NormalTok{Angry3)}
\NormalTok{Objectification <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfGRMS, Obj1}\OperatorTok{:}\NormalTok{Obj10)}
\NormalTok{Marginalization <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfGRMS, Marg1}\OperatorTok{:}\NormalTok{Marg7)}
\NormalTok{Strong <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfGRMS, Strong1}\OperatorTok{:}\NormalTok{Strong5)}
\NormalTok{Angry <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dfGRMS, Angry1}\OperatorTok{:}\NormalTok{Angry3)}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-and-extracting-item-total-correlation-coefficients}{%
\subsection{Calculating and Extracting Item-Total Correlation Coefficients}\label{calculating-and-extracting-item-total-correlation-coefficients}}

\hypertarget{corrected-item-total-correlations-from-the-psychalpha}{%
\subsubsection{\texorpdfstring{Corrected item-total correlations from the \emph{psych::alpha()}}{Corrected item-total correlations from the psych::alpha()}}\label{corrected-item-total-correlations-from-the-psychalpha}}

Let's first ask, ``Is there support for this instrument as a unidimensional measure?'' To do that, we get an alpha for the whole scale score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSalpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(GRMStotal) }\CommentTok{#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{GRMSalpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = GRMStotal)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.75      0.75    0.81      0.11   3 0.022  2.1 0.42    0.071

 lower alpha upper     95% confidence boundaries
0.71 0.75 0.8 

 Reliability if an item is dropped:
        raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
Obj1         0.74      0.74    0.80      0.11 2.8    0.023 0.026 0.070
Obj2         0.74      0.74    0.80      0.11 2.9    0.023 0.025 0.073
Obj3         0.74      0.74    0.80      0.11 2.8    0.023 0.027 0.069
Obj4         0.74      0.74    0.80      0.11 2.8    0.023 0.027 0.070
Obj5         0.75      0.74    0.81      0.11 2.9    0.023 0.026 0.070
Obj6         0.74      0.74    0.80      0.10 2.8    0.024 0.027 0.067
Obj7         0.74      0.74    0.80      0.10 2.8    0.024 0.027 0.066
Obj8         0.74      0.74    0.81      0.11 2.9    0.023 0.027 0.070
Obj9         0.74      0.74    0.80      0.11 2.8    0.023 0.028 0.067
Obj10        0.74      0.74    0.81      0.11 2.9    0.023 0.028 0.066
Marg1        0.73      0.74    0.79      0.10 2.8    0.024 0.023 0.074
Marg2        0.74      0.74    0.80      0.11 2.8    0.024 0.024 0.072
Marg3        0.74      0.74    0.80      0.11 2.9    0.023 0.025 0.078
Marg4        0.74      0.74    0.80      0.10 2.8    0.024 0.026 0.071
Marg5        0.74      0.74    0.80      0.11 2.8    0.024 0.026 0.067
Marg6        0.74      0.74    0.80      0.11 2.9    0.023 0.026 0.070
Marg7        0.75      0.75    0.81      0.11 3.0    0.023 0.026 0.070
Strong1      0.75      0.75    0.81      0.11 3.0    0.022 0.028 0.071
Strong2      0.75      0.75    0.81      0.11 3.0    0.022 0.027 0.074
Strong3      0.75      0.75    0.81      0.11 3.0    0.022 0.027 0.072
Strong4      0.75      0.75    0.81      0.11 3.0    0.022 0.027 0.074
Strong5      0.75      0.75    0.81      0.11 3.0    0.023 0.028 0.072
Angry1       0.75      0.75    0.81      0.11 3.0    0.023 0.027 0.069
Angry2       0.75      0.75    0.81      0.11 3.0    0.022 0.027 0.079
Angry3       0.75      0.75    0.81      0.11 3.1    0.022 0.026 0.079

 Item statistics 
          n raw.r std.r r.cor r.drop mean   sd
Obj1    259  0.44  0.44  0.41   0.34  1.8 1.09
Obj2    259  0.38  0.38  0.35   0.28  1.9 1.14
Obj3    259  0.45  0.46  0.43   0.36  2.0 1.01
Obj4    259  0.42  0.44  0.41   0.35  1.9 0.89
Obj5    259  0.35  0.37  0.32   0.25  2.1 1.19
Obj6    259  0.49  0.48  0.44   0.39  1.8 1.29
Obj7    259  0.49  0.49  0.47   0.40  2.0 1.09
Obj8    259  0.37  0.39  0.34   0.28  2.2 1.00
Obj9    259  0.42  0.44  0.39   0.34  1.8 0.99
Obj10   259  0.40  0.41  0.37   0.31  1.9 1.05
Marg1   259  0.53  0.50  0.52   0.46  2.0 1.02
Marg2   259  0.46  0.43  0.43   0.38  3.5 0.99
Marg3   259  0.45  0.41  0.39   0.34  2.4 1.30
Marg4   259  0.50  0.47  0.45   0.41  3.3 1.17
Marg5   259  0.49  0.45  0.43   0.38  2.4 1.31
Marg6   259  0.40  0.38  0.35   0.28  2.9 1.37
Marg7   259  0.34  0.32  0.27   0.23  2.7 1.21
Strong1 259  0.26  0.29  0.23   0.18  1.3 0.88
Strong2 259  0.22  0.25  0.19   0.13  2.3 0.95
Strong3 259  0.21  0.26  0.20   0.13  1.5 0.83
Strong4 259  0.27  0.29  0.24   0.17  1.6 1.10
Strong5 259  0.27  0.31  0.24   0.19  1.4 0.83
Angry1  259  0.33  0.31  0.27   0.22  2.0 1.15
Angry2  259  0.27  0.26  0.22   0.16  2.5 1.20
Angry3  259  0.24  0.23  0.18   0.13  2.4 1.16

Non missing response frequency for each item
           0    1    2    3    4    5 miss
Obj1    0.12 0.30 0.30 0.22 0.05 0.00    0
Obj2    0.12 0.25 0.31 0.24 0.07 0.01    0
Obj3    0.06 0.28 0.36 0.24 0.07 0.00    0
Obj4    0.04 0.28 0.45 0.19 0.04 0.00    0
Obj5    0.12 0.19 0.35 0.22 0.12 0.01    0
Obj6    0.17 0.27 0.24 0.20 0.09 0.02    0
Obj7    0.09 0.24 0.37 0.22 0.06 0.01    0
Obj8    0.04 0.20 0.35 0.33 0.08 0.00    0
Obj9    0.10 0.26 0.41 0.19 0.04 0.00    0
Obj10   0.09 0.27 0.39 0.17 0.08 0.00    0
Marg1   0.06 0.24 0.41 0.22 0.08 0.00    0
Marg2   0.00 0.03 0.12 0.34 0.36 0.15    0
Marg3   0.07 0.17 0.27 0.25 0.18 0.05    0
Marg4   0.00 0.05 0.20 0.29 0.25 0.20    0
Marg5   0.08 0.17 0.31 0.24 0.14 0.07    0
Marg6   0.06 0.10 0.24 0.26 0.22 0.13    0
Marg7   0.03 0.12 0.27 0.35 0.14 0.08    0
Strong1 0.20 0.42 0.31 0.08 0.00 0.00    0
Strong2 0.03 0.18 0.35 0.37 0.07 0.00    0
Strong3 0.12 0.41 0.38 0.09 0.00 0.00    0
Strong4 0.18 0.29 0.32 0.16 0.04 0.00    0
Strong5 0.14 0.38 0.41 0.07 0.00 0.00    0
Angry1  0.09 0.25 0.31 0.26 0.08 0.02    0
Angry2  0.05 0.14 0.31 0.28 0.18 0.04    0
Angry3  0.06 0.15 0.33 0.28 0.15 0.02    0
\end{verbatim}

And now each of the subscales:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ObjAlpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(Objectification) }\CommentTok{#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{ObjAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Objectification)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
       0.8       0.8    0.79      0.29 4.1 0.018  1.9 0.65     0.28

 lower alpha upper     95% confidence boundaries
0.77 0.8 0.84 

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Obj1       0.77      0.78    0.76      0.28 3.5    0.021 0.0038  0.27
Obj2       0.77      0.77    0.76      0.28 3.4    0.021 0.0030  0.26
Obj3       0.78      0.78    0.77      0.28 3.6    0.020 0.0038  0.27
Obj4       0.78      0.78    0.77      0.28 3.6    0.020 0.0044  0.26
Obj5       0.78      0.79    0.77      0.29 3.7    0.020 0.0044  0.28
Obj6       0.79      0.79    0.78      0.29 3.8    0.019 0.0040  0.28
Obj7       0.78      0.79    0.77      0.29 3.7    0.020 0.0044  0.28
Obj8       0.79      0.79    0.78      0.30 3.8    0.019 0.0045  0.30
Obj9       0.79      0.80    0.78      0.30 3.9    0.019 0.0038  0.31
Obj10      0.79      0.80    0.78      0.30 3.9    0.019 0.0039  0.31

 Item statistics 
        n raw.r std.r r.cor r.drop mean   sd
Obj1  259  0.67  0.67  0.62   0.56  1.8 1.09
Obj2  259  0.69  0.69  0.65   0.58  1.9 1.14
Obj3  259  0.64  0.64  0.59   0.53  2.0 1.01
Obj4  259  0.62  0.64  0.59   0.53  1.9 0.89
Obj5  259  0.62  0.61  0.54   0.48  2.1 1.19
Obj6  259  0.60  0.57  0.50   0.44  1.8 1.29
Obj7  259  0.60  0.60  0.53   0.48  2.0 1.09
Obj8  259  0.55  0.56  0.48   0.43  2.2 1.00
Obj9  259  0.50  0.52  0.42   0.38  1.8 0.99
Obj10 259  0.51  0.52  0.42   0.37  1.9 1.05

Non missing response frequency for each item
         0    1    2    3    4    5 miss
Obj1  0.12 0.30 0.30 0.22 0.05 0.00    0
Obj2  0.12 0.25 0.31 0.24 0.07 0.01    0
Obj3  0.06 0.28 0.36 0.24 0.07 0.00    0
Obj4  0.04 0.28 0.45 0.19 0.04 0.00    0
Obj5  0.12 0.19 0.35 0.22 0.12 0.01    0
Obj6  0.17 0.27 0.24 0.20 0.09 0.02    0
Obj7  0.09 0.24 0.37 0.22 0.06 0.01    0
Obj8  0.04 0.20 0.35 0.33 0.08 0.00    0
Obj9  0.10 0.26 0.41 0.19 0.04 0.00    0
Obj10 0.09 0.27 0.39 0.17 0.08 0.00    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MargAlpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(Marginalization) }\CommentTok{#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{MargAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Marginalization)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.83      0.84    0.83      0.42 5.1 0.017  2.7 0.84     0.42

 lower alpha upper     95% confidence boundaries
0.79 0.83 0.86 

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Marg1      0.77      0.78    0.75      0.37 3.5    0.022 0.0069  0.40
Marg2      0.79      0.80    0.79      0.40 4.0    0.020 0.0148  0.40
Marg3      0.80      0.81    0.80      0.42 4.3    0.020 0.0162  0.42
Marg4      0.80      0.82    0.81      0.43 4.5    0.019 0.0153  0.42
Marg5      0.81      0.82    0.81      0.44 4.7    0.018 0.0147  0.42
Marg6      0.82      0.83    0.82      0.45 4.9    0.017 0.0135  0.42
Marg7      0.82      0.84    0.82      0.46 5.1    0.017 0.0137  0.44

 Item statistics 
        n raw.r std.r r.cor r.drop mean   sd
Marg1 259  0.86  0.87  0.88   0.81  2.0 1.02
Marg2 259  0.77  0.78  0.75   0.68  3.5 0.99
Marg3 259  0.73  0.73  0.67   0.60  2.4 1.30
Marg4 259  0.69  0.70  0.63   0.56  3.3 1.17
Marg5 259  0.67  0.66  0.58   0.52  2.4 1.31
Marg6 259  0.64  0.62  0.53   0.47  2.9 1.37
Marg7 259  0.60  0.60  0.49   0.45  2.7 1.21

Non missing response frequency for each item
         0    1    2    3    4    5 miss
Marg1 0.06 0.24 0.41 0.22 0.08 0.00    0
Marg2 0.00 0.03 0.12 0.34 0.36 0.15    0
Marg3 0.07 0.17 0.27 0.25 0.18 0.05    0
Marg4 0.00 0.05 0.20 0.29 0.25 0.20    0
Marg5 0.08 0.17 0.31 0.24 0.14 0.07    0
Marg6 0.06 0.10 0.24 0.26 0.22 0.13    0
Marg7 0.03 0.12 0.27 0.35 0.14 0.08    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{StrongAlpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(Strong) }\CommentTok{#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{StrongAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Strong)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.63      0.63    0.59      0.26 1.7 0.035  1.6 0.59     0.26

 lower alpha upper     95% confidence boundaries
0.56 0.63 0.7 

 Reliability if an item is dropped:
        raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r
Strong1      0.57      0.58    0.51      0.26 1.4    0.043 0.0014  0.26
Strong2      0.57      0.57    0.51      0.25 1.3    0.043 0.0027  0.25
Strong3      0.58      0.58    0.51      0.26 1.4    0.041 0.0036  0.24
Strong4      0.56      0.56    0.49      0.24 1.3    0.045 0.0010  0.25
Strong5      0.61      0.61    0.54      0.28 1.6    0.039 0.0016  0.27

 Item statistics 
          n raw.r std.r r.cor r.drop mean   sd
Strong1 259  0.64  0.64  0.50   0.40  1.3 0.88
Strong2 259  0.66  0.65  0.50   0.40  2.3 0.95
Strong3 259  0.61  0.64  0.49   0.38  1.5 0.83
Strong4 259  0.71  0.67  0.54   0.43  1.6 1.10
Strong5 259  0.57  0.59  0.41   0.33  1.4 0.83

Non missing response frequency for each item
           0    1    2    3    4 5 miss
Strong1 0.20 0.42 0.31 0.08 0.00 0    0
Strong2 0.03 0.18 0.35 0.37 0.07 0    0
Strong3 0.12 0.41 0.38 0.09 0.00 0    0
Strong4 0.18 0.29 0.32 0.16 0.04 0    0
Strong5 0.14 0.38 0.41 0.07 0.00 0    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AngryAlpha <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{alpha}\NormalTok{(Angry) }\CommentTok{#creating an object from this analysis so I can extract and manipulate the item statistics (specifically the r.drop)}
\NormalTok{AngryAlpha}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Reliability analysis   
Call: psych::alpha(x = Angry)

  raw_alpha std.alpha G6(smc) average_r S/N   ase mean   sd median_r
      0.71      0.71    0.62      0.45 2.4 0.031  2.3 0.93     0.44

 lower alpha upper     95% confidence boundaries
0.65 0.71 0.77 

 Reliability if an item is dropped:
       raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r
Angry1      0.63      0.64    0.47      0.47 1.7    0.045    NA  0.47
Angry2      0.60      0.60    0.43      0.43 1.5    0.049    NA  0.43
Angry3      0.61      0.61    0.44      0.44 1.6    0.048    NA  0.44

 Item statistics 
         n raw.r std.r r.cor r.drop mean  sd
Angry1 259  0.78  0.79  0.61   0.51  2.0 1.2
Angry2 259  0.81  0.80  0.64   0.54  2.5 1.2
Angry3 259  0.79  0.80  0.63   0.53  2.4 1.2

Non missing response frequency for each item
          0    1    2    3    4    5 miss
Angry1 0.09 0.25 0.31 0.26 0.08 0.02    0
Angry2 0.05 0.14 0.31 0.28 0.18 0.04    0
Angry3 0.06 0.15 0.33 0.28 0.15 0.02    0
\end{verbatim}

\hypertarget{correlating-items-with-other-subscale-totals}{%
\subsubsection{Correlating items with other subscale totals}\label{correlating-items-with-other-subscale-totals}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Obj_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS[}\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Marg_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS[}\KeywordTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Str_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS[}\KeywordTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ang_othR <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS[}\KeywordTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{,  }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\hypertarget{exctracting-values-binding-them-together-and-joining-the-files}{%
\subsubsection{Exctracting values, binding them together, and joining the files}\label{exctracting-values-binding-them-together-and-joining-the-files}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#names(Obj_other)}
\CommentTok{#Extracting the item-level statistics from the alpha object}
\NormalTok{Obj_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Obj_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#Makes the item-total(other) correlation matrix a df}
\CommentTok{#Adding variable names so we don't get lost}
\NormalTok{Obj_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{#deleting the rows with the total scale scores}
\NormalTok{Obj_othR <-}\StringTok{ }\NormalTok{Obj_othR[}\OperatorTok{!}\NormalTok{Obj_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Marginalized"}\NormalTok{,]}
\NormalTok{Obj_othR <-}\StringTok{ }\NormalTok{Obj_othR[}\OperatorTok{!}\NormalTok{Obj_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Strong"}\NormalTok{,]}
\NormalTok{Obj_othR <-}\StringTok{ }\NormalTok{Obj_othR[}\OperatorTok{!}\NormalTok{Obj_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Angry"}\NormalTok{,]}
\NormalTok{Obj_othR[ , }\StringTok{'Objectified'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA} \CommentTok{#We need a column for this to bind the items, later.}
\NormalTok{Obj_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Obj_othR, Items, Objectified, Marginalized, Strong, Angry) }\CommentTok{#Putting items in order}
\CommentTok{#Item Corrected Total Correlations }
\NormalTok{ObjAlpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(ObjAlpha}\OperatorTok{$}\NormalTok{item.stats)}\CommentTok{#Grabbing the alpha objet we created earlier and making it a df }
\NormalTok{ObjAlpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{)}
\CommentTok{#Joining the two and selecting the vars of interest}
\NormalTok{ObjStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(ObjAlpha, Obj_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{ObjStats}\OperatorTok{$}\NormalTok{Objectified <-}\StringTok{ }\NormalTok{ObjStats}\OperatorTok{$}\NormalTok{r.drop }\CommentTok{#Copy the item-corrected total (r.drop) into the Objectified variable}
\NormalTok{ObjStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(ObjStats, Items, Objectified, Marginalized, Strong, Angry)}
\KeywordTok{rm}\NormalTok{(ObjAlpha, Obj_othR) }\CommentTok{#It's messay, dropping all the no-longer-necessary objects from the Global Environment}


\CommentTok{#Extracting the item-level statistics from the alpha object}
\NormalTok{Marg_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Marg_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#Makes the item-total(other) correlation matrix a df}
\CommentTok{#Adding variable names so we don't get lost}
\NormalTok{Marg_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{#deleting the rows with the total scale scores}
\NormalTok{Marg_othR <-}\StringTok{ }\NormalTok{Marg_othR[}\OperatorTok{!}\NormalTok{Marg_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Objectified"}\NormalTok{,]}
\NormalTok{Marg_othR <-}\StringTok{ }\NormalTok{Marg_othR[}\OperatorTok{!}\NormalTok{Marg_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Strong"}\NormalTok{,]}
\NormalTok{Marg_othR <-}\StringTok{ }\NormalTok{Marg_othR[}\OperatorTok{!}\NormalTok{Marg_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Angry"}\NormalTok{,]}
\NormalTok{Marg_othR[ , }\StringTok{'Marginalized'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA} \CommentTok{#We need a column for this to bind the items, later.}
\NormalTok{Marg_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Marg_othR, Items, Objectified, Marginalized, Strong, Angry)}
\CommentTok{#Item Corrected Total Correlations}
\NormalTok{MargAlpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(MargAlpha}\OperatorTok{$}\NormalTok{item.stats)}\CommentTok{#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{MargAlpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{)}
\CommentTok{#Joining the two and selecting the vars of interest}
\NormalTok{MargStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(MargAlpha, Marg_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{MargStats}\OperatorTok{$}\NormalTok{Marginalized <-}\StringTok{ }\NormalTok{MargStats}\OperatorTok{$}\NormalTok{r.drop }\CommentTok{#Copy the item-corrected total (r.drop) into the Marginalized variable}
\NormalTok{MargStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(MargStats, Items, Objectified, Marginalized, Strong, Angry)}
\KeywordTok{rm}\NormalTok{(MargAlpha, Marg_othR) }\CommentTok{#It's messay, dropping all the no-longer-necessary objects from the Global Environment}

\NormalTok{Str_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Str_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#Makes the item-total(other) correlation matrix a df}
\CommentTok{#Adding variable names so we don't get lost}
\NormalTok{Str_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\CommentTok{#deleting the rows with the total scale scores}
\NormalTok{Str_othR <-}\StringTok{ }\NormalTok{Str_othR[}\OperatorTok{!}\NormalTok{Str_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Objectified"}\NormalTok{,]}
\NormalTok{Str_othR <-}\StringTok{ }\NormalTok{Str_othR[}\OperatorTok{!}\NormalTok{Str_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Marginalized"}\NormalTok{,]}
\NormalTok{Str_othR <-}\StringTok{ }\NormalTok{Str_othR[}\OperatorTok{!}\NormalTok{Str_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Angry"}\NormalTok{,]}
\NormalTok{Str_othR[ , }\StringTok{'Strong'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{Str_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Str_othR, Items, Objectified, Marginalized, Strong, Angry)}
\CommentTok{#Item Corrected Total Correlations}
\NormalTok{StrongAlpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(StrongAlpha}\OperatorTok{$}\NormalTok{item.stats) }\CommentTok{#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{StrongAlpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{)}
\CommentTok{#Joining the two and selecting the vars of interest}
\NormalTok{StrStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(StrongAlpha, Str_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{StrStats}\OperatorTok{$}\NormalTok{Strong <-}\StringTok{ }\NormalTok{StrStats}\OperatorTok{$}\NormalTok{r.drop }\CommentTok{#Copy the item-corrected total (r.drop) into the Strong variable}
\NormalTok{StrStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(StrStats, Items, Objectified, Marginalized, Strong, Angry)}
\KeywordTok{rm}\NormalTok{(StrongAlpha, Str_othR) }\CommentTok{#It's messay, dropping all the no-longer-necessary objects from the Global Environment}

\NormalTok{Ang_othR <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(Ang_othR}\OperatorTok{$}\NormalTok{r)}\CommentTok{#Makes the item-total(other) correlation matrix a df}
\CommentTok{#Adding variable names so we don't get lost}
\NormalTok{Ang_othR}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{, }\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{)}
\CommentTok{#deleting the rows with the total scale scores}
\NormalTok{Ang_othR <-}\StringTok{ }\NormalTok{Ang_othR[}\OperatorTok{!}\NormalTok{Ang_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Objectified"}\NormalTok{,]}
\NormalTok{Ang_othR <-}\StringTok{ }\NormalTok{Ang_othR[}\OperatorTok{!}\NormalTok{Ang_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Marginalized"}\NormalTok{,]}
\NormalTok{Ang_othR <-}\StringTok{ }\NormalTok{Ang_othR[}\OperatorTok{!}\NormalTok{Ang_othR}\OperatorTok{$}\NormalTok{Items }\OperatorTok{==}\StringTok{ "Strong"}\NormalTok{,]}
\NormalTok{Ang_othR[ , }\StringTok{'Angry'}\NormalTok{] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{Ang_othR <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(Ang_othR, Items, Objectified, Marginalized, Strong, Angry)}
\CommentTok{#Item Corrected Total Correlations}
\NormalTok{AngryAlpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(AngryAlpha}\OperatorTok{$}\NormalTok{item.stats) }\CommentTok{#Grabbing the alpha objet we created earlier and making it a df  }
\NormalTok{AngryAlpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}
\CommentTok{#Joining the two and selecting the vars of interest}
\NormalTok{AngStats <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(AngryAlpha, Ang_othR, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}
\NormalTok{AngStats}\OperatorTok{$}\NormalTok{Angry <-}\StringTok{ }\NormalTok{AngStats}\OperatorTok{$}\NormalTok{r.drop }\CommentTok{#Copy the item-corrected total (r.drop) into the Angry variable}
\NormalTok{AngStats <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(AngStats, Items, Objectified, Marginalized, Strong, Angry)}
\KeywordTok{rm}\NormalTok{(AngryAlpha, Ang_othR) }\CommentTok{#It's messay, dropping all the no-longer-necessary objects from the Global Environment}

\CommentTok{#Adding all the variables into a single table}
\NormalTok{ItemAnalysis <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(ObjStats, MargStats, StrStats, AngStats)}

\CommentTok{#Preparing and adding the r.drop for total scale score}
\NormalTok{TotAlpha <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(GRMSalpha}\OperatorTok{$}\NormalTok{item.stats)}
\NormalTok{TotAlpha}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{)}
\NormalTok{TotAlpha <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(TotAlpha, Items, r.drop) }\CommentTok{#deleting the rows with the total scale scores}


\CommentTok{#Adding the r.drop for the total scale score}
\NormalTok{ItemAnalysis <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(TotAlpha, ItemAnalysis, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}

\CommentTok{#Adding the values from the Othogonal rotation}
\NormalTok{pcaORTH_loadings <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{unclass}\NormalTok{(pcaORTH}\OperatorTok{$}\NormalTok{loadings)) }\CommentTok{#I had to add "unclass" to the loadings to render them into a df}
\NormalTok{pcaORTH_loadings}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#Item names for joining (and to make sure we know which variable is which)}
\CommentTok{#Deleting those lower rows}
\CommentTok{#pcaORTH_loadings <- pcaORTH_loadings[!pcaORTH_loadings$Items == "GRMSTot",]}
\CommentTok{#pcaORTH_loadings <- pcaORTH_loadings[!pcaORTH_loadings$Items == "Objectified",]}
\CommentTok{#pcaORTH_loadings <- pcaORTH_loadings[!pcaORTH_loadings$Items == "Marginalized",]}
\CommentTok{#pcaORTH_loadings <- pcaORTH_loadings[!pcaORTH_loadings$Items == "Strong",]}
\CommentTok{#pcaORTH_loadings <- pcaORTH_loadings[!pcaORTH_loadings$Items == "Angry",]}
\NormalTok{pcaORTH_loadings <-}\StringTok{ }\KeywordTok{rename}\NormalTok{(pcaORTH_loadings, }\DataTypeTok{objORTH =}\NormalTok{ RC1, }\DataTypeTok{margORTH =}\NormalTok{ RC2, }\DataTypeTok{strORTH =}\NormalTok{ RC3, }\DataTypeTok{angORTH2 =}\NormalTok{ RC4)}

\CommentTok{#Joining with the Item Stats}
\NormalTok{Comparisons <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(ItemAnalysis, pcaORTH_loadings, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}

\CommentTok{#Adding the oblique loadings}
\NormalTok{pcaOBLQ_loadings <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{unclass}\NormalTok{(pcaOBL}\OperatorTok{$}\NormalTok{loadings)) }\CommentTok{#I had to add "unclass" to the loadings to render them into a df}
\NormalTok{pcaOBLQ_loadings}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#Item names for joining (and to make sure we know which variable is which)}
\CommentTok{#Deleting those lower rows}
\CommentTok{#pcaOBLQ_loadings <- pcaOBLQ_loadings[!pcaORTH_loadings$Items == "GRMSTot",]}
\CommentTok{#pcaOBLQ_loadings <- pcaOBLQ_loadings[!pcaORTH_loadings$Items == "Objectified",]}
\CommentTok{#pcaOBLQ_loadings <- pcaOBLQ_loadings[!pcaORTH_loadings$Items == "Marginalized",]}
\CommentTok{#pcaOBLQ_loadings <- pcaOBLQ_loadings[!pcaORTH_loadings$Items == "Strong",]}
\CommentTok{#pcaOBLQ_loadings <- pcaOBLQ_loadings[!pcaORTH_loadings$Items == "Angry",]}
\NormalTok{pcaOBLQ_loadings <-}\StringTok{ }\KeywordTok{rename}\NormalTok{(pcaOBLQ_loadings, }\DataTypeTok{objOBLQ =}\NormalTok{ TC1, }\DataTypeTok{margOBLQ =}\NormalTok{ TC2, }\DataTypeTok{strOBLQ =}\NormalTok{ TC3, }\DataTypeTok{angOBLQ =}\NormalTok{ TC4)}

\CommentTok{#Joining with the Item Stats}
\NormalTok{Comparisons <-}\StringTok{ }\KeywordTok{full_join}\NormalTok{(Comparisons, pcaOBLQ_loadings, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}

\KeywordTok{write.csv}\NormalTok{(Comparisons, }\DataTypeTok{file =} \StringTok{"GRMS_Comparisons.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{)}\CommentTok{#Writes the table to a .csv file where you can open it with Excel and format}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in write.csv(Comparisons, file = "GRMS_Comparisons.csv", sep = ",", :
attempt to set 'col.names' ignored
\end{verbatim}

\begin{verbatim}
Warning in write.csv(Comparisons, file = "GRMS_Comparisons.csv", sep = ",", :
attempt to set 'sep' ignored
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{saveRDS}\NormalTok{(Comparisons, }\StringTok{"GRMS_Comparisons.rds"}\NormalTok{)}\CommentTok{#Writes the file as an .rds so that if anything is specially formatted, it is retained}
\end{Highlighting}
\end{Shaded}

\hypertarget{interpreting-the-result}{%
\subsubsection{Interpreting the result}\label{interpreting-the-result}}

The result of this work is a table that includes:

\begin{itemize}
\tightlist
\item
  \textbf{r.drop} Corrected item-total (entire GRMS) coefficients
\item
  \textbf{Item-total correlations} of the items correlated with their own subscale (bold; correlation does not include the item being correlated) and the other subscales
\item
  \textbf{PCA: Orthogonal rotation} factor loadings of the four-scales with a rotation that maximizes the independents (uncorrelatedness) of the scales
\item
  \textbf{PCA: Oblique rotation} factor loadings of the four-scales with a rotation that permits correlation between subscales
\end{itemize}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.25in,height=4.16667in]{images/PCA/ComparisonsTable.png}
\caption{Image of a table of values from the item analysis and PCA solutions with orthogonal and oblique rotations}\label{id}
}
\end{figure}

We are looking for

\begin{itemize}
\tightlist
\item
  items that \emph{load} higher on their own scales than they do on other scales
\item
  when they are ``close'' or have a number of strong loadings, it means that it's not going to discriminate well (think within-in scale discriminant validity).

  \begin{itemize}
  \tightlist
  \item
    if there are a number of these, there will likely be stronger correlations between subscales (indicating that the oblique rotation was an appropriate choice).
  \item
    with low/no cross-loadings, this supports the choices of an orthogonal (uncorrelated) solution
  \end{itemize}
\item
  when the item has a strong, positive loading on its own scale, it supports within-scale convergent validity.
\item
  similarities and differences across the item-analysis, PCA orthogonal, and PCA oblique solutions. Our biggest interest is in whether items change scale membership and/or have cross-loadings. This scale is performing extremely well with a great deal of stability

  \begin{itemize}
  \tightlist
  \item
    This, in part, is likely facilitated by the data simulation where we had the benefit of factors ``telling'' items where to load.
  \end{itemize}
\end{itemize}

\hypertarget{practice-problems-6}{%
\section{Practice Problems}\label{practice-problems-6}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The most complex is to use data of your own.In either case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of components to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of components).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-3}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-3}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If PCA is new to you, perhaps you just change the number in ``set.seed(210921)'' from 210921 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-2-conduct-a-pca-with-the-szymanski-and-bissonette--szymanski_perceptions_2020-research-vignette-that-was-used-in-prior-lessons.}{%
\subsection{\texorpdfstring{Problem \#2: Conduct a PCA with the Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} research vignette that was used in prior lessons.}{Problem \#2: Conduct a PCA with the Szymanski and Bissonette {[}-@szymanski\_perceptions\_2020{]} research vignette that was used in prior lessons.}}\label{problem-2-conduct-a-pca-with-the-szymanski-and-bissonette--szymanski_perceptions_2020-research-vignette-that-was-used-in-prior-lessons.}}

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's \citeyearpar{szymanski_perceptions_2020}Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PCA.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-3-try-something-entirely-new.-3}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-3}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete PCA. The data should allow for at least two (ideally three) components/subscales.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{PAF}{%
\chapter{EFA: Principal Axis Factoring}\label{PAF}}

\href{link}{Screencasted Lecture Link}

This is the second week of \emph{exploratory} principal components analysis (PCA) and factor analysis (EFA). This time the focus is on actual \emph{factor analysis}. There are numerous approaches. I will be demonstrating principal axis factoring (PAF).

\hypertarget{navigating-this-lesson-7}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-7}}

There is AMOUNT OF TIME of lecture. If you work through the materials with me it would be plan for an additional MORE TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-7}{%
\subsection{Learning Objectives}\label{learning-objectives-7}}

Focusing on this week's materials, make sure you can:

\begin{itemize}
\tightlist
\item
  Distinguish between PCA and EFA on several levels:

  \begin{itemize}
  \tightlist
  \item
    recognize PCA and EFA from a path diagram
  \item
    define keywords associated with each: factor loadings, linear components, describe v. explain.\\
  \end{itemize}
\item
  Recognize/define an identity matrix -- what test would you use to diagnose it?
\item
  Recognize/define multicollinearity and singularity -- what test would you use to diagnose it?
\item
  Describe the desired pattern of ``loadings'' (i.e., the relative weights of an item on its own scale compared to other scales)
\item
  Compare the results from item analysis, PCA, PAF, and omega.
\end{itemize}

\hypertarget{planning-for-practice-7}{%
\subsection{Planning for Practice}\label{planning-for-practice-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. Whichever you choose, it would be terrific if you used the same dataframe across as many psychometrics lessons as possible so you can compare the results.

The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's \citeyearpar{szymanski_perceptions_2020}Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PCA.

As a third option, you are welcome to use data to which you have access and is suitable for PCA. These could include other vignettes from this OER, other simualated data, or your own data (presuming you have permissoin to use it). In either case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of components to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of components).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data (especially the \protect\hyperlink{ItemAnalSurvey}{item analysis} and \protect\hyperlink{PCA}{PCA} lessons).
\end{itemize}

\hypertarget{readings-resources-7}{%
\subsection{Readings \& Resources}\label{readings-resources-7}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Revelle, William. (n.d.). Chapter 6: Constructs, components, and factor models. In \emph{An introduction to psychometric theory with applications in R}. Retrieved from \url{https://personality-project.org/r/book/\#chapter6}

  \begin{itemize}
  \tightlist
  \item
    pp.~150 to 167. Stop at ``Non-Simple Structure Solutions: The Simplex and Circumplex.''
  \item
    A simultaneously theoretical review of psychometric theory while working with R and data to understand the concepts.
  \end{itemize}
\item
  Revelle, W. (2019). \emph{How To: Use the psych package for Factor Analysis and data reduction}.

  \begin{itemize}
  \tightlist
  \item
    Treat as reference. Pages 13 through 24 provide technical information about what we are doing.
  \end{itemize}
\end{itemize}

\hypertarget{packages-7}{%
\subsection{Packages}\label{packages-7}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#will install the package if not already installed}
\CommentTok{#if(!require(psych))\{install.packages("psych")\}}
\CommentTok{#if(!require(tidyverse))\{install.packages("tidyverse")\}}
\CommentTok{#if(!require(MASS))\{install.packages("MASS")\}}
\CommentTok{#if(!require(sjstats))\{install.packages("sjstats")\}}
\CommentTok{#if(!require(apaTables))\{install.packages("apaTables")\}}
\CommentTok{#if(!require(qualtRics))\{install.packages("qualtRics")\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploratory-factor-analysis-with-a-quick-contrast-to-principal-components-analysis}{%
\section{Exploratory Factor Analysis (with a quick contrast to Principal Components Analysis)}\label{exploratory-factor-analysis-with-a-quick-contrast-to-principal-components-analysis}}

Whereas principal components analysis (PCA) is a regression analysis technique, principal factor analysis is ``\ldots a latent variable model'' \citep{revelle_william_chapter_nodate}.

Exploratory factor analysis has a rich history. In 1904, Spearman used it for a single factor. In 1947, Thurstone generalized it to multiple factors. Factor analysis is frequently used and controversial.

Factor analysis and principal components are commonly confused:

\textbf{Principal components}

\begin{itemize}
\tightlist
\item
  linear sums of variables,
\item
  solved with an eigenvalue or singular decomposition
\item
  represents a \(n*n\) matrix in terms of the first \emph{k} components and attempts to reproduce all of the \(R\) matrix.
\item
  paths point from the items to a total scale score -- all represented as observed/manifest (square) variables
\end{itemize}

\textbf{Factor analysis}

\begin{itemize}
\tightlist
\item
  linear sums of unknown factors
\item
  estimated as best fitting solutions, normally through iterative procedures.
\item
  Controversial because

  \begin{itemize}
  \tightlist
  \item
    at the \emph{structural} level (i.e., covariance or correlation matrix), there are normally more observed variables than parameters to estimate them and the procedure seeks to find the best fitting solution using ordinary least squares, weighted least squares, or maximum likelihood
  \item
    at the \emph{data} level, the model is indeterminate, although scores can be extimated
  \item
    this leads some to argue for using principal components; but fans of factor analysis suggest that it is useful for theory construction and evaluation
  \end{itemize}
\item
  attempts to model only the \emph{common} part of the matrix, which means all of the off-diagonal elements and the common part of the diagonal (the \emph{communalities}); the \emph{uniquenesses} are the non-common (leftover) part
\item
  Stated another way, the factor model partitions the correlation or covariance matrix into

  \begin{itemize}
  \tightlist
  \item
    \emph{common factors}, \(FF'\), and
  \item
    that which is \emph{unique}, \(U^2\) (the diagonal matrix of \emph{uniquenesses})
  \end{itemize}
\item
  paths point from latent variable representing the factor (oval) to the items (squares) illustrating that the factor/LV ``causes'' the item's score
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/PAF/PCAvPAF.png}
\caption{Comparison of path models for PCA and EFA}
\end{figure}

Our focus today is on the PAF approach to scale construction. By utilizing the same research vignette as in the \protect\hyperlink{PCA}{PCA lesson}, we can identify similarities in differences in the approach, results, and interpretation. Let's first take a look at the workflow for PAF.

\hypertarget{paf-workflow}{%
\section{PAF Workflow}\label{paf-workflow}}

Below is a screenshot of the workflow. The original document is located in the \href{https://github.com/lhbikos/ReC_Psychometrics}{Github site} that hosts the ReCentering Psych Stats: Psychometrics OER. You may find it refreshing that, with the exception of the change from ``components'' to ``factors,'' the workflow for PCA and PAF are quite similar.

\begin{figure}
\centering
\includegraphics{images/PAF/PAFworkflow.png}
\caption{Image of the workflow for PAF}
\end{figure}

Steps in the process include:

\begin{itemize}
\tightlist
\item
  Creating an items only dataframe where any items are scaled in the same direction (e.g., negatively worded items are reverse-scored).
\item
  Conducting tests that assess the statistical assumptions of PCA to ensure that the data is appropriate for PCA.
\item
  Determining the number of factors (think ``subscales'') to extract.
\item
  Conducting the factor extraction -- this process will likely occur iteratively,

  \begin{itemize}
  \tightlist
  \item
    exploring orthogonal (uncorrelated/independent) and oblique (correlated)factors, and
  \item
    changing the number of factors to extract
  \end{itemize}
\end{itemize}

Because the intended audience for the ReCentering Psych Stats OER is the scientist-practitioner-advocate, this lesson focuses on the workflow and decisions. As you might guess, the details of PCA can be quite complex. Some important notions to consider that may not be obvious from lesson, are these:

\begin{itemize}
\tightlist
\item
  The values of factor loadings are directly related to the correlation matrix.

  \begin{itemize}
  \tightlist
  \item
    Although I do not explain this in detail, nearly every analytic step attempts to convey this notion by presenting equivalent analytic options using the raw data and correlation matrix.
  \end{itemize}
\item
  PAF (like PCA and related EFA procecures) is about \emph{dimension reduction}; our goal is fewer factors (think subscales) than there are items.

  \begin{itemize}
  \tightlist
  \item
    In this lesson's vignette there are 25 items on the scale and we will have 4 subscales.
  \end{itemize}
\item
  As latent variable procedure,PAF is both \emph{exploratory} and \emph{factor analysis.} This is in contrast to our prior \protect\hyperlink{PCA}{PCA lesson}. Recall that PCA is a regression-based model and therefore not ``factor analysis.''
\item
  Matrix algebra (e.g., using the transpose of a matrix, multiplying matrices together) plays a critical role in the analytic solution.
\end{itemize}

\hypertarget{research-vignette-6}{%
\section{Research Vignette}\label{research-vignette-6}}

This lesson's research vignette emerges from Lewis and Neville's, Gendered Racial Microaggressions Scale for Black Women \citeyearpar{lewis_construction_2015}. The article reports on two separate studies that comprised the development, refinement, and psychometric evaluation of two, parallel, versions (stress appraisal, frequency) of scale. We simulate data from the final construction of the stress appraisal version as the basis of the lecture.

Lewis and Neville \citeyearpar{lewis_construction_2015} reported support for a total scale score (25 items) and four subscales. Below, I list the four subscales, their number of items, and a single example item. At the outset, let me provide a content warning. For those who hold this particular identity (or related identities) the content in the items may be upsetting. In other lessons, I often provide a variable name that gives an indication of the primary content of the item. In the case of the GRMS, I will simply provide an abbreviation of the subscale name and its respective item number. This will allow us to easily inspect the alignment of the item with its intended factor/subscale, and hopefully minimize discomfort. If you are not a member of this particular identity, I encourage you to learn about these microaggressions by reading the article in its entirety. Please do not ask members of this group to explain why these microaggressions are harmful or ask if they have encountered them. The four factors, number of items, and sample item are as follows:

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification

  \begin{itemize}
  \tightlist
  \item
    10 items
  \item
    ``Objectified me based on physical features.''
  \item
    Abbreviated in the simulated data as ``Obj\#''
  \end{itemize}
\item
  Silenced and Marginalized

  \begin{itemize}
  \tightlist
  \item
    7 items
  \item
    ``Someone has tried to `put me in my place.'\,''
  \item
    Abbreviated in the simulated data as ``Marg\#''
  \end{itemize}
\item
  Strong Black Woman Stereotype

  \begin{itemize}
  \tightlist
  \item
    5 items
  \item
    ``I have been told that I am too assertive.''
  \item
    Abbreviated in the simulated data as ``Strong\#''
  \end{itemize}
\item
  Angry Black Woman Stereotype

  \begin{itemize}
  \tightlist
  \item
    3 items
  \item
    ``Someone accused me of being angry when speaking calm.''
  \item
    Abbreviated in the simulated data as ``Angry\#''
  \end{itemize}
\end{itemize}

Below I walk through the data simulation. This is not an essential portion of the lesson, but I will lecture it in case you are interested. None of the items are negatively worded (relative to the other items), so there is no need to reverse-score any items. This is the same data as used in the \protect\hyperlink{PCA}{PCA lesson}. If you have already simulated and exported it, you only need to import it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{210921}\NormalTok{)}
\NormalTok{GRMSmat <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(.}\DecValTok{69}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.60}\NormalTok{, }\FloatTok{.59}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.41}\NormalTok{, }\FloatTok{.41}\NormalTok{, }\FloatTok{.04}\NormalTok{, }\FloatTok{-.15}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.12}\NormalTok{, }\FloatTok{.20}\NormalTok{, }\FloatTok{-.01}\NormalTok{, }\FloatTok{-.22}\NormalTok{, }\FloatTok{-.02}\NormalTok{, }\FloatTok{.02}\NormalTok{, }\FloatTok{.12}\NormalTok{, }\FloatTok{-.09}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.19}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{-.13}\NormalTok{,}
                  \FloatTok{.07}\NormalTok{, }\FloatTok{-.07}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.07}\NormalTok{, }\FloatTok{-.18}\NormalTok{, }\FloatTok{.22}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{-.01}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.02}\NormalTok{, }\FloatTok{.93}\NormalTok{, }\FloatTok{.81}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.67}\NormalTok{, }\FloatTok{.61}\NormalTok{, }\FloatTok{.58}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{-.07}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.19}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.04}\NormalTok{, }\FloatTok{.08}\NormalTok{,}
                  \FloatTok{-.08}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\DecValTok{00}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.08}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.22}\NormalTok{, }\FloatTok{.23}\NormalTok{, }\FloatTok{-.04}\NormalTok{, }\FloatTok{.01}\NormalTok{, }\FloatTok{-.05}\NormalTok{, }\FloatTok{-.11}\NormalTok{, }\FloatTok{-.16}\NormalTok{, }\FloatTok{.25}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.59}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.51}\NormalTok{, }\FloatTok{-.12}\NormalTok{, }\FloatTok{.08}\NormalTok{, }\FloatTok{.03}\NormalTok{,}
                  \FloatTok{-.06}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.16}\NormalTok{, }\FloatTok{.01}\NormalTok{, }\FloatTok{.05}\NormalTok{, }\FloatTok{.09}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.07}\NormalTok{, }\FloatTok{-.03}\NormalTok{, }\FloatTok{-.08}\NormalTok{, }\FloatTok{.18}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{.06}\NormalTok{, }\FloatTok{-.21}\NormalTok{, }\FloatTok{.21}\NormalTok{, }\FloatTok{.21}\NormalTok{, }\FloatTok{.03}\NormalTok{, }\FloatTok{-.06}\NormalTok{, }\FloatTok{.26}\NormalTok{, }\FloatTok{-.14}\NormalTok{, }\FloatTok{.70}\NormalTok{, }\FloatTok{.69}\NormalTok{, }\FloatTok{.68}\NormalTok{), }\DataTypeTok{ncol=}\DecValTok{4}\NormalTok{) }\CommentTok{#primary factor loadings for the four factors}
\KeywordTok{rownames}\NormalTok{(GRMSmat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{, }\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#variable names for the six items}
\CommentTok{#rownames(Szyf2) <- paste("V", seq(1:6), sep=" ") #prior code I replaced with above}
\KeywordTok{colnames}\NormalTok{(GRMSmat) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Objectified"}\NormalTok{, }\StringTok{"Marginalized"}\NormalTok{, }\StringTok{"Strong"}\NormalTok{, }\StringTok{"Angry"}\NormalTok{)}
\NormalTok{GRMSCorMat <-}\StringTok{ }\NormalTok{GRMSmat }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(GRMSmat) }\CommentTok{#create the correlation matrix}
\KeywordTok{diag}\NormalTok{(GRMSCorMat) <-}\StringTok{ }\DecValTok{1}
\CommentTok{#SzyCorMat #prints the correlation matrix}
\NormalTok{GRMS_M <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.78}\NormalTok{,   }\FloatTok{1.85}\NormalTok{,   }\FloatTok{1.97}\NormalTok{,   }\FloatTok{1.93}\NormalTok{,   }\FloatTok{2.01}\NormalTok{,   }\FloatTok{1.76}\NormalTok{,   }\FloatTok{1.91}\NormalTok{,   }\FloatTok{2.22}\NormalTok{,   }\FloatTok{1.83}\NormalTok{,   }\FloatTok{1.88}\NormalTok{, }\DecValTok{2}\NormalTok{,    }\FloatTok{3.5}\NormalTok{,    }\FloatTok{2.43}\NormalTok{,   }\FloatTok{3.44}\NormalTok{,   }\FloatTok{2.39}\NormalTok{,   }\FloatTok{2.89}\NormalTok{,   }\FloatTok{2.7}\NormalTok{, }\FloatTok{1.28}\NormalTok{,  }\FloatTok{2.25}\NormalTok{,   }\FloatTok{1.45}\NormalTok{,   }\FloatTok{1.57}\NormalTok{,   }\FloatTok{1.4}\NormalTok{, }\FloatTok{2.02}\NormalTok{,  }\FloatTok{2.53}\NormalTok{,   }\FloatTok{2.39}\NormalTok{) }\CommentTok{#item means; I made these up based on the M and SDs for the factors}
\NormalTok{GRMS_SD <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{1.11}\NormalTok{,  }\FloatTok{1.23}\NormalTok{,   }\FloatTok{0.97}\NormalTok{,   }\FloatTok{0.85}\NormalTok{,   }\FloatTok{1.19}\NormalTok{,   }\FloatTok{1.32}\NormalTok{,   }\FloatTok{1.04}\NormalTok{,   }\FloatTok{0.98}\NormalTok{,   }\FloatTok{1.01}\NormalTok{,   }\FloatTok{1.03}\NormalTok{, }\FloatTok{1.01}\NormalTok{, }\FloatTok{0.97}\NormalTok{,   }\FloatTok{1.32}\NormalTok{,   }\FloatTok{1.24}\NormalTok{,   }\FloatTok{1.31}\NormalTok{,   }\FloatTok{1.42}\NormalTok{,   }\FloatTok{1.2}\NormalTok{, }\FloatTok{0.85}\NormalTok{,  }\FloatTok{0.94}\NormalTok{,   }\FloatTok{0.78}\NormalTok{,   }\FloatTok{1.11}\NormalTok{,   }\FloatTok{0.84}\NormalTok{, }\FloatTok{1.14}\NormalTok{, }\FloatTok{1.2}\NormalTok{,    }\FloatTok{1.21}\NormalTok{) }\CommentTok{#item standard deviations; I made these up based on the M and SDs for the factors}
\NormalTok{GRMSCovMat <-}\StringTok{ }\NormalTok{GRMS_SD }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(GRMS_SD) }\OperatorTok{*}\StringTok{ }\NormalTok{GRMSCorMat }\CommentTok{#creates a covariance matrix from the correlation matrix}
\CommentTok{#SzyCovMat #displays the covariance matrix}

\NormalTok{dfGRMS <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(MASS}\OperatorTok{::}\KeywordTok{mvrnorm}\NormalTok{(}\DataTypeTok{n=}\DecValTok{259}\NormalTok{, }\DataTypeTok{mu =}\NormalTok{ GRMS_M, }\DataTypeTok{Sigma =}\NormalTok{ GRMSCovMat, }\DataTypeTok{empirical =} \OtherTok{TRUE}\NormalTok{),}\DecValTok{0}\NormalTok{)) }\CommentTok{#creates the item level data from the sample size, mean, and covariance matrix}
\NormalTok{dfGRMS[dfGRMS}\OperatorTok{>}\DecValTok{5}\NormalTok{]<-}\DecValTok{5} \CommentTok{#restricts the upperbound of all variables to be 5 or less}
\NormalTok{dfGRMS[dfGRMS}\OperatorTok{<}\DecValTok{0}\NormalTok{]<-}\DecValTok{0} \CommentTok{#resticts the lowerbound of all variable to be 0 or greater}
\CommentTok{#colMeans(GRMS) #displays column means}

\CommentTok{#Below is code if you would like and ID number. For this lesson's purposes and ID number would just need to be removed, so I will not include it in the original simulation.}
\CommentTok{#library(tidyverse)}
\CommentTok{#dfGRMS <- dfGRMS %>% dplyr::mutate(ID = row_number()) #add ID to each row}
\CommentTok{#dfGRMS <- dfGRMS%>%dplyr::select(ID, everything())#moving the ID number to the first column; requires}
\end{Highlighting}
\end{Shaded}

Let's take a quick peek at the data to see if everthing looks right.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        vars   n mean   sd median trimmed  mad min max range  skew kurtosis
Obj1       1 259 1.78 1.09      2    1.78 1.48   0   5     5  0.16    -0.63
Obj2       2 259 1.90 1.14      2    1.90 1.48   0   5     5  0.10    -0.61
Obj3       3 259 1.97 1.01      2    1.96 1.48   0   4     4  0.10    -0.56
Obj4       4 259 1.92 0.89      2    1.90 1.48   0   4     4  0.18    -0.17
Obj5       5 259 2.05 1.19      2    2.05 1.48   0   5     5  0.00    -0.63
Obj6       6 259 1.81 1.29      2    1.75 1.48   0   5     5  0.32    -0.70
Obj7       7 259 1.95 1.09      2    1.95 1.48   0   5     5  0.16    -0.22
Obj8       8 259 2.21 1.00      2    2.21 1.48   0   5     5 -0.13    -0.39
Obj9       9 259 1.81 0.99      2    1.84 1.48   0   4     4  0.02    -0.40
Obj10     10 259 1.88 1.05      2    1.87 1.48   0   4     4  0.18    -0.43
Marg1     11 259 2.02 1.02      2    2.00 1.48   0   5     5  0.13    -0.28
Marg2     12 259 3.47 0.99      4    3.50 1.48   1   5     4 -0.31    -0.33
Marg3     13 259 2.44 1.30      2    2.45 1.48   0   5     5 -0.01    -0.71
Marg4     14 259 3.35 1.17      3    3.38 1.48   1   5     4 -0.14    -0.92
Marg5     15 259 2.40 1.31      2    2.39 1.48   0   5     5  0.11    -0.58
Marg6     16 259 2.85 1.37      3    2.89 1.48   0   5     5 -0.22    -0.67
Marg7     17 259 2.68 1.21      3    2.66 1.48   0   5     5  0.02    -0.32
Strong1   18 259 1.27 0.88      1    1.23 1.48   0   4     4  0.26    -0.49
Strong2   19 259 2.29 0.95      2    2.30 1.48   0   5     5 -0.17    -0.31
Strong3   20 259 1.45 0.83      1    1.44 1.48   0   4     4  0.09    -0.37
Strong4   21 259 1.60 1.10      2    1.57 1.48   0   5     5  0.27    -0.49
Strong5   22 259 1.41 0.83      1    1.41 1.48   0   4     4 -0.01    -0.40
Angry1    23 259 2.03 1.15      2    2.01 1.48   0   5     5  0.13    -0.48
Angry2    24 259 2.53 1.20      3    2.54 1.48   0   5     5 -0.04    -0.49
Angry3    25 259 2.39 1.16      2    2.41 1.48   0   5     5 -0.07    -0.45
          se
Obj1    0.07
Obj2    0.07
Obj3    0.06
Obj4    0.06
Obj5    0.07
Obj6    0.08
Obj7    0.07
Obj8    0.06
Obj9    0.06
Obj10   0.07
Marg1   0.06
Marg2   0.06
Marg3   0.08
Marg4   0.07
Marg5   0.08
Marg6   0.09
Marg7   0.08
Strong1 0.05
Strong2 0.06
Strong3 0.05
Strong4 0.07
Strong5 0.05
Angry1  0.07
Angry2  0.07
Angry3  0.07
\end{verbatim}

The optional script below will let you save the simulated data to your computing environment as either a .csv file (think ``Excel lite'') or .rds object (preserves any formatting you might do).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#write the simulated data  as a .csv}
\CommentTok{#write.table(dfGRMS, file="dfGRMS.csv", sep=",", col.names=TRUE, row.names=FALSE)}
\CommentTok{#bring back the simulated dat from a .csv file}
\CommentTok{#dfGRMS <- read.csv ("dfGRMS.csv", header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with}
\CommentTok{#saveRDS(dfGRMS, "dfGRMS.rds")}
\CommentTok{#bring back the simulated dat from an .rds file}
\CommentTok{#dfGRMS <- readRDS("dfGRMS.rds")}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-vignette-1}{%
\section{Working the Vignette}\label{working-the-vignette-1}}

It may be useful to recall how we might understand factors in the psychometric sense:

\begin{itemize}
\tightlist
\item
  clusters of correlated items in an \(R\)-matrix
\item
  statistical entities that can be plotted as classification axes where coordinates of variables along each axis represen the strength of the relationship between that variable to each factor.
\item
  mathematical equations, resembling regression equations, where each variable is represented according to its relative weight
\end{itemize}

\hypertarget{data-prep-1}{%
\subsection{Data Prep}\label{data-prep-1}}

Since the first step is data preparation we start each analysis let's start by:

\begin{itemize}
\tightlist
\item
  reverse coding any items that are phrased in the opposite direction
\item
  creating a \emph{df} (as an object) that only contains the items in their properly scored direction (i.e., you might need to replace the original item with the reverse-coded item); there shoud be no other variables (e.g., ID, demographic variables, other scales) in this df

  \begin{itemize}
  \tightlist
  \item
    because the GRMS has no items like this we can skip these two steps
  \end{itemize}
\end{itemize}

Our example today requries no reverse coding and the dataset I simulated has only item-level data (with no ID and no other variables). This means we are ready to start the PAF process.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{describe}\NormalTok{(dfGRMS) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        vars   n mean   sd median trimmed  mad min max range  skew kurtosis
Obj1       1 259 1.78 1.09      2    1.78 1.48   0   5     5  0.16    -0.63
Obj2       2 259 1.90 1.14      2    1.90 1.48   0   5     5  0.10    -0.61
Obj3       3 259 1.97 1.01      2    1.96 1.48   0   4     4  0.10    -0.56
Obj4       4 259 1.92 0.89      2    1.90 1.48   0   4     4  0.18    -0.17
Obj5       5 259 2.05 1.19      2    2.05 1.48   0   5     5  0.00    -0.63
Obj6       6 259 1.81 1.29      2    1.75 1.48   0   5     5  0.32    -0.70
Obj7       7 259 1.95 1.09      2    1.95 1.48   0   5     5  0.16    -0.22
Obj8       8 259 2.21 1.00      2    2.21 1.48   0   5     5 -0.13    -0.39
Obj9       9 259 1.81 0.99      2    1.84 1.48   0   4     4  0.02    -0.40
Obj10     10 259 1.88 1.05      2    1.87 1.48   0   4     4  0.18    -0.43
Marg1     11 259 2.02 1.02      2    2.00 1.48   0   5     5  0.13    -0.28
Marg2     12 259 3.47 0.99      4    3.50 1.48   1   5     4 -0.31    -0.33
Marg3     13 259 2.44 1.30      2    2.45 1.48   0   5     5 -0.01    -0.71
Marg4     14 259 3.35 1.17      3    3.38 1.48   1   5     4 -0.14    -0.92
Marg5     15 259 2.40 1.31      2    2.39 1.48   0   5     5  0.11    -0.58
Marg6     16 259 2.85 1.37      3    2.89 1.48   0   5     5 -0.22    -0.67
Marg7     17 259 2.68 1.21      3    2.66 1.48   0   5     5  0.02    -0.32
Strong1   18 259 1.27 0.88      1    1.23 1.48   0   4     4  0.26    -0.49
Strong2   19 259 2.29 0.95      2    2.30 1.48   0   5     5 -0.17    -0.31
Strong3   20 259 1.45 0.83      1    1.44 1.48   0   4     4  0.09    -0.37
Strong4   21 259 1.60 1.10      2    1.57 1.48   0   5     5  0.27    -0.49
Strong5   22 259 1.41 0.83      1    1.41 1.48   0   4     4 -0.01    -0.40
Angry1    23 259 2.03 1.15      2    2.01 1.48   0   5     5  0.13    -0.48
Angry2    24 259 2.53 1.20      3    2.54 1.48   0   5     5 -0.04    -0.49
Angry3    25 259 2.39 1.16      2    2.41 1.48   0   5     5 -0.07    -0.45
          se
Obj1    0.07
Obj2    0.07
Obj3    0.06
Obj4    0.06
Obj5    0.07
Obj6    0.08
Obj7    0.07
Obj8    0.06
Obj9    0.06
Obj10   0.07
Marg1   0.06
Marg2   0.06
Marg3   0.08
Marg4   0.07
Marg5   0.08
Marg6   0.09
Marg7   0.08
Strong1 0.05
Strong2 0.06
Strong3 0.05
Strong4 0.07
Strong5 0.05
Angry1  0.07
Angry2  0.07
Angry3  0.07
\end{verbatim}

Let's take a look at (and make an object of) the correlation matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMSr <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(dfGRMS) }\CommentTok{#correlation matrix (with the negatively scored item already reversed) created and saved as object}
\KeywordTok{round}\NormalTok{(GRMSr, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Obj1  Obj2  Obj3  Obj4  Obj5  Obj6  Obj7  Obj8  Obj9 Obj10 Marg1 Marg2
Obj1     1.00  0.41  0.38  0.33  0.34  0.34  0.37  0.35  0.25  0.24  0.11  0.00
Obj2     0.41  1.00  0.43  0.40  0.40  0.34  0.35  0.29  0.28  0.24 -0.02 -0.13
Obj3     0.38  0.43  1.00  0.36  0.34  0.31  0.30  0.28  0.22  0.24  0.01 -0.03
Obj4     0.33  0.40  0.36  1.00  0.32  0.32  0.32  0.25  0.27  0.28  0.06 -0.03
Obj5     0.34  0.40  0.34  0.32  1.00  0.23  0.26  0.26  0.27  0.25 -0.14 -0.18
Obj6     0.34  0.34  0.31  0.32  0.23  1.00  0.31  0.24  0.19  0.16  0.20  0.16
Obj7     0.37  0.35  0.30  0.32  0.26  0.31  1.00  0.26  0.19  0.26  0.23  0.11
Obj8     0.35  0.29  0.28  0.25  0.26  0.24  0.26  1.00  0.21  0.22  0.00 -0.09
Obj9     0.25  0.28  0.22  0.27  0.27  0.19  0.19  0.21  1.00  0.22  0.07  0.01
Obj10    0.24  0.24  0.24  0.28  0.25  0.16  0.26  0.22  0.22  1.00  0.06 -0.03
Marg1    0.11 -0.02  0.01  0.06 -0.14  0.20  0.23  0.00  0.07  0.06  1.00  0.66
Marg2    0.00 -0.13 -0.03 -0.03 -0.18  0.16  0.11 -0.09  0.01 -0.03  0.66  1.00
Marg3    0.05 -0.01  0.02  0.04 -0.11  0.16  0.12 -0.03  0.07  0.02  0.62  0.50
Marg4    0.17  0.05  0.10  0.07 -0.03  0.21  0.21  0.05  0.05  0.07  0.58  0.47
Marg5    0.20  0.07  0.11  0.13  0.00  0.21  0.22  0.04  0.10  0.12  0.55  0.41
Marg6    0.00 -0.09 -0.06  0.03 -0.11  0.06  0.16  0.03  0.05  0.13  0.51  0.43
Marg7   -0.14 -0.22 -0.09 -0.13 -0.17  0.00 -0.01 -0.10 -0.04 -0.04  0.42  0.42
Strong1 -0.05 -0.07  0.03 -0.02  0.11 -0.05  0.01  0.03  0.14  0.10 -0.04  0.05
Strong2  0.01 -0.06  0.01  0.04  0.13 -0.01  0.04  0.13  0.12  0.13 -0.10 -0.06
Strong3  0.04  0.05  0.07  0.14  0.15  0.02  0.10  0.10  0.13  0.14 -0.06 -0.09
Strong4 -0.11 -0.11  0.01 -0.04  0.06 -0.04 -0.05  0.05  0.11  0.09 -0.04  0.05
Strong5  0.02  0.00 -0.01  0.01  0.04  0.00  0.10  0.11  0.05  0.11  0.14  0.10
Angry1   0.11  0.15  0.20  0.15  0.11  0.15  0.01  0.08  0.08  0.03 -0.04  0.06
Angry2  -0.12 -0.04  0.08 -0.02  0.01  0.01 -0.07 -0.01  0.07 -0.05 -0.03  0.11
Angry3  -0.12 -0.09  0.03 -0.06 -0.05  0.02 -0.12 -0.11  0.05 -0.07  0.02  0.21
        Marg3 Marg4 Marg5 Marg6 Marg7 Strong1 Strong2 Strong3 Strong4 Strong5
Obj1     0.05  0.17  0.20  0.00 -0.14   -0.05    0.01    0.04   -0.11    0.02
Obj2    -0.01  0.05  0.07 -0.09 -0.22   -0.07   -0.06    0.05   -0.11    0.00
Obj3     0.02  0.10  0.11 -0.06 -0.09    0.03    0.01    0.07    0.01   -0.01
Obj4     0.04  0.07  0.13  0.03 -0.13   -0.02    0.04    0.14   -0.04    0.01
Obj5    -0.11 -0.03  0.00 -0.11 -0.17    0.11    0.13    0.15    0.06    0.04
Obj6     0.16  0.21  0.21  0.06  0.00   -0.05   -0.01    0.02   -0.04    0.00
Obj7     0.12  0.21  0.22  0.16 -0.01    0.01    0.04    0.10   -0.05    0.10
Obj8    -0.03  0.05  0.04  0.03 -0.10    0.03    0.13    0.10    0.05    0.11
Obj9     0.07  0.05  0.10  0.05 -0.04    0.14    0.12    0.13    0.11    0.05
Obj10    0.02  0.07  0.12  0.13 -0.04    0.10    0.13    0.14    0.09    0.11
Marg1    0.62  0.58  0.55  0.51  0.42   -0.04   -0.10   -0.06   -0.04    0.14
Marg2    0.50  0.47  0.41  0.43  0.42    0.05   -0.06   -0.09    0.05    0.10
Marg3    1.00  0.42  0.40  0.35  0.34   -0.06   -0.09   -0.09   -0.02    0.07
Marg4    0.42  1.00  0.44  0.28  0.27   -0.03   -0.08   -0.08   -0.04    0.06
Marg5    0.40  0.44  1.00  0.25  0.26   -0.10   -0.08   -0.10   -0.06    0.01
Marg6    0.35  0.28  0.25  1.00  0.29    0.10    0.09    0.10    0.11    0.23
Marg7    0.34  0.27  0.26  0.29  1.00    0.09    0.08   -0.01    0.19    0.13
Strong1 -0.06 -0.03 -0.10  0.10  0.09    1.00    0.26    0.24    0.34    0.19
Strong2 -0.09 -0.08 -0.08  0.09  0.08    0.26    1.00    0.27    0.31    0.21
Strong3 -0.09 -0.08 -0.10  0.10 -0.01    0.24    0.27    1.00    0.25    0.27
Strong4 -0.02 -0.04 -0.06  0.11  0.19    0.34    0.31    0.25    1.00    0.22
Strong5  0.07  0.06  0.01  0.23  0.13    0.19    0.21    0.27    0.22    1.00
Angry1   0.05  0.08  0.07 -0.16  0.08    0.09   -0.05   -0.10    0.06   -0.11
Angry2   0.02  0.06 -0.01 -0.08  0.19    0.22    0.05   -0.02    0.24   -0.02
Angry3   0.05  0.07  0.04 -0.09  0.20    0.18    0.03   -0.07    0.20   -0.05
        Angry1 Angry2 Angry3
Obj1      0.11  -0.12  -0.12
Obj2      0.15  -0.04  -0.09
Obj3      0.20   0.08   0.03
Obj4      0.15  -0.02  -0.06
Obj5      0.11   0.01  -0.05
Obj6      0.15   0.01   0.02
Obj7      0.01  -0.07  -0.12
Obj8      0.08  -0.01  -0.11
Obj9      0.08   0.07   0.05
Obj10     0.03  -0.05  -0.07
Marg1    -0.04  -0.03   0.02
Marg2     0.06   0.11   0.21
Marg3     0.05   0.02   0.05
Marg4     0.08   0.06   0.07
Marg5     0.07  -0.01   0.04
Marg6    -0.16  -0.08  -0.09
Marg7     0.08   0.19   0.20
Strong1   0.09   0.22   0.18
Strong2  -0.05   0.05   0.03
Strong3  -0.10  -0.02  -0.07
Strong4   0.06   0.24   0.20
Strong5  -0.11  -0.02  -0.05
Angry1    1.00   0.44   0.43
Angry2    0.44   1.00   0.47
Angry3    0.43   0.47   1.00
\end{verbatim}

In case you want to examine it in sections (easier to view):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#round(GRMSr[,1:8], 2)}
\CommentTok{#round(GRMSr[,9:16], 2)}
\CommentTok{#round(GRMSr[,17:25], 2)}
\end{Highlighting}
\end{Shaded}

As with PCA, we can analyze the data with either raw data or correlation matrix. I will do both to demonstrate (a) that it's possible and to (b) continue emphasizing that this is a \emph{structural} analysis. That is, we are trying to see if our more parsimonious extraction \emph{reproduces} this original correlation matrix.

\hypertarget{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis-1}{%
\subsubsection{Three diagnostic tests to evaluate the appropriateness of the data for component-or-factor analysis}\label{three-diagnostic-tests-to-evaluate-the-appropriateness-of-the-data-for-component-or-factor-analysis-1}}

\hypertarget{is-my-sample-adequate-for-paf}{%
\subsubsection{Is my sample adequate for PAF?}\label{is-my-sample-adequate-for-paf}}

We return to the \textbf{KMO} (Kaiser-Meyer-Olkin), an index of \emph{sampling adequacy} that can be used with the actual sample to let us know if the sample size is sufficient (or if we should collect more data).

Kaiser's 1974 recommendations were:

\begin{itemize}
\tightlist
\item
  bare minimum of .5
\item
  values between .5 and .7 as mediocre
\item
  values between .7 and .8 as good
\item
  values above .9 are superb
\end{itemize}

We use the \emph{KMO()} function from the \emph{psych} package with either raw or matrix dat.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{KMO}\NormalTok{(dfGRMS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Kaiser-Meyer-Olkin factor adequacy
Call: psych::KMO(r = dfGRMS)
Overall MSA =  0.86
MSA for each item = 
   Obj1    Obj2    Obj3    Obj4    Obj5    Obj6    Obj7    Obj8    Obj9   Obj10 
   0.89    0.88    0.90    0.90    0.91    0.92    0.92    0.90    0.88    0.90 
  Marg1   Marg2   Marg3   Marg4   Marg5   Marg6   Marg7 Strong1 Strong2 Strong3 
   0.83    0.88    0.91    0.91    0.91    0.87    0.90    0.79    0.79    0.80 
Strong4 Strong5  Angry1  Angry2  Angry3 
   0.79    0.81    0.72    0.75    0.75 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#psych::KMO(GRMSr) #for the KMO function, do not specify sample size if using the matrix form of the data}
\end{Highlighting}
\end{Shaded}

We examine the KMO values for both the overall matrix and the individual items.

At the matrix level, our \(KMO = .86\), which falls into Kaiser's definition of \emph{superb}.

At the item level, the KMO should be \textgreater{} .50. Variables with values below .5 should be evaluated for exclusion from the analysis (or run the analysis with and without the variable and compare the difference). Because removing/adding variables impacts the KMO, be sure to re-evaluate.

At the item level, our KMO values range between .72 (Angry1) and .92 (Obj6, Obj7).

Considering both item- and matrix- levels, we conclude that the sample size and the data are adequate for component-or-factor analysis.

\hypertarget{are-there-correlations-among-the-variables-that-are-big-enough-to-be-analyzed}{%
\subsubsection{Are there correlations among the variables that are big enough to be analyzed?}\label{are-there-correlations-among-the-variables-that-are-big-enough-to-be-analyzed}}

\textbf{Bartlett's} lets us know if a matrix is an \emph{identity matrix.} In an identity matrix, then all correlation coefficients (everything on the off-diagonal) would be 0.0 (and everything on the diagonal would be 1.0).

A significant Barlett's (i.e., \(p < .05\)) tells that the \(R\)-matrix is not an identity matrix. That is, there are some relationships between variables that can be analyzed.

The \emph{cortest.bartlett()} function in the \emph{psych} package and can be run either from the raw data or R matrix formats.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{cortest.bartlett}\NormalTok{(dfGRMS) }\CommentTok{#from the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
R was not square, finding R from data
\end{verbatim}

\begin{verbatim}
$chisq
[1] 1683.76

$p.value
[1] 0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000005520916

$df
[1] 300
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#raw data produces the warning "R was not square, finding R from data." This means nothing other than we fed it raw data and the function is creating a matrix from which to do the analysis.}

\CommentTok{#psych::cortest.bartlett(GRMSr, n = 259) #if using the matrix, must specify sample size}
\end{Highlighting}
\end{Shaded}

Our Bartlett's test is significant: \(\chi ^{1}(300)=1683.76, p < .001\). This supports a component-or-factor analytic approach for investigating the data.

\hypertarget{is-there-multicollinearity-or-singularity-in-my-data-1}{%
\subsubsection{Is there multicollinearity or singularity in my data?}\label{is-there-multicollinearity-or-singularity-in-my-data-1}}

The \textbf{determinant of the correlation matrix} should be greater than 0.00001 (that would be 4 zeros, then the 1). If it is smaller than 0.00001 then we may have an issue with \emph{multicollinearity} (i.e., variables that are too highly correlated) or \emph{singularity} (variables that are perfectly correlated).

The determinant function comes from base R. It is easiest to compute when the correlation matrix is the object. However, it is also possible to specify the command to work with the raw data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#det(GRMSr) }
\KeywordTok{det}\NormalTok{(}\KeywordTok{cor}\NormalTok{(dfGRMS))}\CommentTok{#if using the raw data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.001151581
\end{verbatim}

With a value of 0.00115, our determinant is greater than the 0.00001 requirement. If it were not, then we could identify problematic variables (i.e., those correlating too highly with others; those not correlating sufficiently with others) and re-run the diagnostic statitics.

\textbf{Summary:} Data screening were conducted to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi ^{1}(300)=1683.76, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

\emph{Note}: If this looks familiar. It is! The same diagnostics are used in PAF and \protect\hyperlink{PCA}{PCA}.

\hypertarget{principal-axis-factoring-paf}{%
\subsection{Principal Axis Factoring (PAF)}\label{principal-axis-factoring-paf}}

We can use the \emph{fa()} function, specifying \emph{fm = ``pa''} from the \emph{psych} package with raw or matrix data.

One difference from PCA is that factor analysis will not (cannot) calculate as many factors as there are items. This means that we should select a reasonable number like 20 (since there are 25 items). However, I received a number of errors/warnings and 10 is the first number that would run. I also received the warning, ``maximum iteration exceeded.'' Therefore I increased ``max.iter'' to 100.

Our goal is to begin to get an idea of the cumulative variance explained and number of factors to extract, so we really only need to identify more than factors we expect to extract.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#grmsPAF1 <- psych::fa(GRMSr, nfactors=10, fm = "pa", max.iter = 100, rotate="none")# using the matrix data and specifying the # of factors.}

\NormalTok{grmsPAF1 <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{10}\NormalTok{, }\DataTypeTok{fm =} \StringTok{"pa"}\NormalTok{, }\DataTypeTok{max.iter =} \DecValTok{100}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"none"}\NormalTok{)}\CommentTok{# using raw data and specifying the max number of factors}

\CommentTok{#I received the warning "maximum iteration exceeded". It gave output, but it's best if we don't get that warning, so I increased it to 100. }

\NormalTok{grmsPAF1 }\CommentTok{#this object holds a great deal of information }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 10, rotate = "none", max.iter = 100, 
    fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
         PA1   PA2   PA3   PA4   PA5   PA6   PA7   PA8   PA9  PA10   h2   u2
Obj1    0.44  0.46 -0.14 -0.04  0.09 -0.15 -0.06 -0.09  0.00 -0.08 0.48 0.52
Obj2    0.35  0.59 -0.11 -0.14 -0.03  0.13 -0.08 -0.04  0.02  0.14 0.54 0.46
Obj3    0.37  0.46  0.07 -0.15  0.08  0.06  0.02 -0.07  0.01  0.02 0.39 0.61
Obj4    0.39  0.46 -0.01 -0.03 -0.09  0.20  0.13  0.15  0.01 -0.08 0.46 0.54
Obj5    0.21  0.55  0.11  0.02 -0.03 -0.02  0.05 -0.12  0.02  0.02 0.38 0.62
Obj6    0.47  0.26 -0.04 -0.13  0.06  0.06 -0.03  0.03  0.16 -0.10 0.35 0.65
Obj7    0.49  0.29 -0.09  0.09  0.07  0.06  0.02 -0.06 -0.05 -0.01 0.36 0.64
Obj8    0.29  0.42  0.05  0.10  0.22 -0.21 -0.15  0.19 -0.02  0.01 0.42 0.58
Obj9    0.32  0.32  0.19  0.05 -0.43 -0.17 -0.10  0.06  0.04  0.03 0.48 0.52
Obj10   0.31  0.31  0.08  0.18 -0.04 -0.01  0.14  0.05 -0.19 -0.03 0.29 0.71
Marg1   0.74 -0.50 -0.15  0.08 -0.03  0.02 -0.03  0.00 -0.01  0.07 0.83 0.17
Marg2   0.56 -0.56  0.06 -0.05 -0.01  0.07 -0.13 -0.06  0.05 -0.17 0.69 0.31
Marg3   0.54 -0.39 -0.08 -0.04 -0.07  0.02 -0.01  0.07  0.05  0.09 0.47 0.53
Marg4   0.58 -0.28 -0.07 -0.09  0.09 -0.06  0.01 -0.11 -0.02  0.03 0.45 0.55
Marg5   0.59 -0.22 -0.14 -0.10 -0.03 -0.16  0.22 -0.06 -0.02  0.02 0.50 0.50
Marg6   0.43 -0.33  0.01  0.38 -0.02  0.10 -0.07  0.12 -0.14 -0.06 0.50 0.50
Marg7   0.29 -0.49  0.24  0.03  0.07 -0.04  0.10  0.05  0.08  0.08 0.41 0.59
Strong1 0.03  0.02  0.53  0.22 -0.04  0.01 -0.09 -0.17 -0.13 -0.04 0.40 0.60
Strong2 0.01  0.12  0.38  0.37  0.06 -0.12  0.11  0.04  0.10 -0.09 0.35 0.65
Strong3 0.05  0.20  0.27  0.42  0.00  0.15  0.03 -0.03  0.08  0.04 0.32 0.68
Strong4 0.02 -0.04  0.56  0.25  0.02 -0.03  0.05 -0.01  0.07  0.03 0.39 0.61
Strong5 0.16 -0.03  0.21  0.41  0.12  0.05 -0.07  0.00  0.03  0.11 0.27 0.73
Angry1  0.16  0.10  0.39 -0.56  0.08  0.03  0.01  0.09 -0.07  0.00 0.52 0.48
Angry2  0.04 -0.09  0.57 -0.38  0.04  0.02 -0.03  0.05 -0.06  0.09 0.50 0.50
Angry3  0.04 -0.19  0.53 -0.42 -0.04  0.02  0.00 -0.03  0.03 -0.08 0.50 0.50
        com
Obj1    2.7
Obj2    2.2
Obj3    2.4
Obj4    3.0
Obj5    1.5
Obj6    2.3
Obj7    2.0
Obj8    4.0
Obj9    3.9
Obj10   4.0
Marg1   1.9
Marg2   2.4
Marg3   2.1
Marg4   1.7
Marg5   2.0
Marg6   3.6
Marg7   2.5
Strong1 1.8
Strong2 2.9
Strong3 2.7
Strong4 1.5
Strong5 2.4
Angry1  2.2
Angry2  1.9
Angry3  2.3

                       PA1  PA2  PA3  PA4  PA5  PA6  PA7  PA8  PA9 PA10
SS loadings           3.55 3.11 1.84 1.52 0.32 0.25 0.20 0.18 0.15 0.14
Proportion Var        0.14 0.12 0.07 0.06 0.01 0.01 0.01 0.01 0.01 0.01
Cumulative Var        0.14 0.27 0.34 0.40 0.41 0.42 0.43 0.44 0.44 0.45
Proportion Explained  0.32 0.28 0.16 0.14 0.03 0.02 0.02 0.02 0.01 0.01
Cumulative Proportion 0.32 0.59 0.75 0.89 0.92 0.94 0.96 0.97 0.99 1.00

Mean item complexity =  2.5
Test of the hypothesis that 10 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 95  and the objective function was  0.05 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.01 

The harmonic number of observations is  259 with the empirical chi square  8.19  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  13.01  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.193
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -514.88
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4   PA5
Correlation of (regression) scores with factors   0.95 0.93 0.88 0.86  0.61
Multiple R square of scores with factors          0.89 0.87 0.77 0.73  0.37
Minimum correlation of possible factor scores     0.79 0.74 0.54 0.46 -0.26
                                                    PA6   PA7   PA8   PA9  PA10
Correlation of (regression) scores with factors    0.56  0.52  0.50  0.45  0.49
Multiple R square of scores with factors           0.32  0.27  0.25  0.20  0.24
Minimum correlation of possible factor scores     -0.36 -0.45 -0.51 -0.60 -0.52
\end{verbatim}

\emph{The total variance for a particular variable will have two factors: some of it will be share with other variables (common variance) and some of it will be specific to that measure (unique variance). Random variance is also specific to one item, but not reliably so.}

We can examine this most easily by examining the matrix (second screen).

The columns PA1 thru PA10 are the (uninteresting at this point) unrotated loadings. These are the loading from each factor to each variable. PA stands for ``principal axis.''

Scrolling to the far right we are interested in:

\textbf{Communalities} are represented as \(h^2\). These are the proportions of common variance present in the variables. A variable that has no specific (or random) variance would have a communality of 1.0. If a variable shares none of its variance with any other variable its communality would be 0.0. As a point of comparison, in PCA these started as 1.0 because we extracted the same number of components as items. In PAF, because we must extract fewer factors than items, these will have some value.

**Uniquenessess* are represented as \(u2\). These are the amount of unique variance for each variable. They are calculated as \(1 - h^2\) (or 1 minus the communality).

The final column, \emph{com} represents \emph{item complexity.} This is an indication of how well an item reflects a single construct. If it is 1.0 then the item loads only on one component If it is 2.0, it loads evenly on two components, and so forth. For now, we can ignore this. \emph{I mostly wanted to reassure you that ``com'' is not ``communality''; h2 is communality}.

Let's switch to the first screen of output.

\textbf{Eigenvalues} are displayed in the row called, \emph{SS loadings} (i.e., the sum of squared loadings). They represent the variance explained by the particular linear component. PC1 explains 4.04 units of variance (out of a possible 25; the \# of components). As a proportion, this is 4.04/23 = 0.16 (reported in the \emph{Proportion Var} row).

PA1 explains 3.51 units of variance (out of a possible 25; the \# of potential factors). As a proportion, this is 3.51/25 = 0.1404 (reported in the \emph{Proportion Var} row).

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{3.51}\OperatorTok{/}\DecValTok{25}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1404
\end{verbatim}

Note.

We look at the eigenvalues to see how many are \textgreater{} 1.0 (Kaiser's eigenvalue \textgreater{} 1 criteria criteria). We see there are 4 that meet Kaiser's critera and 4 that meet Joliffe's criteria (eigenvalues \textgreater{} .77).

\textbf{Cumulative Var} is helpful to determine how many factors we'd like to retain to balance parsimony (few as possible) with the amount of variance we want to explain. The eigenvalues are in descending order. Using both Kaiser's (eigenvalue \textgreater{} 1.0) and Joiliffe's (eigenvalue \textgreater{} 0.7) criteria, we landed on a four-factor solution. Extracting four factors (like we did with PCA will) will explain 40\% of the variance. Eigenvalues are only one criteria, let's look at the scree plot.

\textbf{Scree plot}:\\
Eigenvalues are stored in the \emph{grmsPAF1} object's variable, ``values''. We can see all the values captured by this object with the \emph{names()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(grmsPAF1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "residual"               "dof"                    "chi"                   
 [4] "nh"                     "rms"                    "EPVAL"                 
 [7] "crms"                   "EBIC"                   "ESABIC"                
[10] "fit"                    "fit.off"                "sd"                    
[13] "factors"                "complexity"             "n.obs"                 
[16] "objective"              "criteria"               "STATISTIC"             
[19] "PVAL"                   "Call"                   "null.model"            
[22] "null.dof"               "null.chisq"             "TLI"                   
[25] "F0"                     "RMSEA"                  "BIC"                   
[28] "SABIC"                  "r.scores"               "R2"                    
[31] "valid"                  "score.cor"              "weights"               
[34] "rotation"               "communality"            "communalities"         
[37] "uniquenesses"           "values"                 "e.values"              
[40] "loadings"               "model"                  "fm"                    
[43] "Structure"              "communality.iterations" "method"                
[46] "scores"                 "R2.scores"              "r"                     
[49] "np.obs"                 "fn"                     "Vaccounted"            
\end{verbatim}

Plotting the eigen\emph{values} produces a scree plot. We can use this to further guage the number of factors we should extract.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(grmsPAF1}\OperatorTok{$}\NormalTok{values, }\DataTypeTok{type =} \StringTok{"b"}\NormalTok{) }\CommentTok{#type = "b" gives us "both" lines and points;  type = "l" gives lines and is relatively worthless}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-143-1.pdf}

We look for the point of \emph{inflexion}. That is, where the baseline levels out into a plateau. As I noted in the lesson PCA, this is one of the most clear scree plots (suggesting 4 factors) I've seen. We are benefitting by having created the simulated data from the factor results.

\hypertarget{specifying-the-number-of-factors}{%
\subsubsection{Specifying the number of factors}\label{specifying-the-number-of-factors}}

Having determined the number of components, we rerun the analysis with this specification. Especially when researchers may not have a clear theoretical structure that guides the process, researchers may do this iteratively with varying numbers of factors. Lewis and Neville {[}-lewis\_construction\_2015{]} examined solutions with 2, 3, 4, and 5 factors (they did a parallel \emph{factor} analysis; this lesson demonstrates principal axis factoring).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#grmsPAF2 <- psych::fa(GRMSr, nfactors=4, fm = "pa", rotate="none")}
\NormalTok{grmsPAF2 <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{fm =} \StringTok{"pa"}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"none"}\NormalTok{) }\CommentTok{#can copy prior script, but change nfactors and object name}
\NormalTok{grmsPAF2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "none", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
         PA1   PA2   PA3   PA4   h2   u2 com
Obj1    0.43  0.47 -0.13 -0.05 0.43 0.57 2.2
Obj2    0.33  0.59 -0.09 -0.14 0.49 0.51 1.7
Obj3    0.35  0.48  0.08 -0.15 0.38 0.62 2.1
Obj4    0.37  0.46 -0.01 -0.03 0.35 0.65 1.9
Obj5    0.19  0.56  0.12  0.03 0.37 0.63 1.3
Obj6    0.46  0.27 -0.03 -0.13 0.30 0.70 1.8
Obj7    0.48  0.31 -0.09  0.09 0.35 0.65 1.9
Obj8    0.26  0.41  0.04  0.09 0.25 0.75 1.8
Obj9    0.29  0.31  0.17  0.05 0.21 0.79 2.6
Obj10   0.30  0.31  0.07  0.17 0.22 0.78 2.7
Marg1   0.76 -0.48 -0.15  0.07 0.84 0.16 1.8
Marg2   0.57 -0.53  0.06 -0.04 0.61 0.39 2.0
Marg3   0.55 -0.37 -0.08 -0.05 0.45 0.55 1.8
Marg4   0.59 -0.26 -0.06 -0.09 0.43 0.57 1.4
Marg5   0.58 -0.19 -0.12 -0.10 0.40 0.60 1.4
Marg6   0.44 -0.32 -0.01  0.37 0.43 0.57 2.8
Marg7   0.30 -0.48  0.24  0.04 0.38 0.62 2.2
Strong1 0.03  0.02  0.51  0.24 0.31 0.69 1.4
Strong2 0.00  0.11  0.36  0.38 0.29 0.71 2.2
Strong3 0.04  0.20  0.25  0.43 0.29 0.71 2.1
Strong4 0.02 -0.04  0.56  0.27 0.39 0.61 1.5
Strong5 0.16 -0.02  0.19  0.41 0.23 0.77 1.8
Angry1  0.16  0.11  0.40 -0.54 0.49 0.51 2.1
Angry2  0.05 -0.09  0.58 -0.35 0.48 0.52 1.7
Angry3  0.05 -0.19  0.54 -0.40 0.50 0.50 2.1

                       PA1  PA2  PA3  PA4
SS loadings           3.50 3.05 1.80 1.49
Proportion Var        0.14 0.12 0.07 0.06
Cumulative Var        0.14 0.26 0.33 0.39
Proportion Explained  0.36 0.31 0.18 0.15
Cumulative Proportion 0.36 0.67 0.85 1.00

Mean item complexity =  1.9
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.94 0.92 0.87 0.84
Multiple R square of scores with factors          0.89 0.85 0.76 0.71
Minimum correlation of possible factor scores     0.78 0.71 0.52 0.42
\end{verbatim}

Our eigenvalues/SS loadings wiggle around a bit from the initial run. With four factors, we now, cumulatively, explain 30\% of the variance.

\emph{Communality} is the proportion of common variance within a variable. Changing from 10 to 4 factors will change this value (\(h2\)) as well as its associated \emph{uniqueness} (\(u2\)), which is calculated as ``1.0 minus the communality.''

Now we see that 43\% of the variance associate with Obj1 is common/shared (the \(h2\) value).

As a reminder of what we are doing, recall that we are looking for a more \emph{parsimonious} explanation than 25 items on the GRMS By respecifying a smaller number of factors, we lose some information. That is, the retained factors (now 4) cannot explain all of the variance present in the data (as we saw, it explains about 39\%, cumulatively). The amount of variance explained in each variable is represented by the communalities after extraction.

We can also inspect the communalities through the lens of Kaiser's criterion (the eigenvalue \textgreater{} 1 criteria) to see if we think that ``four'' was a good number of factors to extract.

Kaiser's criterion is believed to be accurate if:

\begin{itemize}
\tightlist
\item
  when there are fewer than 30 variables (we had 25) and, after extraction, the communalities are greater than .70

  \begin{itemize}
  \tightlist
  \item
    looking at our data, only 1 communality (Marg1) is \textgreater{} .70, so, this does not support extracting four components
  \end{itemize}
\item
  when the sample size is greater than 250 (ours was 259) and the average communality is \textgreater{} .60

  \begin{itemize}
  \tightlist
  \item
    we can extract the communalities from our object and calculate the mean the average communality
  \end{itemize}
\end{itemize}

Using the \emph{names()} function again, we see that ``communality'' is available for manipulation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(grmsPAF2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "residual"               "dof"                    "chi"                   
 [4] "nh"                     "rms"                    "EPVAL"                 
 [7] "crms"                   "EBIC"                   "ESABIC"                
[10] "fit"                    "fit.off"                "sd"                    
[13] "factors"                "complexity"             "n.obs"                 
[16] "objective"              "criteria"               "STATISTIC"             
[19] "PVAL"                   "Call"                   "null.model"            
[22] "null.dof"               "null.chisq"             "TLI"                   
[25] "F0"                     "RMSEA"                  "BIC"                   
[28] "SABIC"                  "r.scores"               "R2"                    
[31] "valid"                  "score.cor"              "weights"               
[34] "rotation"               "communality"            "communalities"         
[37] "uniquenesses"           "values"                 "e.values"              
[40] "loadings"               "model"                  "fm"                    
[43] "Structure"              "communality.iterations" "method"                
[46] "scores"                 "R2.scores"              "r"                     
[49] "np.obs"                 "fn"                     "Vaccounted"            
\end{verbatim}

We can use this value to calculate their mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(grmsPAF2}\OperatorTok{$}\NormalTok{communality)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3934452
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#sum(grmsPAF2$communality) #checking my work by calculating the sum and dividing by 25}
\CommentTok{#9.836131/25}
\end{Highlighting}
\end{Shaded}

We see that our average communality is 0.39. These two criteria suggest that we may not have the best solution. That said (in our defense):

\begin{itemize}
\tightlist
\item
  We used the scree plot as a guide and it was very clear.
\item
  We have an adequate sample size and that was supported with the KMO.
\item
  Are the number of factors consistent with theory? We have not yet inspected the factor loadings. This will provide us with more information.
\end{itemize}

We could do several things:

\begin{itemize}
\tightlist
\item
  rerun with a different number of factors (recall Lewis and Neville \citeyearpar{lewis_construction_2015} ran models with 2, 3, 4, and 5 factors)
\item
  conduct more diagnostics tests

  \begin{itemize}
  \tightlist
  \item
    reproduced correlation matrix
  \item
    the difference between the reproduced correlation matrix and the correlation matrix in the data
  \end{itemize}
\end{itemize}

The \emph{factor.model()} function in \emph{psych} produces the \emph{reproduced correlation matrix} by using the \emph{loadings} in our extracted object. Conceptually, this matrix is the correlations that should be produced if we did not have the raw data but we only had the factor loadings. We could do fancy matrix algebra and produce these.

The questions, though, is: How close did we get? How different is the \emph{reproduced correlation matrix} from \emph{GRMSmatrix} -- the \(R\)-matrix produced from our raw data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(psych}\OperatorTok{::}\KeywordTok{factor.model}\NormalTok{(grmsPAF2}\OperatorTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}\CommentTok{#produces the reproduced correlation matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1     0.426  0.439  0.372  0.378  0.330  0.335  0.362  0.296  0.243  0.256
Obj2     0.439  0.487  0.411  0.400  0.381  0.333  0.339  0.312  0.253  0.252
Obj3     0.372  0.411  0.380  0.354  0.340  0.309  0.299  0.278  0.253  0.233
Obj4     0.378  0.400  0.354  0.350  0.329  0.299  0.320  0.283  0.245  0.247
Obj5     0.330  0.381  0.340  0.329  0.366  0.234  0.261  0.288  0.248  0.246
Obj6     0.335  0.333  0.309  0.299  0.234  0.303  0.299  0.219  0.205  0.197
Obj7     0.362  0.339  0.299  0.320  0.261  0.299  0.349  0.259  0.227  0.251
Obj8     0.296  0.312  0.278  0.283  0.288  0.219  0.259  0.246  0.213  0.224
Obj9     0.243  0.253  0.253  0.245  0.248  0.205  0.227  0.213  0.208  0.203
Obj10    0.256  0.252  0.233  0.247  0.246  0.197  0.251  0.224  0.203  0.221
Marg1    0.111 -0.035  0.016  0.053 -0.141  0.212  0.236  0.000  0.051  0.074
Marg2   -0.013 -0.126 -0.038 -0.034 -0.180  0.122  0.102 -0.068  0.012  0.000
Marg3    0.071 -0.027  0.018  0.031 -0.114  0.159  0.152 -0.017  0.030  0.032
Marg4    0.143  0.060  0.096  0.101 -0.040  0.215  0.203  0.039  0.078  0.074
Marg5    0.178  0.101  0.119  0.128 -0.014  0.231  0.223  0.059  0.084  0.085
Marg6    0.021 -0.094 -0.051  0.003 -0.084  0.068  0.148  0.018  0.049  0.094
Marg7   -0.132 -0.214 -0.109 -0.115 -0.182 -0.005 -0.021 -0.104 -0.017 -0.036
Strong1 -0.057 -0.058  0.025  0.009  0.084 -0.028 -0.003  0.059  0.112  0.093
Strong2 -0.011 -0.017  0.027  0.039  0.117 -0.027  0.041  0.096  0.116  0.129
Strong3  0.057  0.047  0.064  0.091  0.160  0.010  0.100  0.141  0.137  0.167
Strong4 -0.099 -0.107 -0.008 -0.024  0.055 -0.055 -0.027  0.037  0.102  0.082
Strong5  0.013 -0.035  0.000  0.034  0.052  0.009  0.093  0.078  0.094  0.127
Angry1   0.088  0.151  0.219  0.120  0.123  0.157  0.023  0.053  0.116  0.015
Angry2  -0.085 -0.044  0.072 -0.019  0.017  0.022 -0.091 -0.032  0.063 -0.034
Angry3  -0.122 -0.090  0.032 -0.061 -0.043  0.006 -0.121 -0.077  0.025 -0.075
         Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7 Strong1 Strong2
Obj1     0.111 -0.013  0.071  0.143  0.178  0.021 -0.132  -0.057  -0.011
Obj2    -0.035 -0.126 -0.027  0.060  0.101 -0.094 -0.214  -0.058  -0.017
Obj3     0.016 -0.038  0.018  0.096  0.119 -0.051 -0.109   0.025   0.027
Obj4     0.053 -0.034  0.031  0.101  0.128  0.003 -0.115   0.009   0.039
Obj5    -0.141 -0.180 -0.114 -0.040 -0.014 -0.084 -0.182   0.084   0.117
Obj6     0.212  0.122  0.159  0.215  0.231  0.068 -0.005  -0.028  -0.027
Obj7     0.236  0.102  0.152  0.203  0.223  0.148 -0.021  -0.003   0.041
Obj8     0.000 -0.068 -0.017  0.039  0.059  0.018 -0.104   0.059   0.096
Obj9     0.051  0.012  0.030  0.078  0.084  0.049 -0.017   0.112   0.116
Obj10    0.074  0.000  0.032  0.074  0.085  0.094 -0.036   0.093   0.129
Marg1    0.835  0.674  0.604  0.574  0.544  0.511  0.427  -0.051  -0.079
Marg2    0.674  0.609  0.506  0.472  0.429  0.399  0.437   0.023  -0.051
Marg3    0.604  0.506  0.448  0.428  0.404  0.341  0.322  -0.046  -0.086
Marg4    0.574  0.472  0.428  0.426  0.409  0.306  0.281  -0.044  -0.083
Marg5    0.544  0.429  0.404  0.409  0.399  0.279  0.233  -0.076  -0.101
Marg6    0.511  0.399  0.341  0.306  0.279  0.427  0.297   0.088   0.103
Marg7    0.427  0.437  0.322  0.281  0.233  0.297  0.378   0.127   0.049
Strong1 -0.051  0.023 -0.046 -0.044 -0.076  0.088  0.127   0.314   0.275
Strong2 -0.079 -0.051 -0.086 -0.083 -0.101  0.103  0.049   0.275   0.286
Strong3 -0.070 -0.082 -0.090 -0.080 -0.087  0.113 -0.003   0.235   0.276
Strong4 -0.031  0.055 -0.032 -0.038 -0.078  0.118  0.169   0.349   0.302
Strong5  0.135  0.098  0.064  0.052  0.034  0.230  0.123   0.198   0.223
Angry1  -0.033  0.081  0.038  0.088  0.073 -0.168  0.068   0.084  -0.045
Angry2  -0.036  0.124  0.027  0.045  0.006 -0.086  0.179   0.211   0.068
Angry3   0.017  0.177  0.071  0.079  0.036 -0.073  0.215   0.177   0.023
        Strong3 Strong4 Strong5 Angry1 Angry2 Angry3
Obj1      0.057  -0.099   0.013  0.088 -0.085 -0.122
Obj2      0.047  -0.107  -0.035  0.151 -0.044 -0.090
Obj3      0.064  -0.008   0.000  0.219  0.072  0.032
Obj4      0.091  -0.024   0.034  0.120 -0.019 -0.061
Obj5      0.160   0.055   0.052  0.123  0.017 -0.043
Obj6      0.010  -0.055   0.009  0.157  0.022  0.006
Obj7      0.100  -0.027   0.093  0.023 -0.091 -0.121
Obj8      0.141   0.037   0.078  0.053 -0.032 -0.077
Obj9      0.137   0.102   0.094  0.116  0.063  0.025
Obj10     0.167   0.082   0.127  0.015 -0.034 -0.075
Marg1    -0.070  -0.031   0.135 -0.033 -0.036  0.017
Marg2    -0.082   0.055   0.098  0.081  0.124  0.177
Marg3    -0.090  -0.032   0.064  0.038  0.027  0.071
Marg4    -0.080  -0.038   0.052  0.088  0.045  0.079
Marg5    -0.087  -0.078   0.034  0.073  0.006  0.036
Marg6     0.113   0.118   0.230 -0.168 -0.086 -0.073
Marg7    -0.003   0.169   0.123  0.068  0.179  0.215
Strong1   0.235   0.349   0.198  0.084  0.211  0.177
Strong2   0.276   0.302   0.223 -0.045  0.068  0.023
Strong3   0.288   0.253   0.228 -0.102 -0.019 -0.070
Strong4   0.253   0.392   0.224  0.078  0.235  0.203
Strong5   0.228   0.224   0.234 -0.125 -0.026 -0.052
Angry1   -0.102   0.078  -0.125  0.493  0.425  0.427
Angry2   -0.019   0.235  -0.026  0.425  0.475  0.479
Angry3   -0.070   0.203  -0.052  0.427  0.479  0.496
\end{verbatim}

We're not really interested in this matrix. We just need it to compare it to the \emph{GRMSmatrix} to produce the residuals. We do that next.

\textbf{Residuals} are the difference between the reproduced (i.e., those created from our factor loadings) and \(R\)-matrix produced by the raw data.

If we look at the \(r_{_{Obj1Obj2}}\) in our original correlation matrix (theoreticallky fro the raw data {[}although we simulated data{]}), the value is 0.41. The reproduced correlation for this pair is 0.426. The diffference is -0.016. Our table shows -0.013 because of rounding error.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{.41} \OperatorTok{-}\StringTok{ }\FloatTok{.426}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.016
\end{verbatim}

By using the \emph{factor.residuals()} function we can calculate the residuals. Here we will see this difference calculated for us, for all the elements in the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(psych}\OperatorTok{::}\KeywordTok{factor.residuals}\NormalTok{(GRMSr, grmsPAF2}\OperatorTok{$}\NormalTok{loadings), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Obj1   Obj2   Obj3   Obj4   Obj5   Obj6   Obj7   Obj8   Obj9  Obj10
Obj1     0.574 -0.031  0.010 -0.049  0.008  0.000  0.004  0.053  0.011 -0.016
Obj2    -0.031  0.513  0.019  0.001  0.016  0.007  0.013 -0.019  0.024 -0.015
Obj3     0.010  0.019  0.620  0.003  0.001  0.003  0.001  0.002 -0.037  0.003
Obj4    -0.049  0.001  0.003  0.650 -0.007  0.022  0.004 -0.029  0.023  0.029
Obj5     0.008  0.016  0.001 -0.007  0.634 -0.006 -0.005 -0.026  0.018  0.001
Obj6     0.000  0.007  0.003  0.022 -0.006  0.697  0.006  0.019 -0.011 -0.033
Obj7     0.004  0.013  0.001  0.004 -0.005  0.006  0.651 -0.004 -0.034  0.009
Obj8     0.053 -0.019  0.002 -0.029 -0.026  0.019 -0.004  0.754 -0.005  0.000
Obj9     0.011  0.024 -0.037  0.023  0.018 -0.011 -0.034 -0.005  0.792  0.017
Obj10   -0.016 -0.015  0.003  0.029  0.001 -0.033  0.009  0.000  0.017  0.779
Marg1   -0.006  0.010 -0.004  0.005 -0.003 -0.010 -0.005 -0.001  0.014 -0.010
Marg2    0.012 -0.001  0.004  0.006 -0.004  0.034  0.004 -0.017 -0.002 -0.028
Marg3   -0.017  0.014  0.000  0.012  0.005  0.002 -0.029 -0.010  0.036 -0.007
Marg4    0.023 -0.011  0.008 -0.030  0.008 -0.005  0.011  0.014 -0.032 -0.008
Marg5    0.018 -0.029 -0.011 -0.002  0.016 -0.023  0.002 -0.014  0.020  0.040
Marg6   -0.019  0.006 -0.011  0.023 -0.023 -0.005  0.010  0.008  0.002  0.039
Marg7   -0.012 -0.008  0.019 -0.016  0.010  0.007  0.009  0.000 -0.023 -0.003
Strong1  0.011 -0.009  0.005 -0.031  0.025 -0.026  0.008 -0.024  0.031  0.006
Strong2  0.017 -0.042 -0.015  0.002  0.011  0.015 -0.001  0.034  0.004  0.001
Strong3 -0.015  0.007  0.006  0.051 -0.015  0.011  0.005 -0.037 -0.008 -0.025
Strong4 -0.007  0.002  0.016 -0.015  0.008  0.014 -0.021  0.008  0.005  0.005
Strong5  0.009  0.032 -0.005 -0.023 -0.014 -0.007  0.009  0.033 -0.040 -0.017
Angry1   0.022 -0.002 -0.018  0.029 -0.016 -0.003 -0.012  0.022 -0.034  0.010
Angry2  -0.030  0.008  0.005 -0.004 -0.006 -0.016  0.023  0.025  0.003 -0.011
Angry3  -0.002 -0.002  0.001  0.000 -0.008  0.012  0.000 -0.031  0.024  0.008
         Marg1  Marg2  Marg3  Marg4  Marg5  Marg6  Marg7 Strong1 Strong2
Obj1    -0.006  0.012 -0.017  0.023  0.018 -0.019 -0.012   0.011   0.017
Obj2     0.010 -0.001  0.014 -0.011 -0.029  0.006 -0.008  -0.009  -0.042
Obj3    -0.004  0.004  0.000  0.008 -0.011 -0.011  0.019   0.005  -0.015
Obj4     0.005  0.006  0.012 -0.030 -0.002  0.023 -0.016  -0.031   0.002
Obj5    -0.003 -0.004  0.005  0.008  0.016 -0.023  0.010   0.025   0.011
Obj6    -0.010  0.034  0.002 -0.005 -0.023 -0.005  0.007  -0.026   0.015
Obj7    -0.005  0.004 -0.029  0.011  0.002  0.010  0.009   0.008  -0.001
Obj8    -0.001 -0.017 -0.010  0.014 -0.014  0.008  0.000  -0.024   0.034
Obj9     0.014 -0.002  0.036 -0.032  0.020  0.002 -0.023   0.031   0.004
Obj10   -0.010 -0.028 -0.007 -0.008  0.040  0.039 -0.003   0.006   0.001
Marg1    0.165 -0.010  0.011  0.004  0.004  0.002 -0.007   0.010  -0.016
Marg2   -0.010  0.391 -0.003  0.002 -0.020  0.033 -0.016   0.024  -0.010
Marg3    0.011 -0.003  0.552 -0.013 -0.003  0.005  0.016  -0.015  -0.009
Marg4    0.004  0.002 -0.013  0.574  0.035 -0.024 -0.007   0.019   0.001
Marg5    0.004 -0.020 -0.003  0.035  0.601 -0.033  0.026  -0.022   0.026
Marg6    0.002  0.033  0.005 -0.024 -0.033  0.573 -0.005   0.013  -0.013
Marg7   -0.007 -0.016  0.016 -0.007  0.026 -0.005  0.622  -0.036   0.030
Strong1  0.010  0.024 -0.015  0.019 -0.022  0.013 -0.036   0.686  -0.014
Strong2 -0.016 -0.010 -0.009  0.001  0.026 -0.013  0.030  -0.014   0.714
Strong3  0.012 -0.008  0.004  0.001 -0.015 -0.011 -0.011   0.008  -0.007
Strong4 -0.009 -0.003  0.008  0.001  0.013 -0.013  0.021  -0.006   0.013
Strong5  0.010 -0.001  0.002  0.006 -0.023  0.000  0.012  -0.005  -0.009
Angry1  -0.003 -0.018  0.011 -0.009 -0.003  0.012  0.011   0.004  -0.005
Angry2   0.003 -0.012 -0.004  0.015 -0.012  0.006  0.006   0.004  -0.017
Angry3   0.005  0.029 -0.017 -0.010  0.001 -0.013 -0.015   0.003   0.010
        Strong3 Strong4 Strong5 Angry1 Angry2 Angry3
Obj1     -0.015  -0.007   0.009  0.022 -0.030 -0.002
Obj2      0.007   0.002   0.032 -0.002  0.008 -0.002
Obj3      0.006   0.016  -0.005 -0.018  0.005  0.001
Obj4      0.051  -0.015  -0.023  0.029 -0.004  0.000
Obj5     -0.015   0.008  -0.014 -0.016 -0.006 -0.008
Obj6      0.011   0.014  -0.007 -0.003 -0.016  0.012
Obj7      0.005  -0.021   0.009 -0.012  0.023  0.000
Obj8     -0.037   0.008   0.033  0.022  0.025 -0.031
Obj9     -0.008   0.005  -0.040 -0.034  0.003  0.024
Obj10    -0.025   0.005  -0.017  0.010 -0.011  0.008
Marg1     0.012  -0.009   0.010 -0.003  0.003  0.005
Marg2    -0.008  -0.003  -0.001 -0.018 -0.012  0.029
Marg3     0.004   0.008   0.002  0.011 -0.004 -0.017
Marg4     0.001   0.001   0.006 -0.009  0.015 -0.010
Marg5    -0.015   0.013  -0.023 -0.003 -0.012  0.001
Marg6    -0.011  -0.013   0.000  0.012  0.006 -0.013
Marg7    -0.011   0.021   0.012  0.011  0.006 -0.015
Strong1   0.008  -0.006  -0.005  0.004  0.004  0.003
Strong2  -0.007   0.013  -0.009 -0.005 -0.017  0.010
Strong3   0.712  -0.004   0.040  0.000  0.000  0.003
Strong4  -0.004   0.608  -0.007 -0.017  0.000 -0.004
Strong5   0.040  -0.007   0.766  0.015  0.005  0.000
Angry1    0.000  -0.017   0.015  0.507  0.018  0.006
Angry2    0.000   0.000   0.005  0.018  0.525 -0.013
Angry3    0.003  -0.004   0.000  0.006 -0.013  0.504
\end{verbatim}

There are several strategies to evaluate this matrix:

\begin{itemize}
\tightlist
\item
  see how large the residuals are, compared to the original correlations

  \begin{itemize}
  \tightlist
  \item
    the worst possible model would occur if we extracted no factors and would be the size of the original correlations
  \item
    if the correlations were small to start with, we expect small residuals
  \item
    if the correlations were large to start with, the residuals will be relatively larger (this is not terribly problematic)
  \end{itemize}
\item
  comparing residuals requires squaring them first (because residuals can be both positive and negative)

  \begin{itemize}
  \tightlist
  \item
    the sum of the squared residuals divided by the sum of the squared correlations is an estimate of model fit. Subtracting this from 1.0 means that it ranges from 0 to 1. Values \textgreater{} .95 are an indication of good fit.
  \end{itemize}
\end{itemize}

Analyzing the residuals means we need to extract only the upper right of the triangle them into an object. We can do this in steps.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2_resids <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{factor.residuals}\NormalTok{(GRMSr, grmsPAF2}\OperatorTok{$}\NormalTok{loadings)}\CommentTok{#first extract the resids}
\NormalTok{grmsPAF2_resids <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(grmsPAF2_resids[}\KeywordTok{upper.tri}\NormalTok{(grmsPAF2_resids)])}\CommentTok{#the object has the residuals in a single column}
\KeywordTok{head}\NormalTok{(grmsPAF2_resids)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              [,1]
[1,] -0.0309870563
[2,]  0.0100666134
[3,]  0.0185760343
[4,] -0.0490479537
[5,]  0.0008521008
[6,]  0.0030123717
\end{verbatim}

One criteria of residual analysis is to see how many residuals there are that are greater than an absolute value of 0.05. The result will be a single column with TRUE if it is \textgreater{} \textbar0.05\textbar{} and false if it is smaller. The sum function will tell us how many TRUE repsonses are in the matrix. Further, we can write script to obtain the proportion of total number of residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{large.resid <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(grmsPAF2_resids) }\OperatorTok{>}\StringTok{ }\FloatTok{0.05}
\CommentTok{#large.resid}
\KeywordTok{sum}\NormalTok{(large.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(large.resid) }\OperatorTok{/}\StringTok{ }\KeywordTok{nrow}\NormalTok{(grmsPAF2_resids),}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007
\end{verbatim}

We learn that there are 2 residuals greater than the absolute value of 0.05. This represents less than 1\% of the total number of residuals.

There are no hard rules about what proportion of residuals can be greater than 0.05. Field recommends that it stay below 50\% \citep{field_discovering_2012}.

Another approach to analyzing residuals is to look at their mean. Because of the +/- valences, we need to square them (to eliminate the negative), take the average, then take the square root.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{mean}\NormalTok{(grmsPAF2_resids}\OperatorTok{^}\DecValTok{2}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.017
\end{verbatim}

While there are no clear guidelines to interpret these, one recommendation is to consider extracting more components if the value is higher than 0.08 \citep{field_discovering_2012}.

Finally, we expect our residuals to be normally distributed. A histogram can help us inspect the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(grmsPAF2_resids)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-153-1.pdf}

Not bad! It looks reasonably normal. No outliers.

\hypertarget{quick-recap-of-how-to-evaluate-the-of-factors-we-extracted}{%
\subsubsection{Quick recap of how to evaluate the \# of factors we extracted}\label{quick-recap-of-how-to-evaluate-the-of-factors-we-extracted}}

\begin{itemize}
\tightlist
\item
  If fewer than 30 variables, the eigenvalue \textgreater{} 1 (Kaiser's) critera is fine, so long as communalities are all \textgreater{} .70.
\item
  If sample size \textgreater{} 250 and the average communalitie are .6 or greater, this is fine.
\item
  When \emph{N} \textgreater{} 200, the scree plot can be used.
\item
  Regarding residuals

  \begin{itemize}
  \tightlist
  \item
    fewer than 50\% should have absolute values \textgreater{} 0.05
  \item
    model fit should be \textgreater{} 0.90
  \end{itemize}
\end{itemize}

\hypertarget{factor-rotation}{%
\subsection{Factor rotation}\label{factor-rotation}}

The original solution of a principal components or principal axes factor analysis is a set of vectors that best account for the observed covariance or correlation matrix. Each additional component or factor accounts for progressively less and less variance. The solution is efficient (yay) but difficult to interpret (boo).

Thanks to Thurstone's five rules toward a simple structure (circa 1947), interpretation of a matrix is facilitaed by \emph{rotation} (multiplying a matrix by a matrix of orthogonal vectors that preserve the communalities of each variable). Both the original matrix and the solution will be orthogonal.

\emph{Parsimony} becomes a statistical consideration (an equation, in fact) and goal and is maximized when each variable has a 1.0 loading on one factor and the rest are zero.

Different rotation strategies emphasize different goals related to parsimony:

\emph{Quartimax} seeks to maximize the notion of variable parsimony (each variable is associated with one factor) and permits the rotation toward a general factor (ignoring smaller factors).

\emph{Varimax} maximizes the variance of squared loadings taken over items instead of over factors and \emph{avoids} a general factor.

Rotation improves the interpretation of the factor by maximizing the loading on each variable on one of the extracted factors while minimizing the loading on all other factors Rotation works by changing the absolute values of the variables while keeping their differential values constant.

There are two big choices (to be made on theoretical grounds):

\begin{itemize}
\tightlist
\item
  Orthogonal rotation if you think that the factors are independent/unrelated.

  \begin{itemize}
  \tightlist
  \item
    most common orthogonal rotation is varimax
  \end{itemize}
\item
  Oblique rotation if you think that the factors are related correlated.

  \begin{itemize}
  \tightlist
  \item
    oblimin and promax are common oblique rotations
  \end{itemize}
\end{itemize}

\hypertarget{orthogonal-rotation-1}{%
\subsubsection{Orthogonal Rotation}\label{orthogonal-rotation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#grmsPAF2ORTH <- psych::fa(GRMSr, nfactors = 4, fm = "pa", rotate = "varimax")}
\NormalTok{grmsPAF2ORTH <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{fm =} \StringTok{"pa"}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{grmsPAF2ORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
          PA1   PA2   PA3   PA4   h2   u2 com
Obj1     0.07  0.64 -0.08 -0.08 0.43 0.57 1.1
Obj2    -0.09  0.68  0.00 -0.12 0.49 0.51 1.1
Obj3    -0.01  0.60  0.14 -0.02 0.38 0.62 1.1
Obj4     0.02  0.59  0.00  0.01 0.35 0.65 1.0
Obj5    -0.19  0.56  0.04  0.13 0.37 0.63 1.3
Obj6     0.20  0.50  0.07 -0.08 0.30 0.70 1.4
Obj7     0.21  0.53 -0.13  0.05 0.35 0.65 1.5
Obj8    -0.03  0.48 -0.05  0.12 0.25 0.75 1.2
Obj9     0.04  0.41  0.08  0.18 0.21 0.79 1.5
Obj10    0.05  0.41 -0.07  0.20 0.22 0.78 1.6
Marg1    0.91  0.06 -0.08 -0.03 0.84 0.16 1.0
Marg2    0.76 -0.08  0.14  0.02 0.61 0.39 1.1
Marg3    0.66  0.03  0.03 -0.07 0.45 0.55 1.0
Marg4    0.63  0.15  0.06 -0.09 0.43 0.57 1.2
Marg5    0.58  0.20  0.02 -0.13 0.40 0.60 1.3
Marg6    0.55 -0.02 -0.21  0.28 0.43 0.57 1.8
Marg7    0.51 -0.21  0.19  0.19 0.38 0.62 2.0
Strong1 -0.02  0.01  0.21  0.52 0.31 0.69 1.3
Strong2 -0.07  0.06  0.00  0.53 0.29 0.71 1.1
Strong3 -0.08  0.15 -0.11  0.50 0.29 0.71 1.4
Strong4  0.01 -0.05  0.23  0.58 0.39 0.61 1.3
Strong5  0.15  0.05 -0.13  0.44 0.23 0.77 1.4
Angry1   0.01  0.21  0.66 -0.11 0.49 0.51 1.3
Angry2   0.03 -0.03  0.68  0.13 0.48 0.52 1.1
Angry3   0.09 -0.11  0.69  0.06 0.50 0.50 1.1

                       PA1  PA2  PA3  PA4
SS loadings           3.33 3.20 1.67 1.64
Proportion Var        0.13 0.13 0.07 0.07
Cumulative Var        0.13 0.26 0.33 0.39
Proportion Explained  0.34 0.33 0.17 0.17
Cumulative Proportion 0.34 0.66 0.83 1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.95 0.91 0.87 0.84
Multiple R square of scores with factors          0.91 0.84 0.76 0.71
Minimum correlation of possible factor scores     0.81 0.67 0.52 0.42
\end{verbatim}

Essentially we have the same information as before, except that loadings are calculated after rotation (which adjusts the absolute values of the factor loadings while keeping their differential vales constant). Our communality and uniqueness values remain the same. The eigenvalues (SS loadings) should even out, but the proportion of variance explained and cumulative variance (39\%) will remain the same.

The \emph{print.psych()} function facilitates interpretation and prioritizes the information about which we care most:

\begin{itemize}
\tightlist
\item
  ``cut'' will display loadings above .3, this allows us to see

  \begin{itemize}
  \tightlist
  \item
    if some items load on no factors
  \item
    if some items have cross-loadings (and their relative weights)
  \end{itemize}
\item
  ``sort'' will reorder the loadings to make it clearer (to the best of its ability\ldots in the case of ties) to which factor/scale it belongs
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2_table <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{print.psych}\NormalTok{(grmsPAF2ORTH, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{sort =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
        item   PA1   PA2   PA3   PA4   h2   u2 com
Marg1     11  0.91                   0.84 0.16 1.0
Marg2     12  0.76                   0.61 0.39 1.1
Marg3     13  0.66                   0.45 0.55 1.0
Marg4     14  0.63                   0.43 0.57 1.2
Marg5     15  0.58                   0.40 0.60 1.3
Marg6     16  0.55                   0.43 0.57 1.8
Marg7     17  0.51                   0.38 0.62 2.0
Obj2       2        0.68             0.49 0.51 1.1
Obj1       1        0.64             0.43 0.57 1.1
Obj3       3        0.60             0.38 0.62 1.1
Obj4       4        0.59             0.35 0.65 1.0
Obj5       5        0.56             0.37 0.63 1.3
Obj7       7        0.53             0.35 0.65 1.5
Obj6       6        0.50             0.30 0.70 1.4
Obj8       8        0.48             0.25 0.75 1.2
Obj10     10        0.41             0.22 0.78 1.6
Obj9       9        0.41             0.21 0.79 1.5
Angry3    25              0.69       0.50 0.50 1.1
Angry2    24              0.68       0.48 0.52 1.1
Angry1    23              0.66       0.49 0.51 1.3
Strong4   21                    0.58 0.39 0.61 1.3
Strong2   19                    0.53 0.29 0.71 1.1
Strong1   18                    0.52 0.31 0.69 1.3
Strong3   20                    0.50 0.29 0.71 1.4
Strong5   22                    0.44 0.23 0.77 1.4

                       PA1  PA2  PA3  PA4
SS loadings           3.33 3.20 1.67 1.64
Proportion Var        0.13 0.13 0.07 0.07
Cumulative Var        0.13 0.26 0.33 0.39
Proportion Explained  0.34 0.33 0.17 0.17
Cumulative Proportion 0.34 0.66 0.83 1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.95 0.91 0.87 0.84
Multiple R square of scores with factors          0.91 0.84 0.76 0.71
Minimum correlation of possible factor scores     0.81 0.67 0.52 0.42
\end{verbatim}

In the unrotated solution, most variables loaded on the first component. After rotation, there are four clear components/scales. Further, there is clear (or at least reasonable) component/scale membership for each item and no cross-loadings. This is a bit different than the PCA orthogonal rotation where the Marg7 item had a cross-loading.

If this were a new scale and we had not yet established ideas for subscales, the next step is to look back at the items, themselves, and try to name the scales/components. If our scale construction included a priori/planned subscales, here's where we hope the items fall where they were hypothesized to do so. Our simulated data worked perfectly and replicated the four scales that Lewis and Neville \citep{lewis_construction_2015} reported in the article.

\begin{itemize}
\tightlist
\item
  Assumptions of Beauty and Sexual Objectification
\item
  Silenced and Marginalized
\item
  Strong Woman Stereotype
\item
  Angry Woman Stereotype
\end{itemize}

We can also create a figure of the result. Note the direction of the arrows from the factor (latent variable) to the items in PAF; in PCA the arrows went from item to component.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.diagram}\NormalTok{(grmsPAF2ORTH)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-156-1.pdf}

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#names(grmsPAF2ORTH)}
\NormalTok{pafORTH_table <-}\StringTok{ }\KeywordTok{round}\NormalTok{(grmsPAF2ORTH}\OperatorTok{$}\NormalTok{loadings,}\DecValTok{3}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(pafORTH_table, }\DataTypeTok{file=}\StringTok{"pafORTH_table.csv"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{pafORTH_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
        PA1    PA2    PA3    PA4   
Obj1            0.638              
Obj2            0.681        -0.124
Obj3            0.600  0.142       
Obj4            0.591              
Obj5    -0.186  0.560         0.131
Obj6     0.204  0.501              
Obj7     0.214  0.534 -0.126       
Obj8            0.477         0.124
Obj9            0.410         0.179
Obj10           0.414         0.205
Marg1    0.907                     
Marg2    0.762         0.142       
Marg3    0.664                     
Marg4    0.626  0.153              
Marg5    0.584  0.200        -0.133
Marg6    0.553        -0.208  0.279
Marg7    0.509 -0.213  0.195  0.189
Strong1                0.213  0.518
Strong2                       0.526
Strong3         0.149 -0.114  0.496
Strong4                0.232  0.579
Strong5  0.147        -0.129  0.440
Angry1          0.205  0.662 -0.113
Angry2                 0.675  0.132
Angry3         -0.106  0.687       

                 PA1   PA2   PA3   PA4
SS loadings    3.327 3.203 1.670 1.638
Proportion Var 0.133 0.128 0.067 0.066
Cumulative Var 0.133 0.261 0.328 0.393
\end{verbatim}

\hypertarget{oblique-rotation-1}{%
\subsubsection{Oblique Rotation}\label{oblique-rotation-1}}

Whereas the orthogonal rotation sought to maximize the independence/unrelatedness of the coponents, an oblique rotation will allow them to be correlated. Researchers often explore both solutions, but then report only one.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#grmsPAF2obl <- psych::fa(GRMSr, nfactors = 4, fm = "pa", rotate = "oblimin")}
\NormalTok{grmsPAF2obl <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{fm =} \StringTok{"pa"}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"oblimin"}\NormalTok{)}
\NormalTok{grmsPAF2obl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "oblimin", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
          PA1   PA2   PA3   PA4   h2   u2 com
Obj1     0.08  0.64 -0.07 -0.06 0.43 0.57 1.1
Obj2    -0.08  0.68  0.01 -0.10 0.49 0.51 1.1
Obj3    -0.01  0.60  0.15  0.00 0.38 0.62 1.1
Obj4     0.02  0.59  0.01  0.03 0.35 0.65 1.0
Obj5    -0.19  0.57  0.04  0.15 0.37 0.63 1.4
Obj6     0.21  0.49  0.08 -0.06 0.30 0.70 1.4
Obj7     0.22  0.53 -0.12  0.07 0.35 0.65 1.5
Obj8    -0.03  0.48 -0.04  0.14 0.25 0.75 1.2
Obj9     0.04  0.41  0.08  0.19 0.21 0.79 1.5
Obj10    0.05  0.41 -0.08  0.22 0.22 0.78 1.6
Marg1    0.91  0.03 -0.08 -0.02 0.84 0.16 1.0
Marg2    0.76 -0.11  0.14  0.02 0.61 0.39 1.1
Marg3    0.67  0.01  0.03 -0.07 0.45 0.55 1.0
Marg4    0.63  0.13  0.07 -0.08 0.43 0.57 1.2
Marg5    0.59  0.18  0.03 -0.12 0.40 0.60 1.3
Marg6    0.55 -0.04 -0.22  0.28 0.43 0.57 1.8
Marg7    0.50 -0.23  0.18  0.18 0.38 0.62 2.0
Strong1 -0.04  0.01  0.19  0.52 0.31 0.69 1.3
Strong2 -0.09  0.06 -0.02  0.53 0.29 0.71 1.1
Strong3 -0.09  0.15 -0.14  0.50 0.29 0.71 1.4
Strong4 -0.01 -0.05  0.20  0.58 0.39 0.61 1.3
Strong5  0.14  0.04 -0.15  0.44 0.23 0.77 1.5
Angry1  -0.01  0.21  0.67 -0.11 0.49 0.51 1.3
Angry2   0.00 -0.03  0.67  0.13 0.48 0.52 1.1
Angry3   0.06 -0.10  0.68  0.06 0.50 0.50 1.1

                       PA1  PA2  PA3  PA4
SS loadings           3.33 3.19 1.67 1.65
Proportion Var        0.13 0.13 0.07 0.07
Cumulative Var        0.13 0.26 0.33 0.39
Proportion Explained  0.34 0.32 0.17 0.17
Cumulative Proportion 0.34 0.66 0.83 1.00

 With factor correlations of 
     PA1   PA2   PA3   PA4
PA1 1.00  0.03  0.03  0.02
PA2 0.03  1.00 -0.02 -0.04
PA3 0.03 -0.02  1.00  0.05
PA4 0.02 -0.04  0.05  1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.95 0.91 0.87 0.84
Multiple R square of scores with factors          0.91 0.84 0.76 0.71
Minimum correlation of possible factor scores     0.82 0.67 0.52 0.42
\end{verbatim}

We can make it a little easier to interpret by removing all factor loadings below .30.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{print.psych}\NormalTok{(grmsPAF2obl, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{sort=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "oblimin", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
        item   PA1   PA2   PA3   PA4   h2   u2 com
Marg1     11  0.91                   0.84 0.16 1.0
Marg2     12  0.76                   0.61 0.39 1.1
Marg3     13  0.67                   0.45 0.55 1.0
Marg4     14  0.63                   0.43 0.57 1.2
Marg5     15  0.59                   0.40 0.60 1.3
Marg6     16  0.55                   0.43 0.57 1.8
Marg7     17  0.50                   0.38 0.62 2.0
Obj2       2        0.68             0.49 0.51 1.1
Obj1       1        0.64             0.43 0.57 1.1
Obj3       3        0.60             0.38 0.62 1.1
Obj4       4        0.59             0.35 0.65 1.0
Obj5       5        0.57             0.37 0.63 1.4
Obj7       7        0.53             0.35 0.65 1.5
Obj6       6        0.49             0.30 0.70 1.4
Obj8       8        0.48             0.25 0.75 1.2
Obj10     10        0.41             0.22 0.78 1.6
Obj9       9        0.41             0.21 0.79 1.5
Angry3    25              0.68       0.50 0.50 1.1
Angry1    23              0.67       0.49 0.51 1.3
Angry2    24              0.67       0.48 0.52 1.1
Strong4   21                    0.58 0.39 0.61 1.3
Strong2   19                    0.53 0.29 0.71 1.1
Strong1   18                    0.52 0.31 0.69 1.3
Strong3   20                    0.50 0.29 0.71 1.4
Strong5   22                    0.44 0.23 0.77 1.5

                       PA1  PA2  PA3  PA4
SS loadings           3.33 3.19 1.67 1.65
Proportion Var        0.13 0.13 0.07 0.07
Cumulative Var        0.13 0.26 0.33 0.39
Proportion Explained  0.34 0.32 0.17 0.17
Cumulative Proportion 0.34 0.66 0.83 1.00

 With factor correlations of 
     PA1   PA2   PA3   PA4
PA1 1.00  0.03  0.03  0.02
PA2 0.03  1.00 -0.02 -0.04
PA3 0.03 -0.02  1.00  0.05
PA4 0.02 -0.04  0.05  1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.95 0.91 0.87 0.84
Multiple R square of scores with factors          0.91 0.84 0.76 0.71
Minimum correlation of possible factor scores     0.82 0.67 0.52 0.42
\end{verbatim}

All of the items stayed in their respective factors Note, though, that because our specification included ``sort=TRUE'' that the relative weights wiggled around and so the items are listed in a different order than in the orthogonal rotation.

The oblique rotation allows us to see the correlation between the factors/scales. This was not available in the orthogonal rotation because the assumption of the orthogonal/varimax rotation is that the scales/factors are uncorrelated; hence in the analysis they were fixed to 0.0.

We can see that all the scales have almost no relation with each other. That is the the correlations range between -0.04 to 0.05. This is unusual and likely a biproduct of simulating data. It does, though, support the orthogonal rotation as the preferred one.

Of course there is always a little complexity. In oblique rotations, there is a distinction between the \emph{pattern} matrix (which reports factor loadings and is comparable to the matrix we interpreted for the orthogonal rotation) and the \emph{structure} matrix (takes into account the relationship between the factors/scales -- it is a product of the pattern matrix and the matrix containing the correlation coefficients between the factors/scales). Most interpret the pattern matrix because it is simpler; however it could be that values in the pattern matrix are suppressed because of relations between the factors. Therefore, the structure matrix can be a useful check and some editors will request it.

Obtaining the structure matrix requires two steps. First, multiply the factor loadings with the phi matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grmsPAF2obl}\OperatorTok{$}\NormalTok{loadings }\OperatorTok{%*%}\StringTok{ }\NormalTok{grmsPAF2obl}\OperatorTok{$}\NormalTok{Phi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 PA1         PA2          PA3          PA4
Obj1     0.091617245  0.64179460 -0.082116054 -0.086955997
Obj2    -0.064381577  0.68533828 -0.010177709 -0.126827177
Obj3     0.009666359  0.59740193  0.137430240 -0.013192598
Obj4     0.038521534  0.59022701 -0.003149310  0.008964478
Obj5    -0.167648230  0.55513250  0.026889947  0.128051314
Obj6     0.220900534  0.50057516  0.069632821 -0.073189898
Obj7     0.231920793  0.53246553 -0.121211510  0.045773488
Obj8    -0.016732470  0.47326894 -0.049300632  0.120380779
Obj9     0.054345805  0.40181963  0.077792112  0.183004446
Obj10    0.066416194  0.40685467 -0.071121938  0.201990945
Marg1    0.909316145  0.05658834 -0.052531028 -0.009345473
Marg2    0.758728791 -0.09131277  0.169966028  0.045330248
Marg3    0.665014268  0.02942008  0.049650256 -0.055406392
Marg4    0.630496623  0.15006307  0.084401291 -0.068984727
Marg5    0.590748368  0.19980732  0.037862201 -0.117798546
Marg6    0.551643326 -0.02771386 -0.186799079  0.280811565
Marg7    0.500021599 -0.22688015  0.214615010  0.210736738
Strong1 -0.023734203 -0.01446513  0.214332979  0.527155113
Strong2 -0.074308426  0.04206983  0.002679449  0.523406328
Strong3 -0.079576059  0.13356854 -0.115836132  0.487340804
Strong4  0.003977093 -0.07474787  0.234704732  0.590024683
Strong5  0.147194163  0.03193109 -0.122335743  0.436688544
Angry1   0.013344627  0.19735056  0.659875942 -0.079698748
Angry2   0.026436508 -0.05154707  0.676344447  0.166509373
Angry3   0.085638675 -0.12109116  0.691072295  0.100726154
\end{verbatim}

Then use Field's \citeyearpar{field_discovering_2012} function to produce the matrix.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Field's function to produce the structure matrix}
\NormalTok{factor.structure <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(fa, }\DataTypeTok{cut =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{decimals =} \DecValTok{2}\NormalTok{)\{}
\NormalTok{    structure.matrix <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.sort}\NormalTok{(fa}\OperatorTok{$}\NormalTok{loadings }\OperatorTok{%*%}\StringTok{ }\NormalTok{fa}\OperatorTok{$}\NormalTok{Phi)}
\NormalTok{    structure.matrix <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{abs}\NormalTok{(structure.matrix) }\OperatorTok{<}\StringTok{ }\NormalTok{cut, }\StringTok{""}\NormalTok{, }\KeywordTok{round}\NormalTok{(structure.matrix, decimals)))}
    \KeywordTok{return}\NormalTok{(structure.matrix)}
\NormalTok{    \}}
    
\KeywordTok{factor.structure}\NormalTok{(grmsPAF2obl, }\DataTypeTok{cut =} \FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         PA1  PA2  PA3  PA4
Marg1   0.91               
Marg2   0.76               
Marg3   0.67               
Marg4   0.63               
Marg5   0.59               
Marg6   0.55               
Marg7    0.5               
Obj2         0.69          
Obj1         0.64          
Obj3          0.6          
Obj4         0.59          
Obj5         0.56          
Obj7         0.53          
Obj6          0.5          
Obj8         0.47          
Obj10        0.41          
Obj9          0.4          
Angry3            0.69     
Angry2            0.68     
Angry1            0.66     
Strong4                0.59
Strong1                0.53
Strong2                0.52
Strong3                0.49
Strong5                0.44
\end{verbatim}

Although some of the relative values changed, our items were stable regarding their component membership.

\hypertarget{factor-scores}{%
\subsection{Factor Scores}\label{factor-scores}}

Factor \emph{scores} (PA scores) can be created for each case (row) on each component (column). These can be used to assess the relative standing of one person on the construct/variable to another. We can also use them in regression (in place of means or sums) when groups of predictors correlate so highly that there is multicolliearity.

Computation involves multiplying an individual's item-level responses by the component loadings we obtained through the PCA process. The results will be one score per component for each row/case.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#in all of this, don't forget to be specifiying the datset that has the reverse-coded item replaced }
\NormalTok{grmsPAF2obl <-}\StringTok{ }\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa}\NormalTok{(dfGRMS, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{fm =} \StringTok{"pa"}\NormalTok{, }\DataTypeTok{rotate =} \StringTok{"oblimin"}\NormalTok{, }\DataTypeTok{scores =} \OtherTok{TRUE}\NormalTok{)}
\KeywordTok{head}\NormalTok{(grmsPAF2obl}\OperatorTok{$}\NormalTok{scores, }\DecValTok{10}\NormalTok{) }\CommentTok{#shows us only the first 10 (of N = 2571)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               PA1         PA2         PA3          PA4
 [1,]  1.064294757 -0.36271545 -0.05808223  0.646299885
 [2,]  0.859678584 -0.36907029 -0.14101755 -1.060489782
 [3,] -0.207333849 -1.11352702 -2.20075890 -1.273124196
 [4,]  0.003072714 -0.08097616  1.53024949 -1.097663890
 [5,]  0.341575555  0.37185409 -1.68365215 -0.067680029
 [6,]  1.388246787  0.61847412 -0.18638299  0.429949093
 [7,] -1.094742433 -0.84883604  0.19227720  0.974728637
 [8,] -0.298883211 -1.12083125 -0.88288628 -0.004756817
 [9,]  0.250752016  0.54276969  1.27348563  1.554382437
[10,] -0.202795623  0.35651240  0.26740434  0.263447499
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfGRMS <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(dfGRMS, grmsPAF2obl}\OperatorTok{$}\NormalTok{scores) }\CommentTok{#adds them to our raw dataset}
\end{Highlighting}
\end{Shaded}

To bring this full circle, we can see the correlation of the component scores; the pattern maps onto what we saw previously in the correlations between factors in the oblique rotation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{corr.test}\NormalTok{(dfGRMS [}\KeywordTok{c}\NormalTok{(}\StringTok{"PA1"}\NormalTok{, }\StringTok{"PA2"}\NormalTok{, }\StringTok{"PA3"}\NormalTok{, }\StringTok{"PA4"}\NormalTok{)])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = dfGRMS[c("PA1", "PA2", "PA3", "PA4")])
Correlation matrix 
     PA1   PA2   PA3   PA4
PA1 1.00  0.03  0.04  0.02
PA2 0.03  1.00 -0.03 -0.03
PA3 0.04 -0.03  1.00  0.08
PA4 0.02 -0.03  0.08  1.00
Sample Size 
[1] 259
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
     PA1  PA2  PA3 PA4
PA1 0.00 1.00 1.00   1
PA2 0.59 0.00 1.00   1
PA3 0.56 0.66 0.00   1
PA4 0.71 0.58 0.18   0

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

We can extract the factor loadings and write them to a table. This can be useful in preparing an APA style table for a manuscript or presentation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#names(grmsPAF2obl)}
\NormalTok{pafOBL_table <-}\StringTok{ }\KeywordTok{round}\NormalTok{(grmsPAF2obl}\OperatorTok{$}\NormalTok{loadings,}\DecValTok{3}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(pafOBL_table, }\DataTypeTok{file=}\StringTok{"pafOBL_table.csv"}\NormalTok{, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{pafOBL_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Loadings:
        PA1    PA2    PA3    PA4   
Obj1            0.636              
Obj2            0.684        -0.100
Obj3            0.601  0.152       
Obj4            0.591              
Obj5    -0.187  0.567         0.151
Obj6     0.207  0.495              
Obj7     0.221  0.526 -0.120       
Obj8            0.478         0.142
Obj9            0.410         0.194
Obj10           0.412         0.221
Marg1    0.912                     
Marg2    0.757 -0.107  0.142       
Marg3    0.665                     
Marg4    0.626  0.132              
Marg5    0.588  0.180        -0.125
Marg6    0.554        -0.221  0.280
Marg7    0.496 -0.229  0.183  0.182
Strong1                0.187  0.518
Strong2                       0.529
Strong3         0.152 -0.137  0.502
Strong4                0.202  0.577
Strong5  0.142        -0.150  0.444
Angry1          0.210  0.671 -0.108
Angry2                 0.668  0.129
Angry3         -0.104  0.683       

                 PA1   PA2   PA3   PA4
SS loadings    3.324 3.196 1.660 1.654
Proportion Var 0.133 0.128 0.066 0.066
Cumulative Var 0.133 0.261 0.327 0.393
\end{verbatim}

We can also obtain a figure of this PAF with oblique rotation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\OperatorTok{::}\KeywordTok{fa.diagram}\NormalTok{(grmsPAF2obl)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-165-1.pdf}

\hypertarget{apa-style-results-1}{%
\section{APA Style Results}\label{apa-style-results-1}}

\textbf{Results}

The dimensionality of the 25 items from the Gendered Racial Microagressions Scale for Black Women was analyzed using principal axis factoring. First, data were screened to determine the suitability of the data for this analyses. The Kaiser-Meyer- Olkin measure of sampling adequacy (KMO; Kaiser, 1970) represents the ratio of the squared correlation between variables to the squared partial correlation between variables. KMO ranges from 0.00 to 1.00; values closer to 1.00 indicate that the patterns of correlations are relatively compact and that component analysis should yield distinct and reliable components (Field, 2012). In our dataset, the KMO value was .86, indicating acceptable sampling adequacy. The Barlett's Test of Sphericity examines whether the population correlation matrix resembles an identity matrix (Field, 2012). When the p value for the Bartlett's test is \textless{} .05, we are fairly certain we have clusters of correlated variables. In our dataset, \(\chi ^{1}(300)=1683.76, p < .001\), indicating the correlations between items are sufficiently large enough for principal components analysis. The determinant of the correlation matrix alerts us to any issues of multicollinearity or singularity and should be larger than 0.00001. Our determinant was 0.00115 and, again, indicated that our data was suitable for the analysis.

Four criteria were used to determine the number of factors to rotate: a priori theory, the scree test, the Eigenvalue-greater-than-one criteria, and the interpretability of the solution. Kaiser's eigenvalue-greater-than-one criteria suggested four components, and, in combination explained 49\% of the variance. The scree plot was showed an inflexion that would justified retaining four components. Based on the convergence of these decisions, four components were extracted. We investigated each with orthogonal (Varimax) and oblique (oblimin) procedures. Given the non-significant correlations (ranging from -0.04 to 0.05) and the clear component loadings in the orthogonal rotation, we determined that an orthogonal solution was most appropriate.

The rotated solution, as shown in Table 1 and Figure 1, yielded four interpretable components, each listed with the proportion of variance accounted for: assumptions of beauty and sexual objectification (13\%), silenced and marginalized (13\%), strong woman stereotype (7\%), and angry woman stereotype (7\%).

Regarding the Table 1, I would include a table with ALL the values, bolding those with component membership. This will be easy because we exported all those values to a .csv file.

\hypertarget{comparing-fa-and-pca}{%
\subsection{Comparing FA and PCA}\label{comparing-fa-and-pca}}

\begin{itemize}
\tightlist
\item
  FA drives a mathematical solution from which factors are estimated

  \begin{itemize}
  \tightlist
  \item
    Only FA can estimate underlying factors, but it relies on the various assumptions to be met
  \end{itemize}
\item
  PCA decomposes the original data into a set of linear variates

  \begin{itemize}
  \tightlist
  \item
    This limits its concern to establishing which linear components exist within the data and how a particular variable might contribute to that component
  \end{itemize}
\item
  Generally FA and PCA result in similar solutions

  \begin{itemize}
  \tightlist
  \item
    When there are 30 or more variables and communalities are \textgreater{} .7 for all variables, different solutions are unlikely (Stevens, 2002)
  \item
    When there are \textless{} 20 variables and low communalities (\textless{} .4) different solutions are likely to emerge
  \item
    Both are inferential statistics
  \end{itemize}
\item
  Critics of PCA suggest

  \begin{itemize}
  \tightlist
  \item
    ``at best it is a common factor analysis with some error added and at worst an unrecognizable hodgepodge of things from which nothing can be determined'' (Cliff, 1987, p.~349)
  \item
    PCA should never be described as FA and the resulting components should not be treated as reverently as true, latent variable, \emph{factors}
  \item
    To most of us (i.e., scientist-practitioners), the difference is largely from the algorithm used to drive the solutions. This is true for Field \citep{field_discovering_2012} also, who uses the terms interchangeably. My take: use whichever you like, just be precise in the language describing what you did.
  \end{itemize}
\end{itemize}

\hypertarget{going-back-to-the-future-what-then-is-omega}{%
\section{Going Back to the Future: What, then, is Omega?}\label{going-back-to-the-future-what-then-is-omega}}

Now that we've had an introduction to factor analysis, let's revisit the \(\omega\) grouping of reliability estimates. In the context of \emph{psychometrics} it may be useful to think of factors as scales/subscales where \emph{g} refers to the amount of variance in the \emph{general} factor (or total scale score) and subcales to be items that have something in common that is separate from what is \emph{g}.

Model based estimates examine the correlations or covariances of the items and decompose the test variance into that which is

\begin{itemize}
\tightlist
\item
  common to all items (\textbf{g}, a general factor),
\item
  specific to some items (\textbf{f}, orthogonal group factors), and
\item
  unique to each item (confounding \textbf{s} specific, and \textbf{e} error variance)
\end{itemize}

In the \emph{psych} package

\begin{itemize}
\tightlist
\item
  \(\omega_{t}\) represents the total reliability of the test (\(\omega_{t}\))

  \begin{itemize}
  \tightlist
  \item
    In the \emph{psych} package, this is calculated from a bifactor model where there is one general \emph{g} factor (i.e., each item loads on the single general factor), one or more group factors (\emph{f}), and an item-specific factor (\emph{s}).
  \end{itemize}
\item
  \(\omega_{h}\) extracts a higher order factor from the correlation matrix of lower level factors, then applies the Schmid and Leiman (1957) transformation to find the general loadings on the original items. Stated another way, it is a measure o f the general factor saturation (\emph{g}; the amount of variance attributable to one comon factor). The subscript ``h'' acknowledges the hierarchical nature of the approach.

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{h}\) approach is exploratory and defined if there are three or more group factors (with only two group factors, the default is to assume they are equally important, hence the factor loadings of those subscales will be equal)
  \item
    Najera Catalan \citep{najera_catalan_reliability_2019} suggests that \(\omega_{h}\) is the best measure of reliability when dealing with multiple dimensions.
  \end{itemize}
\item
  \(\omega_{g}\) is an estimate that uses a bifactor solution via the SEM package \emph{lavaan} and tends to be a larger (because it forces all the cross loadings of lower level factors to be 0)

  \begin{itemize}
  \tightlist
  \item
    the \(\omega_{g}\) is confirmatory, requiring the specification of which variables load on each group factor
  \end{itemize}
\item
  \emph{psych::omegaSem()} reports both EFA and CFA solutions

  \begin{itemize}
  \tightlist
  \item
    We will use the \emph{psych::omegaSem()} function
  \end{itemize}
\end{itemize}

Note that in our specification, we indicate there are two factors. We do not tell it (anywhere!) what items belong to what factors (think, \emph{subscales}). One test will be to see if the items align with their respective factors.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Because we added the component scores to our df (and now it has more variables than just our items), I will estimate omegaSem with the correlation matrix; I will need to tell it the n.obs}

\NormalTok{psych}\OperatorTok{::}\KeywordTok{omegaSem}\NormalTok{(GRMSr, }\DataTypeTok{nfactors =} \DecValTok{4}\NormalTok{, }\DataTypeTok{n.obs=}\DecValTok{259}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-166-1.pdf} \includegraphics{ReC_Psychometrics_files/figure-latex/unnamed-chunk-166-2.pdf}

\begin{verbatim}
 
Call: psych::omegaSem(m = GRMSr, nfactors = 4, n.obs = 259)
Omega 
Call: omegah(m = m, nfactors = nfactors, fm = fm, key = key, flip = flip, 
    digits = digits, title = title, sl = sl, labels = labels, 
    plot = plot, n.obs = n.obs, rotate = rotate, Phi = Phi, option = option, 
    covar = covar)
Alpha:                 0.6 
G.6:                   0.71 
Omega Hierarchical:    0.07 
Omega H asymptotic:    0.09 
Omega Total            0.74 

Schmid Leiman Factor loadings greater than  0.2 
            g   F1*   F2*   F3*   F4*   h2   u2   p2
Obj1-               -0.63             0.43 0.57 0.02
Obj2-               -0.68             0.49 0.51 0.02
Obj3-               -0.60             0.38 0.62 0.00
Obj4-               -0.59             0.35 0.65 0.01
Obj5-               -0.56             0.37 0.63 0.00
Obj6-         -0.21 -0.49             0.30 0.70 0.00
Obj7-         -0.22 -0.52             0.35 0.65 0.01
Obj8-               -0.48             0.25 0.75 0.00
Obj9                 0.41             0.21 0.79 0.00
Obj10                0.41        0.21 0.22 0.78 0.00
Marg1          0.91                   0.84 0.16 0.00
Marg2          0.75                   0.61 0.39 0.02
Marg3          0.66                   0.45 0.55 0.00
Marg4          0.62                   0.43 0.57 0.00
Marg5-        -0.59                   0.40 0.60 0.00
Marg6          0.55       -0.22  0.27 0.43 0.57 0.01
Marg7          0.49 -0.23             0.38 0.62 0.06
Strong1                          0.50 0.31 0.69 0.10
Strong2                          0.51 0.29 0.71 0.05
Strong3                          0.48 0.29 0.71 0.02
Strong4  0.20              0.20  0.56 0.39 0.61 0.11
Strong5                          0.43 0.23 0.77 0.04
Angry1               0.21  0.65       0.49 0.51 0.02
Angry2                     0.65       0.48 0.52 0.07
Angry3                     0.67       0.50 0.50 0.07

With eigenvalues of:
   g  F1*  F2*  F3*  F4* 
0.25 3.30 3.16 1.58 1.53 

general/max  0.08   max/min =   2.15
mean percent general =  0.03    with sd =  0.03 and cv of  1.31 
Explained Common Variance of the general factor =  0.03 

The degrees of freedom are 206  and the fit is  0.22 
The number of observations was  259  with Chi Square =  53.89  with prob <  1
The root mean square of the residuals is  0.02 
The df corrected root mean square of the residuals is  0.02
RMSEA index =  0  and the 10 % confidence intervals are  0 0
BIC =  -1090.82

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 275  and the fit is  6.44 
The number of observations was  259  with Chi Square =  1598.25  with prob <  0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004
The root mean square of the residuals is  0.19 
The df corrected root mean square of the residuals is  0.2 

RMSEA index =  0.136  and the 10 % confidence intervals are  0.13 0.143
BIC =  70.12 

Measures of factor score adequacy             
                                                  g  F1*  F2*  F3*  F4*
Correlation of scores with factors             0.31 0.95 0.91 0.85 0.81
Multiple R square of scores with factors       0.10 0.90 0.83 0.72 0.66
Minimum correlation of factor score estimates -0.80 0.81 0.66 0.45 0.32

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*  F4*
Omega total for total scores and subscales    0.74 0.76 0.67 0.72 0.65
Omega general for total scores and subscales  0.07 0.01 0.01 0.04 0.05
Omega group for total scores and subscales    0.61 0.75 0.66 0.69 0.60

 The following analyses were done using the  lavaan  package 

 Omega Hierarchical from a confirmatory model using sem =  0.4
 Omega Total  from a confirmatory model using sem =  0.73 
With loadings of 
             g   F1*   F2*   F3*   F4*   h2   u2   p2
Obj1      0.36       -0.54             0.42 0.58 0.31
Obj2      0.33       -0.59             0.46 0.54 0.24
Obj3                 -0.60             0.37 0.63 0.04
Obj4      0.24       -0.54             0.35 0.65 0.16
Obj5                 -0.55             0.31 0.69 0.03
Obj6                 -0.47             0.25 0.75 0.13
Obj7      0.26       -0.47             0.29 0.71 0.23
Obj8                 -0.45             0.23 0.77 0.11
Obj9-                 0.46             0.21 0.79 0.00
Obj10                -0.41             0.18 0.82 0.04
Marg1          -0.91                   0.84 0.16 0.01
Marg2-          0.75                   0.59 0.41 0.05
Marg3          -0.67                   0.45 0.55 0.00
Marg4          -0.63                   0.40 0.60 0.02
Marg5          -0.58                   0.37 0.63 0.07
Marg6-          0.54                   0.30 0.70 0.00
Marg7-    0.36  0.50                   0.38 0.62 0.34
Strong1-  0.35                    0.41 0.29 0.71 0.42
Strong2-                          0.47 0.26 0.74 0.14
Strong3                          -0.58 0.33 0.67 0.00
Strong4-  0.45                    0.45 0.41 0.59 0.49
Strong5-                          0.45 0.20 0.80 0.00
Angry1-                     0.77       0.60 0.40 0.01
Angry2-   0.44              0.52       0.47 0.53 0.41
Angry3-   0.45              0.51       0.46 0.54 0.44

With sum of squared loadings of:
  g F1* F2* F3* F4* 
1.4 3.1 2.6 1.1 1.1 

The degrees of freedom of the confirmatory model are  250  and the fit is  250.7259  with p =  0.4751848
general/max  0.45   max/min =   2.78
mean percent general =  0.15    with sd =  0.16 and cv of  1.1 
Explained Common Variance of the general factor =  0.15 

Measures of factor score adequacy             
                                                 g  F1*  F2*  F3*  F4*
Correlation of scores with factors            0.78 1.06 0.96 0.91 0.85
Multiple R square of scores with factors      0.60 1.13 0.92 0.82 0.72
Minimum correlation of factor score estimates 0.20 1.26 0.84 0.64 0.44

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*  F3*  F4*
Omega total for total scores and subscales    0.73 0.32 0.75 0.74 0.42
Omega general for total scores and subscales  0.40 0.15 0.12 0.17 0.18
Omega group for total scores and subscales    0.40 0.17 0.62 0.57 0.24

To get the standard sem fit statistics, ask for summary on the fitted object
\end{verbatim}

There's a ton of output! How do we make sense of it?

First, our items aligned perfectly with their respective factors (subscales). That is, it would be problematic if the items switched factors.

Second, we can interpret our results. Like alpha, the omegas range from 0 to 1, where values closer to 1 represent good reliability \citep{najera_catalan_reliability_2019}. For unidimensional measures, \(\omega_{t}\) values above 0.80 seem to be an indicator of good reliability. For multidimensional measures with well-defined dimensions we strive for \(\omega_{h}\) values above 0.65 (and \(\omega_{t}\) \textgreater{} 0.8). These recommendations are based on a Monte Carlo study that examined a host of reliability indicators and how their values corresponded with accurate predictions of poverty status. With this in mind, let's examine the output related to our simulated research vignette.

Let's examine the output in the lower portion where the values are ``from a confirmatory model using sem.''

Omega is a reliability estimate for factor analysis that represents the proportion of variance in the GRMS scale attributable to common variance, rather than error. The omega for the total reliability of the test (\(\omega_{t}\); which included the general factors and the subscale factors) was .73, meaning that 73\% of the variance in the total scale is due to the factors and 27\% (100\% - 73\%) is attributable to error.

Omega hierarchical (\(\omega_{h}\)) estimates are the proportion of variance in the GRMS score attributable to the general factor, which in effect treats the subscales as error. \(\omega_{h}\) for the the GRMS total scale was .40 A quick calculation with \(\omega_{h}\) (.40) and \(\omega_{t}\) (.73; .40/.73 = .55) lets us know that that 55\% of the reliable variance in the GRMS total scale is attributable to the general factor.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{.4}\OperatorTok{/}\NormalTok{.}\DecValTok{73}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5479452
\end{verbatim}

Amongst the output is the Cronbach's alpha coefficient (.60). Lewis and Neville \citeyearpar{lewis_construction_2015} did not report omega results. They reported an alpha of .92 for the version of the GRMS that assessed stress appraisal.

\hypertarget{comparing-pfa-to-item-analysis-and-pca}{%
\section{Comparing PFA to Item Analysis and PCA}\label{comparing-pfa-to-item-analysis-and-pca}}

In the lesson on PCA we began a table that compared our item analysis (item corrected-total correlations with item-other scale correlations) and PCA results (both orthogonal and oblimin). Let's now add our PAF results (both orthogonal and oblimin).

In the prior lecture I saved the file as both .rds and .csv objects. I will bring back in the .rds object and add to it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GRMScomps <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(}\StringTok{"GRMS_Comparisons.rds"}\NormalTok{)}
\NormalTok{grmsPAF2ORTH}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Factor Analysis using method =  pa
Call: psych::fa(r = dfGRMS, nfactors = 4, rotate = "varimax", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
          PA1   PA2   PA3   PA4   h2   u2 com
Obj1     0.07  0.64 -0.08 -0.08 0.43 0.57 1.1
Obj2    -0.09  0.68  0.00 -0.12 0.49 0.51 1.1
Obj3    -0.01  0.60  0.14 -0.02 0.38 0.62 1.1
Obj4     0.02  0.59  0.00  0.01 0.35 0.65 1.0
Obj5    -0.19  0.56  0.04  0.13 0.37 0.63 1.3
Obj6     0.20  0.50  0.07 -0.08 0.30 0.70 1.4
Obj7     0.21  0.53 -0.13  0.05 0.35 0.65 1.5
Obj8    -0.03  0.48 -0.05  0.12 0.25 0.75 1.2
Obj9     0.04  0.41  0.08  0.18 0.21 0.79 1.5
Obj10    0.05  0.41 -0.07  0.20 0.22 0.78 1.6
Marg1    0.91  0.06 -0.08 -0.03 0.84 0.16 1.0
Marg2    0.76 -0.08  0.14  0.02 0.61 0.39 1.1
Marg3    0.66  0.03  0.03 -0.07 0.45 0.55 1.0
Marg4    0.63  0.15  0.06 -0.09 0.43 0.57 1.2
Marg5    0.58  0.20  0.02 -0.13 0.40 0.60 1.3
Marg6    0.55 -0.02 -0.21  0.28 0.43 0.57 1.8
Marg7    0.51 -0.21  0.19  0.19 0.38 0.62 2.0
Strong1 -0.02  0.01  0.21  0.52 0.31 0.69 1.3
Strong2 -0.07  0.06  0.00  0.53 0.29 0.71 1.1
Strong3 -0.08  0.15 -0.11  0.50 0.29 0.71 1.4
Strong4  0.01 -0.05  0.23  0.58 0.39 0.61 1.3
Strong5  0.15  0.05 -0.13  0.44 0.23 0.77 1.4
Angry1   0.01  0.21  0.66 -0.11 0.49 0.51 1.3
Angry2   0.03 -0.03  0.68  0.13 0.48 0.52 1.1
Angry3   0.09 -0.11  0.69  0.06 0.50 0.50 1.1

                       PA1  PA2  PA3  PA4
SS loadings           3.33 3.20 1.67 1.64
Proportion Var        0.13 0.13 0.07 0.07
Cumulative Var        0.13 0.26 0.33 0.39
Proportion Explained  0.34 0.33 0.17 0.17
Cumulative Proportion 0.34 0.66 0.83 1.00

Mean item complexity =  1.3
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  300  and the objective function was  6.77 with Chi Square of  1683.76
The degrees of freedom for the model are 206  and the objective function was  0.22 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.02 

The harmonic number of observations is  259 with the empirical chi square  43.59  with prob <  1 
The total number of observations was  259  with Likelihood Chi Square =  53.89  with prob <  1 

Tucker Lewis Index of factoring reliability =  1.162
RMSEA index =  0  and the 90 % confidence intervals are  0 0
BIC =  -1090.82
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                   PA1  PA2  PA3  PA4
Correlation of (regression) scores with factors   0.95 0.91 0.87 0.84
Multiple R square of scores with factors          0.91 0.84 0.76 0.71
Minimum correlation of possible factor scores     0.81 0.67 0.52 0.42
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#names(grmsPAF2ORTH)}
\NormalTok{pafORTH_loadings <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{unclass}\NormalTok{(grmsPAF2ORTH}\OperatorTok{$}\NormalTok{loadings))}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}
\NormalTok{pafORTH_loadings}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#Item names for joining (and to make sure we know which variable is which)}
\NormalTok{pafORTH_loadings <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{rename}\NormalTok{ (pafORTH_loadings, }\DataTypeTok{PAF_OR_Mar =}\NormalTok{ PA1, }\DataTypeTok{PAF_OR_Obj =}\NormalTok{ PA2, }\DataTypeTok{PAF_OR_Ang =}\NormalTok{ PA3, }\DataTypeTok{PAF_OR_Str =}\NormalTok{ PA4)}
\NormalTok{GRMScomps <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{full_join}\NormalTok{(GRMScomps, pafORTH_loadings, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}

\CommentTok{#Now adding the PAF oblique loadings}
\NormalTok{pafOBLQ_loadings <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{unclass}\NormalTok{(grmsPAF2obl}\OperatorTok{$}\NormalTok{loadings))}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}
\NormalTok{pafOBLQ_loadings}\OperatorTok{$}\NormalTok{Items <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Obj1"}\NormalTok{, }\StringTok{"Obj2"}\NormalTok{, }\StringTok{"Obj3"}\NormalTok{, }\StringTok{"Obj4"}\NormalTok{, }\StringTok{"Obj5"}\NormalTok{, }\StringTok{"Obj6"}\NormalTok{, }\StringTok{"Obj7"}\NormalTok{, }\StringTok{"Obj8"}\NormalTok{, }\StringTok{"Obj9"}\NormalTok{, }\StringTok{"Obj10"}\NormalTok{,}\StringTok{"Marg1"}\NormalTok{, }\StringTok{"Marg2"}\NormalTok{, }\StringTok{"Marg3"}\NormalTok{, }\StringTok{"Marg4"}\NormalTok{, }\StringTok{"Marg5"}\NormalTok{, }\StringTok{"Marg6"}\NormalTok{, }\StringTok{"Marg7"}\NormalTok{, }\StringTok{"Strong1"}\NormalTok{, }\StringTok{"Strong2"}\NormalTok{, }\StringTok{"Strong3"}\NormalTok{, }\StringTok{"Strong4"}\NormalTok{, }\StringTok{"Strong5"}\NormalTok{, }\StringTok{"Angry1"}\NormalTok{, }\StringTok{"Angry2"}\NormalTok{, }\StringTok{"Angry3"}\NormalTok{) }\CommentTok{#Item names for joining (and to make sure we know which variable is which)}
\NormalTok{pafOBLQ_loadings <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{rename}\NormalTok{ (pafOBLQ_loadings, }\DataTypeTok{PAF_OB_Mar =}\NormalTok{ PA1, }\DataTypeTok{PAF_OB_Obj =}\NormalTok{ PA2, }\DataTypeTok{PAF_OB_Ang =}\NormalTok{ PA3, }\DataTypeTok{PAF_OB_Str =}\NormalTok{ PA4)}
\NormalTok{GRMScomps <-}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{full_join}\NormalTok{(GRMScomps, pafOBLQ_loadings, }\DataTypeTok{by =} \StringTok{"Items"}\NormalTok{)}\CommentTok{#I had to add "unclass" to the loadings to render them into a df}

\KeywordTok{write.csv}\NormalTok{(GRMScomps, }\DataTypeTok{file =} \StringTok{"GRMS_Comps.csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{","}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col.names=}\OtherTok{TRUE}\NormalTok{)}\CommentTok{#Writes the table to a .csv file where you can open it with Excel and format )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in write.csv(GRMScomps, file = "GRMS_Comps.csv", sep = ",", row.names =
FALSE, : attempt to set 'col.names' ignored
\end{verbatim}

\begin{verbatim}
Warning in write.csv(GRMScomps, file = "GRMS_Comps.csv", sep = ",", row.names =
FALSE, : attempt to set 'sep' ignored
\end{verbatim}

As a research vignette, this has worked extremely well, modeling consistency across the item analysis, principal components analysis (PCA), and principal axis factoring (PAF). That is, items load highest on their own scale (whether it is a component or factor), have no cross-loadings, and do not switch scale memberships from analysis to analysis.

\begin{figure}
\centering
\includegraphics{images/PAF/GRMScomps.png}
\caption{Comparison of path models for PCA and EFA}
\end{figure}

\hypertarget{practice-problems-7}{%
\section{Practice Problems}\label{practice-problems-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In the \emph{ReCentering Psych Stats: Psychometrics} OER, it would be ideal if you have selected a dataset you can utilize across the lessons. The least complex is to change the random seed in the research and rework the problem demonstrated in the lesson. The most complex is to use data of your own. In either case, please plan to:

\begin{itemize}
\tightlist
\item
  Properly format and prepare the data.
\item
  Conduct diagnostic tests to determine the suitability of the data for PCA.
\item
  Conducting tests to guide the decisions about number of components to extract.
\item
  Conducting orthogonal and oblique extractions (at least two each with different numbers of components).
\item
  Selecting one solution and preparing an APA style results section (with table and figure).
\item
  Compare your results in light of any other psychometrics lessons where you have used this data (especially the \protect\hyperlink{ItemAnalSurvey}{item analysis} and \protect\hyperlink{PCA}{PCA} lessons).
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-4}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-4}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results. If PAF is new to you, perhaps you just change the number in ``set.seed(210921)'' from 210921 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-2-conduct-a-pca-with-the-szymanski-and-bissonette--szymanski_perceptions_2020-research-vignette-that-was-used-in-prior-lessons.-1}{%
\subsection{\texorpdfstring{Problem \#2: Conduct a PCA with the Szymanski and Bissonette \citeyearpar{szymanski_perceptions_2020} research vignette that was used in prior lessons.}{Problem \#2: Conduct a PCA with the Szymanski and Bissonette {[}-@szymanski\_perceptions\_2020{]} research vignette that was used in prior lessons.}}\label{problem-2-conduct-a-pca-with-the-szymanski-and-bissonette--szymanski_perceptions_2020-research-vignette-that-was-used-in-prior-lessons.-1}}

The second option involves utilizing one of the simulated datasets available in this OER. Szymanski and Bissonette's \citeyearpar{szymanski_perceptions_2020}Perceptions of the LGBTQ College Campus Climate Scale: Development and psychometric evaluation was used as the research vignette for the validity, reliability, and item analysis lessons. Although I switched vignettes, the Szymanski and Bissonette example is ready for PAF

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{problem-3-try-something-entirely-new.-4}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-4}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete PAF. The data should allow for at least two (ideally three) components/subscales.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}lcc@{}}
\toprule
\begin{minipage}[b]{0.50\columnwidth}\raggedright
Assignment Component\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\centering
Points Possible\strut
\end{minipage} & \begin{minipage}[b]{0.18\columnwidth}\centering
Points Earned\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.50\columnwidth}\raggedright
1. Check and, if needed, format data\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
2. Conduct and interpret the three diagnostic tests to determine if PCA is appropriate as an analysis (KMO, Bartlett's, determinant).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
3. Determine how many components to extract (e.g., scree plot, eigenvalues, theory).\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
4. Conduct an orthogonal extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
5. Conduct an oblique extraction and rotation.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
6. Repeat the orthogonal and oblique extractions/rotations with a different number of specified factors.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
7. APA style results section with table and figure of one of the solutions.\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
8. Explanation to grader\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
5\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.50\columnwidth}\raggedright
\textbf{Totals}\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\centering
40\strut
\end{minipage} & \begin{minipage}[t]{0.18\columnwidth}\centering
\_\_\_\_\_\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

  \bibliography{STATSnMETH.bib}

\end{document}
